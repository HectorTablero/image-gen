{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Image Generation Library","text":"<p>Welcome to the documentation for our image generation library. This library provides tools for training and using diffusion-based generative models for image generation tasks.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Implementations of various diffusion processes</li> <li>Multiple sampling algorithms</li> <li>Support for conditional image generation</li> <li>Utilities for evaluating model quality</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":""},{"location":"#installation","title":"Installation","text":"<p>Clone the repository:</p> <pre><code>git clone https://github.com/HectorTablero/image-gen.git\ncd image-gen\npip install -e .\n</code></pre>"},{"location":"#basic-usage","title":"Basic Usage","text":"<pre><code>from image_gen import GenerativeModel\n\n# Initialize a generative model\nmodel = GenerativeModel(diffusion=\"ve\", sampler=\"euler-maruyama\")\n\n# Train the model\nmodel.train(dataset, epochs=100, batch_size=32, lr=1e-3)\n\n# Generate images\ngenerated_images = model.generate(num_samples=10, n_steps=500)\n</code></pre>"},{"location":"#documentation-structure","title":"Documentation Structure","text":"<ul> <li>Generative Model</li> <li>Diffusion Processes</li> <li>Noise Schedules</li> <li>Samplers</li> <li>Metrics</li> </ul>"},{"location":"api/generative_model/","title":"Generative Model","text":"<p>The <code>GenerativeModel</code> class is the core of the library. It encapsulates the diffusion process, sampling algorithm, and score model.</p> <p>Generative model for diffusion-based image generation.</p> <p>This class implements a framework for training and using generative diffusion models for tasks such as image generation, colorization, and inpainting.</p> <p>Attributes:</p> Name Type Description <code>diffusion</code> <code>BaseDiffusion</code> <p>The diffusion process to use.</p> <code>sampler</code> <code>BaseSampler</code> <p>The sampling algorithm for generation.</p> <code>model</code> <code>Optional[ScoreNet]</code> <p>The underlying score network model.</p> <code>device</code> <code>device</code> <p>The device on which the model is running.</p> Source code in <code>image_gen\\base.py</code> <pre><code>class GenerativeModel:\n    \"\"\"Generative model for diffusion-based image generation.\n\n    This class implements a framework for training and using generative diffusion models\n    for tasks such as image generation, colorization, and inpainting.\n\n    Attributes:\n        diffusion: The diffusion process to use.\n        sampler: The sampling algorithm for generation.\n        model: The underlying score network model.\n        device: The device on which the model is running.\n    \"\"\"\n\n    DIFFUSION_MAP = {\n        \"variance exploding\": VarianceExploding,\n        \"varianceexploding\": VarianceExploding,\n        \"ve\": VarianceExploding,\n        \"variance preserving\": VariancePreserving,\n        \"variancepreserving\": VariancePreserving,\n        \"vp\": VariancePreserving,\n        \"sub-variance preserving\": SubVariancePreserving,\n        \"sub variance preserving\": SubVariancePreserving,\n        \"subvariancepreserving\": SubVariancePreserving,\n        \"sub-vp\": SubVariancePreserving,\n        \"subvp\": SubVariancePreserving,\n        \"svp\": SubVariancePreserving,\n    }\n    NOISE_SCHEDULE_MAP = {\n        \"linear noise schedule\": LinearNoiseSchedule,\n        \"linearnoiseschedule\": LinearNoiseSchedule,\n        \"linear\": LinearNoiseSchedule,\n        \"lin\": LinearNoiseSchedule,\n        \"l\": LinearNoiseSchedule,\n        \"cosine noise schedule\": CosineNoiseSchedule,\n        \"cosinenoiseschedule\": CosineNoiseSchedule,\n        \"cosine\": CosineNoiseSchedule,\n        \"cos\": CosineNoiseSchedule,\n        \"c\": CosineNoiseSchedule,\n    }\n    SAMPLER_MAP = {\n        \"euler-maruyama\": EulerMaruyama,\n        \"euler maruyama\": EulerMaruyama,\n        \"eulermaruyama\": EulerMaruyama,\n        \"euler\": EulerMaruyama,\n        \"em\": EulerMaruyama,\n        \"exponential integrator\": ExponentialIntegrator,\n        \"exponentialintegrator\": ExponentialIntegrator,\n        \"exponential\": ExponentialIntegrator,\n        \"exp\": ExponentialIntegrator,\n        \"ode probability flow\": ODEProbabilityFlow,\n        \"odeprobabilityflow\": ODEProbabilityFlow,\n        \"ode flow\": ODEProbabilityFlow,\n        \"ode\": ODEProbabilityFlow,\n        \"predictor-corrector\": PredictorCorrector,\n        \"predictor corrector\": PredictorCorrector,\n        \"predictorcorrector\": PredictorCorrector,\n        \"pred\": PredictorCorrector,\n    }\n    METRIC_MAP = {\n        \"bits per dimension\": BitsPerDimension,\n        \"bitsperdimension\": BitsPerDimension,\n        \"bpd\": BitsPerDimension,\n        \"fr\u00e9chet inception distance\": FrechetInceptionDistance,\n        \"frechet inception distance\": FrechetInceptionDistance,\n        \"frechetinceptiondistance\": FrechetInceptionDistance,\n        \"frechet\": FrechetInceptionDistance,\n        \"fr\u00e9chet\": FrechetInceptionDistance,\n        \"fid\": FrechetInceptionDistance,\n        \"inception score\": InceptionScore,\n        \"inceptionscore\": InceptionScore,\n        \"inception\": InceptionScore,\n        \"is\": InceptionScore,\n    }\n\n    def __init__(\n        self,\n        diffusion: Optional[Union[BaseDiffusion, type,\n                                  Literal[\"ve\", \"vp\", \"sub-vp\", \"svp\"]]] = \"ve\",\n        sampler: Optional[Union[BaseSampler, type,\n                                Literal[\"euler-maruyama\", \"euler\", \"em\",\n                                        \"exponential\", \"exp\", \"ode\",\n                                        \"predictor-corrector\", \"pred\"]]] = \"euler-maruyama\",\n        noise_schedule: Optional[Union[BaseNoiseSchedule, type,\n                                       Literal[\"linear\", \"lin\", \"cosine\", \"cos\"]]] = None,\n        verbose: bool = True\n    ) -&gt; None:\n        \"\"\"Initializes the generative model.\n\n        Args:\n            diffusion: The diffusion process to use. Can be a string identifier,\n                a diffusion class, or a diffusion instance.\n            sampler: The sampling algorithm to use. Can be a string identifier,\n                a sampler class, or a sampler instance.\n            noise_schedule: The noise schedule to use. Only required for diffusion\n                processes that need a noise schedule.\n            verbose: Whether to display progress bars during generation and training.\n\n        Raises:\n            ValueError: If an unknown diffusion or sampler string is provided.\n            TypeError: If the diffusion or sampler has an invalid type.\n        \"\"\"\n        self._model = None\n        self._verbose = verbose\n        self._num_classes = None  # Initialize this attribute\n        self._stored_labels = None\n        self._label_map = None\n        self._version = MODEL_VERSION\n        self._num_channels = None\n        self._shape = None  # Changed from _input_shape to _shape\n\n        if diffusion is None:\n            diffusion = \"ve\"\n\n        if isinstance(diffusion, str):\n            diffusion_key = diffusion.lower()\n            try:\n                diffusion = GenerativeModel.DIFFUSION_MAP[diffusion_key]\n            except KeyError:\n                raise ValueError(f\"Unknown diffusion string: {diffusion}\")\n\n        if sampler is None:\n            sampler = \"euler-maruyama\"\n\n        if isinstance(sampler, str):\n            sampler_key = sampler.lower()\n            try:\n                sampler = GenerativeModel.SAMPLER_MAP[sampler_key]\n            except KeyError:\n                raise ValueError(f\"Unknown sampler string: {sampler}\")\n\n        if noise_schedule is None and ((isinstance(diffusion, type) or\n                                        isinstance(diffusion, BaseDiffusion)) and\n                                       diffusion.NEEDS_NOISE_SCHEDULE):\n            noise_schedule = \"linear\"\n\n        if isinstance(noise_schedule, str):\n            ns_key = noise_schedule.lower()\n            try:\n                noise_schedule = GenerativeModel.NOISE_SCHEDULE_MAP[ns_key]\n            except KeyError:\n                raise ValueError(\n                    f\"Unknown noise_schedule string: {noise_schedule}\")\n\n        if isinstance(diffusion, type):\n            if diffusion.NEEDS_NOISE_SCHEDULE:\n                if isinstance(noise_schedule, type):\n                    ns_inst = noise_schedule()\n                else:\n                    ns_inst = noise_schedule\n                self.diffusion = diffusion(ns_inst)\n            else:\n                if noise_schedule is not None:\n                    warnings.warn(\n                        f\"{diffusion.__name__} does not require a noise schedule. \"\n                        f\"The provided noise schedule will be ignored.\",\n                        UserWarning\n                    )\n                self.diffusion = diffusion()\n        else:\n            if not diffusion.NEEDS_NOISE_SCHEDULE and noise_schedule is not None:\n                warnings.warn(\n                    f\"{diffusion.__class__.__name__} does not require a noise schedule. \"\n                    f\"The provided noise schedule will be ignored.\",\n                    UserWarning\n                )\n            self.diffusion = diffusion\n\n        if isinstance(sampler, type):\n            self.sampler = sampler(self.diffusion)\n        else:\n            self.sampler = sampler\n        self.sampler.verbose = verbose\n\n        self._stored_labels = None\n        self._label_map = None\n        self._version = MODEL_VERSION\n\n        self._num_channels = None\n        self._input_shape = None\n\n        self._custom_sampler = None\n        self._custom_diffusion = None\n        self._custom_schedule = None\n\n    @property\n    def device(self) -&gt; torch.device:\n        \"\"\"Device on which the model is running.\"\"\"\n        if self._model is not None:\n            return next(self._model.parameters()).device\n        return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    @property\n    def version(self) -&gt; int:\n        \"\"\"Version of the model.\"\"\"\n        return self._version\n\n    @property\n    def num_channels(self) -&gt; int:\n        \"\"\"Number of input channels (read-only).\"\"\"\n        return self._num_channels if self._num_channels is not None else 0\n\n    @property\n    def shape(self) -&gt; Tuple[int, int]:\n        \"\"\"Spatial dimensions of the input (height, width) (read-only).\"\"\"\n        return self._shape if self._shape is not None else (0, 0)\n\n    @property\n    def stored_labels(self) -&gt; Tuple[Any, ...]:\n        \"\"\"Numeric class labels from training data (read-only).\"\"\"\n        return tuple(self._stored_labels) if self._stored_labels is not None else ()\n\n    @property\n    def num_classes(self) -&gt; Optional[int]:\n        \"\"\"Number of classes (read-only). None if not class-conditional.\"\"\"\n        return self._num_classes\n\n    @property\n    def labels(self) -&gt; List[str]:\n        \"\"\"String labels for classes.\"\"\"\n        return self._label_map if self._label_map is not None else []\n\n    @property\n    def model(self) -&gt; Optional[ScoreNet]:\n        \"\"\"The underlying score model (read-only).\"\"\"\n        return self._model\n\n    @property\n    def verbose(self) -&gt; bool:\n        \"\"\"Whether to display progress bars during operations.\"\"\"\n        return self._verbose\n\n    @property\n    def noise_schedule(self) -&gt; BaseNoiseSchedule:\n        \"\"\"The noise schedule used by the diffusion process.\"\"\"\n        return self.diffusion.schedule if hasattr(self.diffusion, 'schedule') else None\n\n    @verbose.setter\n    def verbose(self, value: bool):\n        \"\"\"Sets the verbose flag for the model and sampler.\n\n        Args:\n            value: Whether to display progress bars.\n        \"\"\"\n        self._verbose = value\n        if hasattr(self.sampler, 'verbose'):\n            self.sampler.verbose = value\n\n    @property\n    def diffusion(self) -&gt; BaseDiffusion:\n        \"\"\"The diffusion process (read-only after training)\"\"\"\n        return self._diffusion\n\n    @diffusion.setter\n    def diffusion(self, value: Union[BaseDiffusion, type, str]):\n        \"\"\"Sets the diffusion process.\n\n        Args:\n            value: The diffusion process to use.\n\n        Raises:\n            ValueError: If the diffusion is not a subclass of BaseDiffusion.\n            TypeError: If the diffusion has an invalid type.\n        \"\"\"\n        if self._model is not None:\n            warnings.warn(\n                \"Diffusion cannot be changed after training\", UserWarning)\n            return\n\n        if isinstance(value, str):\n            value = self.DIFFUSION_MAP.get(value.lower(), VarianceExploding)\n\n        if isinstance(value, type):\n            if issubclass(value, BaseDiffusion):\n                if value.NEEDS_NOISE_SCHEDULE:\n                    ns = LinearNoiseSchedule()\n                    self._diffusion = value(ns)\n                else:\n                    self._diffusion = value()\n            else:\n                raise ValueError(\"Must subclass BaseDiffusion\")\n        elif isinstance(value, BaseDiffusion):\n            self._diffusion = value\n        else:\n            raise TypeError(\"Invalid diffusion type\")\n\n    @property\n    def sampler(self) -&gt; BaseSampler:\n        \"\"\"The sampling algorithm (always settable)\"\"\"\n        return self._sampler\n\n    @sampler.setter\n    def sampler(self, value: Union[BaseSampler, type, str]):\n        \"\"\"Sets the sampling algorithm.\n\n        Args:\n            value: The sampler to use.\n\n        Raises:\n            ValueError: If the sampler is not a subclass of BaseSampler.\n            TypeError: If the sampler has an invalid type.\n        \"\"\"\n        if isinstance(value, str):\n            value = self.SAMPLER_MAP.get(value.lower(), EulerMaruyama)\n\n        if isinstance(value, type):\n            if issubclass(value, BaseSampler):\n                self._sampler = value(self.diffusion, verbose=self.verbose)\n            else:\n                # Dashboard breaks without this line (wtf?)\n                value == issubclass(value, BaseSampler)\n                raise ValueError(\"Must subclass BaseSampler\")\n        elif isinstance(value, BaseSampler):\n            self._sampler = value\n            self._sampler.verbose = self.verbose\n        else:\n            raise TypeError(\"Invalid sampler type\")\n\n        self._sampler.verbose = self.verbose\n\n    def _progress(self, iterable: Iterable, **kwargs: Dict[str, Any]) -&gt; Iterable:\n        \"\"\"Wraps an iterable with a progress bar if verbose is enabled.\n\n        Args:\n            iterable: The iterable to wrap.\n            **kwargs: Additional arguments to pass to tqdm.\n\n        Returns:\n            The wrapped iterable, or the original if verbose is disabled.\n        \"\"\"\n        return tqdm(iterable, **kwargs) if self._verbose else iterable\n\n    def _build_default_model(self, shape: Tuple[int, int, int] = (3, 32, 32)):\n        \"\"\"Builds the default score model.\n\n        Args:\n            shape: The input shape (channels, height, width).\n        \"\"\"\n        device = self.device  # Creating the ScoreNet changes the device, so this line is necessary\n        self._num_channels = shape[0]\n        self._shape = (shape[1], shape[2])\n        self._model = ScoreNet(\n            marginal_prob_std=self.diffusion.schedule,\n            num_c=shape[0],\n            num_classes=self.num_classes\n        )\n        if self.device.type == \"cuda\":\n            self._model = torch.nn.DataParallel(self.model)\n        self._model = self.model.to(device)\n\n    def loss_function(self, x0: torch.Tensor, eps: float = 1e-5,\n                      class_labels: Optional[Tensor] = None) -&gt; torch.Tensor:\n        \"\"\"Computes the loss for training the score model.\n\n        Args:\n            x0: The input data.\n            eps: Small constant to avoid numerical issues.\n            class_labels: Class labels for conditional generation.\n\n        Returns:\n            The computed loss value.\n        \"\"\"\n        t = torch.rand(x0.shape[0], device=x0.device) * (1.0 - eps) + eps\n        xt, noise = self.diffusion.forward_process(x0, t)\n        score = self.model(xt, t, class_label=class_labels)\n        loss_per_example = self.diffusion.compute_loss(score, noise, t)\n        return torch.mean(loss_per_example)\n\n    def train(\n        self,\n        dataset: Union[\n            torch.utils.data.Dataset,\n            List[Union[Tensor, Tuple[Tensor, Tensor]]]\n        ],\n        epochs: int = 100,\n        batch_size: int = 32,\n        lr: float = 1e-3\n    ) -&gt; None:\n        \"\"\"Trains the score model.\n\n        Args:\n            dataset: The dataset to train on. Can be a torch Dataset or a list\n                of tensors or (tensor, label) tuples.\n            epochs: Number of training epochs.\n            batch_size: Batch size for training.\n            lr: Learning rate for the optimizer.\n        \"\"\"\n        first = dataset[0]\n\n        has_labels = isinstance(first, (list, tuple)) and len(first) &gt; 1\n        if has_labels:\n            all_labels = [\n                label if isinstance(label, Tensor) else torch.tensor(label)\n                for _, label in dataset\n            ]\n            all_labels_tensor = torch.cat([lbl.view(-1) for lbl in all_labels])\n            self._stored_labels = sorted(all_labels_tensor.unique().tolist())\n\n            # Create mapping from original labels to 0-based indices\n            self._label_to_index = {\n                lbl: idx for idx, lbl in enumerate(self.stored_labels)\n            }\n            self._num_classes = len(self.stored_labels)\n\n            # Map all labels to indices\n            self._mapped_labels = torch.tensor([\n                self._label_to_index[lbl.item()]\n                for lbl in all_labels_tensor\n            ])\n        else:\n            self._num_classes = None\n\n        first = first[0] if isinstance(first, (list, tuple)) else first\n        self._build_default_model(shape=first.shape)\n\n        optimizer = Adam(self.model.parameters(), lr=lr)\n        dataloader = torch.utils.data.DataLoader(\n            dataset, batch_size=batch_size, shuffle=True)\n\n        epoch_bar = self._progress(range(epochs), desc='Training')\n        for epoch in epoch_bar:\n            avg_loss = 0.0\n            num_items = 0\n\n            batch_bar = self._progress(\n                dataloader, desc=f'Epoch {epoch + 1}', leave=False)\n            for batch in batch_bar:\n                if has_labels:\n                    x0, original_labels = batch[0], batch[1]\n                    # Convert original labels to mapped indices\n                    labels = torch.tensor([\n                        self._label_to_index[lbl.item()]\n                        for lbl in original_labels\n                    ], device=self.device)\n                else:\n                    x0 = batch\n                    labels = None\n\n                x0 = x0.to(self.device)\n\n                optimizer.zero_grad()\n\n                if self.num_classes is not None:\n                    loss = self.loss_function(x0, class_labels=labels)\n                else:\n                    loss = self.loss_function(x0)\n\n                loss.backward()\n                optimizer.step()\n\n                avg_loss += loss.item() * x0.shape[0]\n                num_items += x0.shape[0]\n                # batch_bar.set_postfix(loss=loss.item())\n\n            # epoch_bar.set_postfix(avg_loss=avg_loss / num_items)\n\n    def set_labels(self, labels: List[str]) -&gt; None:\n        \"\"\"Sets string labels for the model's classes.\n\n        Args:\n            labels: List of string labels, one per class.\n\n        Raises:\n            ValueError: If the number of labels doesn't match the number of classes.\n        \"\"\"\n        # Check if model has class conditioning\n        if not hasattr(self, 'num_classes') or self.num_classes is None:\n            warnings.warn(\n                \"Model not initialized for class conditioning - labels will have no effect\")\n            return\n\n        # Check if we have stored numeric labels\n        if not hasattr(self, 'stored_labels') or self.stored_labels is None:\n            warnings.warn(\n                \"No class labels stored from training - cannot map string labels\")\n            return\n\n        # Validate input length\n        if len(labels) != len(self.stored_labels):\n            raise ValueError(\n                f\"Length mismatch: got {len(labels)} string labels, \"\n                f\"but model has {len(self.stored_labels)} classes. \"\n                f\"Current numeric labels: {self.stored_labels}\"\n            )\n\n        # Create new mapping\n        self._label_map = {\n            string_label: numeric_label\n            for numeric_label, string_label in zip(self.stored_labels, labels)\n        }\n\n    def score(self, real: Tensor, generated: Tensor,\n              metrics: List[Union[str, BaseMetric]] = [\"bpd\", \"fid\", \"is\"],\n              *args: Any, **kwargs: Any) -&gt; Dict[str, float]:\n        \"\"\"Evaluates the model using various metrics.\n\n        Args:\n            real: Real data samples.\n            generated: Generated data samples.\n            metrics: List of metrics to compute. Can be strings or BaseMetric instances.\n            *args: Additional arguments for metrics.\n            **kwargs: Additional keyword arguments for metrics.\n\n        Returns:\n            Dictionary mapping metric names to scores.\n\n        Raises:\n            Exception: If metrics is empty or not a list.\n        \"\"\"\n        if not isinstance(metrics, list) or len(metrics) == 0:\n            raise Exception(\n                \"Scores must be a non-empty list.\")\n\n        calculated_scores = {}\n        for score in metrics:\n            # Instantiate the class\n            if isinstance(score, str) and score.lower() in GenerativeModel.METRIC_MAP:\n                score = GenerativeModel.METRIC_MAP[score.lower()](self)\n            elif isinstance(score, type):\n                score = score(self)\n\n            if not isinstance(score, BaseMetric):\n                warnings.warn(f'\"{score}\" is not a metric, skipping...')\n                continue\n\n            if score.name in calculated_scores:\n                warnings.warn(\n                    f'A score with the name of \"{score.name}\" has already been calculated, but it will be overwritten.')\n            calculated_scores[score.name] = score(\n                real, generated, *args, **kwargs)\n\n        return calculated_scores\n\n    def _class_conditional_score(self, class_labels: Union[int, Tensor],\n                                 num_samples: int,\n                                 guidance_scale: float = 3.0) -&gt; Callable[[Tensor, Tensor], Tensor]:\n        \"\"\"Creates a class-conditional score function.\n\n        Args:\n            class_labels: Class labels for conditional generation.\n            num_samples: Number of samples to generate.\n            guidance_scale: Scale factor for classifier-free guidance.\n\n        Returns:\n            A function that computes the score for a given input and time.\n\n        Raises:\n            ValueError: If class_labels has an invalid type.\n        \"\"\"\n        if class_labels is None:\n            return self.model\n\n        processed_labels = None\n        if self.num_classes is None:\n            warnings.warn(\n                \"Ignoring class_labels - model not initialized for class conditioning\")\n            return self.model\n\n        # Convert to tensor and ensure proper type (torch.long)\n        if isinstance(class_labels, int):\n            class_labels = torch.full(\n                (num_samples,), class_labels, dtype=torch.long)\n        elif isinstance(class_labels, list):\n            class_labels = torch.tensor(class_labels, dtype=torch.long)\n        elif isinstance(class_labels, Tensor):\n            class_labels = class_labels.long()  # Convert to long if not already\n        else:\n            raise ValueError(\n                \"class_labels must be int, list or Tensor\")\n\n        class_labels = class_labels.to(self.device)\n\n        # Validate labels\n        if hasattr(self, 'stored_labels') and self.stored_labels is not None:\n            invalid_mask = ~torch.isin(class_labels, torch.tensor(\n                self.stored_labels, device=self.device))\n            if invalid_mask.any():\n                warnings.warn(\n                    f\"Invalid labels detected. Valid labels: {self.stored_labels}\")\n                # Replace invalid with first valid label\n                class_labels[invalid_mask] = self.stored_labels[0]\n\n        processed_labels = class_labels.to(self.device)\n\n        def guided_score(x: Tensor, t: Tensor) -&gt; Tensor:\n            \"\"\"Computes the guided score for classifier-free guidance.\n\n            Args:\n                x: The input tensor.\n                t: The time tensor.\n\n            Returns:\n                The guided score.\n            \"\"\"\n            uncond_score = self.model(x, t, class_label=None)\n\n            # Conditional score - ensure we pass proper labels\n            if processed_labels is not None:\n                # Ensure we have enough labels for the batch\n                if len(processed_labels) != x.shape[0]:\n                    # If single label provided, repeat it for batch\n                    if len(processed_labels) == 1:\n                        current_labels = processed_labels.expand(\n                            x.shape[0])\n                    else:\n                        raise ValueError(\n                            \"Number of labels must match batch size or be 1\")\n                else:\n                    current_labels = processed_labels\n\n                cond_score = self.model(x, t, class_label=current_labels)\n            else:\n                cond_score = uncond_score\n\n            return uncond_score + guidance_scale * (cond_score - uncond_score)\n\n        return guided_score\n\n    def generate(self,\n                 num_samples: int,\n                 n_steps: int = 500,\n                 seed: Optional[int] = None,\n                 class_labels: Optional[Union[int, Tensor]] = None,\n                 guidance_scale: float = 3.0,\n                 progress_callback: Optional[Callable[[\n                     Tensor, int], None]] = None,\n                 callback_frequency: int = 50\n                 ) -&gt; torch.Tensor:\n        \"\"\"Generates samples from the model.\n\n        Args:\n            num_samples: Number of samples to generate.\n            n_steps: Number of sampling steps.\n            seed: Random seed for reproducibility.\n            class_labels: Class labels for conditional generation.\n            guidance_scale: Scale factor for classifier-free guidance.\n            progress_callback: Function to call with intermediate results.\n            callback_frequency: How often to call the progress callback.\n\n        Returns:\n            The generated samples.\n\n        Raises:\n            ValueError: If the model is not initialized.\n        \"\"\"\n        if not hasattr(self, 'model') or self.model is None:\n            raise ValueError(\n                \"Model not initialized. Please load or train the model first.\")\n\n        score_func = self._class_conditional_score(\n            class_labels, num_samples, guidance_scale=guidance_scale)\n\n        x_T = torch.randn(num_samples, self.num_channels, *\n                          self.shape, device=self.device)\n\n        self.model.eval()\n        with torch.no_grad():\n            samples = self.sampler(\n                x_T=x_T,\n                score_model=score_func,\n                n_steps=n_steps,\n                seed=seed,\n                callback=progress_callback,\n                callback_frequency=callback_frequency\n            )\n\n        self.model.train()\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\n        return samples\n\n    def colorize(self, x: Tensor, n_steps: int = 500,\n                 seed: Optional[int] = None,\n                 class_labels: Optional[Union[int, Tensor]] = None,\n                 progress_callback: Optional[Callable[[Tensor, int], None]] = None) -&gt; Tensor:\n        \"\"\"Colorizes grayscale images using YUV-space luminance enforcement.\n\n        Args:\n            x: Grayscale input image(s).\n            n_steps: Number of sampling steps.\n            seed: Random seed for reproducibility.\n            class_labels: Class labels for conditional generation.\n            progress_callback: Function to call with intermediate results.\n\n        Returns:\n            The colorized images.\n\n        Raises:\n            ValueError: If the model doesn't have 3 channels or the input has invalid shape.\n        \"\"\"\n        if not hasattr(self, 'num_channels') or self.num_channels != 3:\n            raise ValueError(\"Colorization requires a 3-channel model\")\n\n        if x.dim() == 3:\n            x = x.unsqueeze(0)  # Add batch dimension\n        if x.shape[1] == 3:\n            y_target = self._rgb_to_grayscale(x)\n        elif x.shape[1] == 1:\n            y_target = x\n        else:\n            raise ValueError(\"Input must be 1 or 3 channels\")\n\n        y_target = (y_target - y_target.min()) / \\\n            (y_target.max() - y_target.min() + 1e-8)\n\n        y_target = y_target.to(self.device).float()\n        batch_size, _, h, w = y_target.shape\n\n        with torch.no_grad():\n            uv = torch.rand(batch_size, 2, h, w, device=self.device) * \\\n                0.5 - 0.25\n            yuv = torch.cat([y_target, uv], dim=1)\n            x_init = self._yuv_to_rgb(yuv)\n\n            t_T = torch.ones(batch_size, device=self.device)\n            x_T, _ = self.diffusion.forward_process(x_init, t_T)\n\n        def enforce_luminance(x_t: Tensor, t: Tensor) -&gt; Tensor:\n            \"\"\"Enforces Y channel while preserving UV color information.\n\n            Args:\n                x_t: Current RGB image.\n                t: Current time step.\n\n            Returns:\n                Modified RGB image with enforced Y channel.\n            \"\"\"\n            with torch.no_grad():\n                yuv = self._rgb_to_yuv(x_t)\n                yuv[:, 0:1] = y_target\n                enforced_rgb = self._yuv_to_rgb(yuv)\n                alpha = t.item() / n_steps\n                return enforced_rgb * (1 - alpha) + x_t * alpha\n\n        score_func = self._class_conditional_score(class_labels, x.shape[0])\n\n        self.model.eval()\n        with torch.no_grad():\n            samples = self.sampler(\n                x_T=x_T,\n                score_model=score_func,\n                n_steps=n_steps,\n                guidance=enforce_luminance,\n                callback=progress_callback,\n                seed=seed\n            )\n\n        self.model.train()\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\n        return samples\n\n    @staticmethod\n    def _rgb_to_grayscale(img: Tensor) -&gt; Tensor:\n        \"\"\"Convert RGB image tensor to grayscale.\n\n        Args:\n            img: Input tensor (B, 3, H, W) in range [0,1] or [-1,1]\n\n        Returns:\n            Grayscale tensor (B, 1, H, W)\n        \"\"\"\n        if img.min() &lt; 0:  # If in [-1,1] range, normalize to [0,1]\n            img = (img + 1) / 2\n\n        # Use standard RGB to grayscale conversion weights\n        gray = 0.2989 * img[:, 0] + 0.5870 * img[:, 1] + 0.1140 * img[:, 2]\n        return gray.unsqueeze(1)  # Add channel dimension\n\n    @staticmethod\n    def _rgb_to_yuv(img: Tensor) -&gt; Tensor:\n        \"\"\"Converts RGB tensor (B,3,H,W) to YUV (B,3,H,W).\n\n        Args:\n            img: RGB image tensor.\n\n        Returns:\n            YUV image tensor.\n        \"\"\"\n        r, g, b = img.chunk(3, dim=1)\n        y = 0.299 * r + 0.587 * g + 0.114 * b\n        u = 0.492 * (b - y) + 0.5\n        v = 0.877 * (r - y) + 0.5\n        return torch.cat([y, u, v], dim=1)\n\n    @staticmethod\n    def _yuv_to_rgb(yuv: Tensor) -&gt; Tensor:\n        \"\"\"Converts YUV tensor (B,3,H,W) to RGB (B,3,H,W).\n\n        Args:\n            yuv: YUV image tensor.\n\n        Returns:\n            RGB image tensor.\n        \"\"\"\n        y, u, v = yuv.chunk(3, dim=1)\n        u = (u - 0.5) / 0.492\n        v = (v - 0.5) / 0.877\n\n        r = y + v\n        b = y + u\n        g = (y - 0.299 * r - 0.114 * b) / 0.587\n        return torch.clamp(torch.cat([r, g, b], dim=1), 0.0, 1.0)\n\n    def imputation(self, x: Tensor, mask: Tensor, n_steps: int = 500,\n                   seed: Optional[int] = None,\n                   class_labels: Optional[Union[int, Tensor]] = None,\n                   progress_callback: Optional[Callable[[Tensor, int], None]] = None) -&gt; Tensor:\n        \"\"\"Performs image inpainting with mask-guided generation.\n\n        Args:\n            x: Input image(s) with missing regions.\n            mask: Binary mask where 1 indicates pixels to generate (missing regions).\n            n_steps: Number of sampling steps.\n            seed: Random seed for reproducibility.\n            class_labels: Class labels for conditional generation.\n            progress_callback: Function to call with intermediate results.\n\n        Returns:\n            Inpainted image(s).\n\n        Raises:\n            ValueError: If image and mask dimensions don't match.\n        \"\"\"\n        if x.shape[-2:] != mask.shape[-2:]:\n            raise ValueError(\n                \"Image and mask must have same spatial dimensions\")\n        if mask.shape[1] != 1:\n            raise ValueError(\"Mask must be single-channel\")\n\n        batch_size, original_channels, _, _ = x.shape\n\n        input_min = x.min()\n        input_max = x.max()\n\n        x_normalized = (x - input_min) / (input_max - input_min + 1e-8) * 2 - 1\n        x_normalized = x_normalized.to(self.device)\n\n        # Convert to grayscale if model expects 1 channel but input has more\n        if self.num_channels == 1 and original_channels != 1:\n            x_normalized = x_normalized.mean(dim=1, keepdim=True)\n\n        generate_mask = mask.to(self.device).bool()\n        generate_mask = generate_mask.expand(-1,\n                                             1, -1, -1).to(self.device)\n        preserve_mask = ~generate_mask\n\n        with torch.no_grad():\n            x_init = x_normalized.clone().to(self.device)\n            noise = torch.randn_like(x_normalized).to(self.device)\n            x_T = torch.where(generate_mask, noise, x_init)\n            t_T = torch.ones(batch_size, device=self.device)\n            x_T, _ = self.diffusion.forward_process(x_T, t_T)\n\n        def inpaint_guidance(x_t: Tensor, t: Tensor) -&gt; Tensor:\n            \"\"\"Preserves known pixels in the image during sampling.\"\"\"\n            with torch.no_grad():\n                return torch.where(preserve_mask, x_normalized, x_t)\n\n        score_func = self._class_conditional_score(class_labels, batch_size)\n\n        self.model.eval()\n        with torch.no_grad():\n            samples_normalized = self.sampler(\n                x_T=x_T,\n                score_model=score_func,\n                n_steps=n_steps,\n                guidance=inpaint_guidance,\n                callback=progress_callback,\n                seed=seed\n            )\n\n        combined_normalized = torch.where(\n            generate_mask, samples_normalized, x_normalized)\n\n        result = (combined_normalized + 1) / 2 * \\\n            (input_max - input_min) + input_min\n\n        self.model.train()\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\n        return result\n\n    def save(self, path: str) -&gt; None:\n        \"\"\"Saves the model to the specified path.\n\n        Args:\n            path: Path where to save the model.\n        \"\"\"\n        save_data = {\n            'model_state': self.model.state_dict(),\n            'shape': self.shape,\n            'diffusion_type': self.diffusion.__class__.__name__.lower(),\n            'sampler_type': self.sampler.__class__.__name__.lower(),\n            'num_channels': self.num_channels,\n            'stored_labels': self.stored_labels,\n            'label_map': self._label_map,\n            'model_version': MODEL_VERSION,\n        }\n\n        if hasattr(self.diffusion, 'config'):\n            save_data['diffusion_config'] = self.diffusion.config()\n        if save_data[\"diffusion_type\"] not in GenerativeModel.DIFFUSION_MAP:\n            save_data[\"diffusion_code\"] = get_class_source(\n                self.diffusion.__class__)\n\n        if self.diffusion.NEEDS_NOISE_SCHEDULE:\n            save_data['noise_schedule_type'] = self.diffusion.schedule.__class__.__name__.lower()\n            if hasattr(self.diffusion.schedule, 'config'):\n                save_data['noise_schedule_config'] = self.diffusion.schedule.config()\n            if save_data[\"noise_schedule_type\"] not in GenerativeModel.NOISE_SCHEDULE_MAP:\n                save_data[\"noise_schedule_code\"] = get_class_source(\n                    self.noise_schedule.__class__)\n\n        if hasattr(self.sampler, 'config'):\n            save_data['sampler_config'] = self.sampler.config()\n        if save_data[\"sampler_type\"] not in GenerativeModel.SAMPLER_MAP:\n            save_data[\"sampler_code\"] = get_class_source(\n                self.sampler.__class__)\n\n        torch.save(save_data, path)\n\n    def _rebuild_diffusion(self, checkpoint: Dict[str, Any], unsafe: bool = False):\n        \"\"\"Rebuilds the diffusion process from a checkpoint.\n\n        Args:\n            checkpoint: The checkpoint data.\n            unsafe: Whether to allow loading custom code.\n\n        Raises:\n            Exception: If the checkpoint contains custom code and unsafe is False.\n        \"\"\"\n        default_diffusion = VarianceExploding.__name__.lower()\n        diffusion_type = checkpoint.get(\"diffusion_type\", default_diffusion)\n        diffusion_cls = GenerativeModel.DIFFUSION_MAP.get(diffusion_type)\n\n        if diffusion_cls is None:\n            diffusion_code = checkpoint.get(\"diffusion_code\")\n            if diffusion_type != default_diffusion and diffusion_code is not None:\n                if unsafe:\n                    self._custom_diffusion = diffusion_code\n                    diffusion_cls = lambda *args, **kwargs: CustomClassWrapper(\n                        diffusion_code, *args, **kwargs)\n                    warnings.warn(\n                        \"This model has been instantiated with a custom diffuser. \"\n                        \"Please verify the safety of the code before calling any methods \"\n                        \"of the GenerativeModel. It can be viewed with \"\n                        \"GenerativeModel.show_custom_code(), and won't be run until needed.\")\n                else:\n                    raise Exception(\n                        \"The saved model uses a custom diffuser, which is not allowed for \"\n                        \"safety reasons. If you want to load the custom class, use \"\n                        \"model.load(path, override=True, unsafe=True).\")\n\n        schedule = self._rebuild_noise_schedule(checkpoint, unsafe=unsafe)\n        config = checkpoint.get('diffusion_config', {})\n        self._diffusion = diffusion_cls(schedule, **config)\n\n    def _rebuild_noise_schedule(self, checkpoint: Dict[str, Any], unsafe: bool = False) -&gt; BaseNoiseSchedule:\n        \"\"\"Rebuilds the noise schedule from a checkpoint.\n\n        Args:\n            checkpoint: The checkpoint data.\n            unsafe: Whether to allow loading custom code.\n\n        Returns:\n            The rebuilt noise schedule.\n\n        Raises:\n            Exception: If the checkpoint contains custom code and unsafe is False.\n        \"\"\"\n        default_schedule = LinearNoiseSchedule.__name__.lower()\n        schedule_type = checkpoint.get(\"noise_schedule_type\", default_schedule)\n        schedule_cls = GenerativeModel.NOISE_SCHEDULE_MAP.get(schedule_type)\n\n        if schedule_cls is None:\n            schedule_code = checkpoint.get(\"noise_schedule_code\")\n            if schedule_type != default_schedule and schedule_code is not None:\n                if unsafe:\n                    self._custom_schedule = schedule_code\n                    schedule_cls = lambda *args, **kwargs: CustomClassWrapper(\n                        schedule_code, *args, **kwargs)\n                    warnings.warn(\n                        \"This model has been instantiated with a custom schedule. \"\n                        \"Please verify the safety of the code before calling any methods \"\n                        \"of the GenerativeModel. It can be viewed with \"\n                        \"GenerativeModel.show_custom_code(), and won't be run until needed.\")\n                else:\n                    raise Exception(\n                        \"The saved model uses a custom schedule, which is not allowed for \"\n                        \"safety reasons. If you want to load the custom class, use \"\n                        \"model.load(path, override=True, unsafe=True).\")\n\n        config = checkpoint.get('noise_schedule_config', {})\n        return schedule_cls(**config)\n\n    def _rebuild_sampler(self, checkpoint: Dict[str, Any], unsafe: bool = False):\n        \"\"\"Rebuilds the sampler from a checkpoint.\n\n        Args:\n            checkpoint: The checkpoint data.\n            unsafe: Whether to allow loading custom code.\n\n        Raises:\n            Exception: If the checkpoint contains custom code and unsafe is False.\n        \"\"\"\n        default_sampler = EulerMaruyama.__name__.lower()\n        sampler_type = checkpoint.get(\"sampler_type\", default_sampler)\n        sampler_cls = GenerativeModel.SAMPLER_MAP.get(sampler_type)\n\n        if sampler_cls is None:\n            sampler_code = checkpoint.get(\"sampler_code\")\n            if sampler_type != default_sampler and sampler_code is not None:\n                if unsafe:\n                    self._custom_sampler = sampler_code\n                    sampler_cls = lambda *args, **kwargs: CustomClassWrapper(\n                        sampler_code, *args, **kwargs)\n                    warnings.warn(\n                        \"This model has been instantiated with a custom sampler. \"\n                        \"Please verify the safety of the code before calling any methods \"\n                        \"of the GenerativeModel. It can be viewed with \"\n                        \"GenerativeModel.show_custom_code(), and won't be run until needed.\")\n                else:\n                    raise Exception(\n                        \"The saved model uses a custom sampler, which is not allowed for \"\n                        \"safety reasons. If you want to load the custom class, use \"\n                        \"model.load(path, override=True, unsafe=True).\")\n\n        if self._sampler.__class__ != sampler_cls:\n            warnings.warn(\n                f\"The model was initialized with sampler {self._sampler.__class__.__name__}, \"\n                f\"but the saved model has {sampler_cls.__name__}. The sampler will be set to \"\n                f\"{sampler_cls.__name__}. If you don't want this behaviour, use \"\n                f\"model.load(path, override=False).\"\n            )\n        config = checkpoint.get('sampler_config', {})\n        self._sampler = sampler_cls(\n            self.diffusion, **config, verbose=self._verbose)\n\n    def get_custom_code(self) -&gt; dict:\n        \"\"\"Returns any custom code components used by the model.\n\n        Returns:\n            Dictionary mapping component names to their source code.\n        \"\"\"\n        custom_components = {}\n\n        if self._custom_diffusion is not None:\n            custom_components[\"diffusion\"] = self._custom_diffusion\n        if self._custom_schedule is not None:\n            custom_components[\"noise_schedule\"] = self._custom_schedule\n        if self._custom_sampler is not None:\n            custom_components[\"sampler\"] = self._custom_sampler\n\n        return custom_components\n\n    def load(self, path: str, override: bool = True, unsafe: bool = False) -&gt; None:\n        \"\"\"Loads a saved model from the specified path.\n\n        Args:\n            path: Path to the saved model file.\n            override: If True, overwrites the current sampler with the saved one.\n            unsafe: If True, allows loading custom code components (potentially unsafe).\n\n        Raises:\n            RuntimeError: If the model state dictionary cannot be loaded properly.\n        \"\"\"\n        self._model = None\n\n        checkpoint = torch.load(path)\n        self._version = checkpoint.get('model_version')\n\n        self._custom_sampler = None\n        self._custom_diffusion = None\n        self._custom_schedule = None\n        self._rebuild_diffusion(checkpoint, unsafe=unsafe)\n        if override:\n            self._rebuild_sampler(checkpoint, unsafe=unsafe)\n\n        self._stored_labels = checkpoint.get('stored_labels')\n        self._num_classes = (\n            len(self.stored_labels) if self.stored_labels is not None else None\n        )\n        self._label_map = checkpoint.get('label_map')\n\n        # Default to grayscale if channels not specified\n        checkpoint_channels = checkpoint.get('num_channels', 1)\n        self._shape = checkpoint.get('shape', (32, 32))\n\n        self._build_default_model(shape=(checkpoint_channels, *self._shape))\n\n        try:\n            # Load only keys that exist in both models\n            model_dict = self.model.state_dict()\n            # Filter checkpoint keys that exist in the current model\n            pretrained_dict = {\n                k: v\n                for k, v in checkpoint['model_state'].items()\n                if k in model_dict\n            }\n            model_dict.update(pretrained_dict)\n            self.model.load_state_dict(model_dict, strict=False)\n        except RuntimeError as original_error:\n            try:\n                # Try with keys without \"module.\" prefix (happens with DataParallel)\n                new_state_dict = {\n                    k.replace('module.', ''): v\n                    for k, v in checkpoint['model_state'].items()\n                }\n                model_dict = self.model.state_dict()\n                pretrained_dict = {\n                    k: v\n                    for k, v in new_state_dict.items()\n                    if k in model_dict\n                }\n                model_dict.update(pretrained_dict)\n                self.model.load_state_dict(model_dict, strict=False)\n            except RuntimeError as secondary_error:\n                # Log both errors for better debugging\n                error_msg = (\n                    f\"Failed to load model state. Original error: {original_error}. \"\n                    f\"Secondary error: {secondary_error}\"\n                )\n                print(f\"Warning: {error_msg}\")\n</code></pre>"},{"location":"api/generative_model/#image_gen.base.GenerativeModel.device","title":"<code>device</code>  <code>property</code>","text":"<p>Device on which the model is running.</p>"},{"location":"api/generative_model/#image_gen.base.GenerativeModel.diffusion","title":"<code>diffusion</code>  <code>property</code> <code>writable</code>","text":"<p>The diffusion process (read-only after training)</p>"},{"location":"api/generative_model/#image_gen.base.GenerativeModel.labels","title":"<code>labels</code>  <code>property</code>","text":"<p>String labels for classes.</p>"},{"location":"api/generative_model/#image_gen.base.GenerativeModel.model","title":"<code>model</code>  <code>property</code>","text":"<p>The underlying score model (read-only).</p>"},{"location":"api/generative_model/#image_gen.base.GenerativeModel.noise_schedule","title":"<code>noise_schedule</code>  <code>property</code>","text":"<p>The noise schedule used by the diffusion process.</p>"},{"location":"api/generative_model/#image_gen.base.GenerativeModel.num_channels","title":"<code>num_channels</code>  <code>property</code>","text":"<p>Number of input channels (read-only).</p>"},{"location":"api/generative_model/#image_gen.base.GenerativeModel.num_classes","title":"<code>num_classes</code>  <code>property</code>","text":"<p>Number of classes (read-only). None if not class-conditional.</p>"},{"location":"api/generative_model/#image_gen.base.GenerativeModel.sampler","title":"<code>sampler</code>  <code>property</code> <code>writable</code>","text":"<p>The sampling algorithm (always settable)</p>"},{"location":"api/generative_model/#image_gen.base.GenerativeModel.shape","title":"<code>shape</code>  <code>property</code>","text":"<p>Spatial dimensions of the input (height, width) (read-only).</p>"},{"location":"api/generative_model/#image_gen.base.GenerativeModel.stored_labels","title":"<code>stored_labels</code>  <code>property</code>","text":"<p>Numeric class labels from training data (read-only).</p>"},{"location":"api/generative_model/#image_gen.base.GenerativeModel.verbose","title":"<code>verbose</code>  <code>property</code> <code>writable</code>","text":"<p>Whether to display progress bars during operations.</p>"},{"location":"api/generative_model/#image_gen.base.GenerativeModel.version","title":"<code>version</code>  <code>property</code>","text":"<p>Version of the model.</p>"},{"location":"api/generative_model/#image_gen.base.GenerativeModel.__init__","title":"<code>__init__(diffusion='ve', sampler='euler-maruyama', noise_schedule=None, verbose=True)</code>","text":"<p>Initializes the generative model.</p> <p>Parameters:</p> Name Type Description Default <code>diffusion</code> <code>Optional[Union[BaseDiffusion, type, Literal['ve', 'vp', 'sub-vp', 'svp']]]</code> <p>The diffusion process to use. Can be a string identifier, a diffusion class, or a diffusion instance.</p> <code>'ve'</code> <code>sampler</code> <code>Optional[Union[BaseSampler, type, Literal['euler-maruyama', 'euler', 'em', 'exponential', 'exp', 'ode', 'predictor-corrector', 'pred']]]</code> <p>The sampling algorithm to use. Can be a string identifier, a sampler class, or a sampler instance.</p> <code>'euler-maruyama'</code> <code>noise_schedule</code> <code>Optional[Union[BaseNoiseSchedule, type, Literal['linear', 'lin', 'cosine', 'cos']]]</code> <p>The noise schedule to use. Only required for diffusion processes that need a noise schedule.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Whether to display progress bars during generation and training.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unknown diffusion or sampler string is provided.</p> <code>TypeError</code> <p>If the diffusion or sampler has an invalid type.</p> Source code in <code>image_gen\\base.py</code> <pre><code>def __init__(\n    self,\n    diffusion: Optional[Union[BaseDiffusion, type,\n                              Literal[\"ve\", \"vp\", \"sub-vp\", \"svp\"]]] = \"ve\",\n    sampler: Optional[Union[BaseSampler, type,\n                            Literal[\"euler-maruyama\", \"euler\", \"em\",\n                                    \"exponential\", \"exp\", \"ode\",\n                                    \"predictor-corrector\", \"pred\"]]] = \"euler-maruyama\",\n    noise_schedule: Optional[Union[BaseNoiseSchedule, type,\n                                   Literal[\"linear\", \"lin\", \"cosine\", \"cos\"]]] = None,\n    verbose: bool = True\n) -&gt; None:\n    \"\"\"Initializes the generative model.\n\n    Args:\n        diffusion: The diffusion process to use. Can be a string identifier,\n            a diffusion class, or a diffusion instance.\n        sampler: The sampling algorithm to use. Can be a string identifier,\n            a sampler class, or a sampler instance.\n        noise_schedule: The noise schedule to use. Only required for diffusion\n            processes that need a noise schedule.\n        verbose: Whether to display progress bars during generation and training.\n\n    Raises:\n        ValueError: If an unknown diffusion or sampler string is provided.\n        TypeError: If the diffusion or sampler has an invalid type.\n    \"\"\"\n    self._model = None\n    self._verbose = verbose\n    self._num_classes = None  # Initialize this attribute\n    self._stored_labels = None\n    self._label_map = None\n    self._version = MODEL_VERSION\n    self._num_channels = None\n    self._shape = None  # Changed from _input_shape to _shape\n\n    if diffusion is None:\n        diffusion = \"ve\"\n\n    if isinstance(diffusion, str):\n        diffusion_key = diffusion.lower()\n        try:\n            diffusion = GenerativeModel.DIFFUSION_MAP[diffusion_key]\n        except KeyError:\n            raise ValueError(f\"Unknown diffusion string: {diffusion}\")\n\n    if sampler is None:\n        sampler = \"euler-maruyama\"\n\n    if isinstance(sampler, str):\n        sampler_key = sampler.lower()\n        try:\n            sampler = GenerativeModel.SAMPLER_MAP[sampler_key]\n        except KeyError:\n            raise ValueError(f\"Unknown sampler string: {sampler}\")\n\n    if noise_schedule is None and ((isinstance(diffusion, type) or\n                                    isinstance(diffusion, BaseDiffusion)) and\n                                   diffusion.NEEDS_NOISE_SCHEDULE):\n        noise_schedule = \"linear\"\n\n    if isinstance(noise_schedule, str):\n        ns_key = noise_schedule.lower()\n        try:\n            noise_schedule = GenerativeModel.NOISE_SCHEDULE_MAP[ns_key]\n        except KeyError:\n            raise ValueError(\n                f\"Unknown noise_schedule string: {noise_schedule}\")\n\n    if isinstance(diffusion, type):\n        if diffusion.NEEDS_NOISE_SCHEDULE:\n            if isinstance(noise_schedule, type):\n                ns_inst = noise_schedule()\n            else:\n                ns_inst = noise_schedule\n            self.diffusion = diffusion(ns_inst)\n        else:\n            if noise_schedule is not None:\n                warnings.warn(\n                    f\"{diffusion.__name__} does not require a noise schedule. \"\n                    f\"The provided noise schedule will be ignored.\",\n                    UserWarning\n                )\n            self.diffusion = diffusion()\n    else:\n        if not diffusion.NEEDS_NOISE_SCHEDULE and noise_schedule is not None:\n            warnings.warn(\n                f\"{diffusion.__class__.__name__} does not require a noise schedule. \"\n                f\"The provided noise schedule will be ignored.\",\n                UserWarning\n            )\n        self.diffusion = diffusion\n\n    if isinstance(sampler, type):\n        self.sampler = sampler(self.diffusion)\n    else:\n        self.sampler = sampler\n    self.sampler.verbose = verbose\n\n    self._stored_labels = None\n    self._label_map = None\n    self._version = MODEL_VERSION\n\n    self._num_channels = None\n    self._input_shape = None\n\n    self._custom_sampler = None\n    self._custom_diffusion = None\n    self._custom_schedule = None\n</code></pre>"},{"location":"api/generative_model/#image_gen.base.GenerativeModel.colorize","title":"<code>colorize(x, n_steps=500, seed=None, class_labels=None, progress_callback=None)</code>","text":"<p>Colorizes grayscale images using YUV-space luminance enforcement.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Grayscale input image(s).</p> required <code>n_steps</code> <code>int</code> <p>Number of sampling steps.</p> <code>500</code> <code>seed</code> <code>Optional[int]</code> <p>Random seed for reproducibility.</p> <code>None</code> <code>class_labels</code> <code>Optional[Union[int, Tensor]]</code> <p>Class labels for conditional generation.</p> <code>None</code> <code>progress_callback</code> <code>Optional[Callable[[Tensor, int], None]]</code> <p>Function to call with intermediate results.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The colorized images.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model doesn't have 3 channels or the input has invalid shape.</p> Source code in <code>image_gen\\base.py</code> <pre><code>def colorize(self, x: Tensor, n_steps: int = 500,\n             seed: Optional[int] = None,\n             class_labels: Optional[Union[int, Tensor]] = None,\n             progress_callback: Optional[Callable[[Tensor, int], None]] = None) -&gt; Tensor:\n    \"\"\"Colorizes grayscale images using YUV-space luminance enforcement.\n\n    Args:\n        x: Grayscale input image(s).\n        n_steps: Number of sampling steps.\n        seed: Random seed for reproducibility.\n        class_labels: Class labels for conditional generation.\n        progress_callback: Function to call with intermediate results.\n\n    Returns:\n        The colorized images.\n\n    Raises:\n        ValueError: If the model doesn't have 3 channels or the input has invalid shape.\n    \"\"\"\n    if not hasattr(self, 'num_channels') or self.num_channels != 3:\n        raise ValueError(\"Colorization requires a 3-channel model\")\n\n    if x.dim() == 3:\n        x = x.unsqueeze(0)  # Add batch dimension\n    if x.shape[1] == 3:\n        y_target = self._rgb_to_grayscale(x)\n    elif x.shape[1] == 1:\n        y_target = x\n    else:\n        raise ValueError(\"Input must be 1 or 3 channels\")\n\n    y_target = (y_target - y_target.min()) / \\\n        (y_target.max() - y_target.min() + 1e-8)\n\n    y_target = y_target.to(self.device).float()\n    batch_size, _, h, w = y_target.shape\n\n    with torch.no_grad():\n        uv = torch.rand(batch_size, 2, h, w, device=self.device) * \\\n            0.5 - 0.25\n        yuv = torch.cat([y_target, uv], dim=1)\n        x_init = self._yuv_to_rgb(yuv)\n\n        t_T = torch.ones(batch_size, device=self.device)\n        x_T, _ = self.diffusion.forward_process(x_init, t_T)\n\n    def enforce_luminance(x_t: Tensor, t: Tensor) -&gt; Tensor:\n        \"\"\"Enforces Y channel while preserving UV color information.\n\n        Args:\n            x_t: Current RGB image.\n            t: Current time step.\n\n        Returns:\n            Modified RGB image with enforced Y channel.\n        \"\"\"\n        with torch.no_grad():\n            yuv = self._rgb_to_yuv(x_t)\n            yuv[:, 0:1] = y_target\n            enforced_rgb = self._yuv_to_rgb(yuv)\n            alpha = t.item() / n_steps\n            return enforced_rgb * (1 - alpha) + x_t * alpha\n\n    score_func = self._class_conditional_score(class_labels, x.shape[0])\n\n    self.model.eval()\n    with torch.no_grad():\n        samples = self.sampler(\n            x_T=x_T,\n            score_model=score_func,\n            n_steps=n_steps,\n            guidance=enforce_luminance,\n            callback=progress_callback,\n            seed=seed\n        )\n\n    self.model.train()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\n    return samples\n</code></pre>"},{"location":"api/generative_model/#image_gen.base.GenerativeModel.generate","title":"<code>generate(num_samples, n_steps=500, seed=None, class_labels=None, guidance_scale=3.0, progress_callback=None, callback_frequency=50)</code>","text":"<p>Generates samples from the model.</p> <p>Parameters:</p> Name Type Description Default <code>num_samples</code> <code>int</code> <p>Number of samples to generate.</p> required <code>n_steps</code> <code>int</code> <p>Number of sampling steps.</p> <code>500</code> <code>seed</code> <code>Optional[int]</code> <p>Random seed for reproducibility.</p> <code>None</code> <code>class_labels</code> <code>Optional[Union[int, Tensor]]</code> <p>Class labels for conditional generation.</p> <code>None</code> <code>guidance_scale</code> <code>float</code> <p>Scale factor for classifier-free guidance.</p> <code>3.0</code> <code>progress_callback</code> <code>Optional[Callable[[Tensor, int], None]]</code> <p>Function to call with intermediate results.</p> <code>None</code> <code>callback_frequency</code> <code>int</code> <p>How often to call the progress callback.</p> <code>50</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The generated samples.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model is not initialized.</p> Source code in <code>image_gen\\base.py</code> <pre><code>def generate(self,\n             num_samples: int,\n             n_steps: int = 500,\n             seed: Optional[int] = None,\n             class_labels: Optional[Union[int, Tensor]] = None,\n             guidance_scale: float = 3.0,\n             progress_callback: Optional[Callable[[\n                 Tensor, int], None]] = None,\n             callback_frequency: int = 50\n             ) -&gt; torch.Tensor:\n    \"\"\"Generates samples from the model.\n\n    Args:\n        num_samples: Number of samples to generate.\n        n_steps: Number of sampling steps.\n        seed: Random seed for reproducibility.\n        class_labels: Class labels for conditional generation.\n        guidance_scale: Scale factor for classifier-free guidance.\n        progress_callback: Function to call with intermediate results.\n        callback_frequency: How often to call the progress callback.\n\n    Returns:\n        The generated samples.\n\n    Raises:\n        ValueError: If the model is not initialized.\n    \"\"\"\n    if not hasattr(self, 'model') or self.model is None:\n        raise ValueError(\n            \"Model not initialized. Please load or train the model first.\")\n\n    score_func = self._class_conditional_score(\n        class_labels, num_samples, guidance_scale=guidance_scale)\n\n    x_T = torch.randn(num_samples, self.num_channels, *\n                      self.shape, device=self.device)\n\n    self.model.eval()\n    with torch.no_grad():\n        samples = self.sampler(\n            x_T=x_T,\n            score_model=score_func,\n            n_steps=n_steps,\n            seed=seed,\n            callback=progress_callback,\n            callback_frequency=callback_frequency\n        )\n\n    self.model.train()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\n    return samples\n</code></pre>"},{"location":"api/generative_model/#image_gen.base.GenerativeModel.get_custom_code","title":"<code>get_custom_code()</code>","text":"<p>Returns any custom code components used by the model.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary mapping component names to their source code.</p> Source code in <code>image_gen\\base.py</code> <pre><code>def get_custom_code(self) -&gt; dict:\n    \"\"\"Returns any custom code components used by the model.\n\n    Returns:\n        Dictionary mapping component names to their source code.\n    \"\"\"\n    custom_components = {}\n\n    if self._custom_diffusion is not None:\n        custom_components[\"diffusion\"] = self._custom_diffusion\n    if self._custom_schedule is not None:\n        custom_components[\"noise_schedule\"] = self._custom_schedule\n    if self._custom_sampler is not None:\n        custom_components[\"sampler\"] = self._custom_sampler\n\n    return custom_components\n</code></pre>"},{"location":"api/generative_model/#image_gen.base.GenerativeModel.imputation","title":"<code>imputation(x, mask, n_steps=500, seed=None, class_labels=None, progress_callback=None)</code>","text":"<p>Performs image inpainting with mask-guided generation.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input image(s) with missing regions.</p> required <code>mask</code> <code>Tensor</code> <p>Binary mask where 1 indicates pixels to generate (missing regions).</p> required <code>n_steps</code> <code>int</code> <p>Number of sampling steps.</p> <code>500</code> <code>seed</code> <code>Optional[int]</code> <p>Random seed for reproducibility.</p> <code>None</code> <code>class_labels</code> <code>Optional[Union[int, Tensor]]</code> <p>Class labels for conditional generation.</p> <code>None</code> <code>progress_callback</code> <code>Optional[Callable[[Tensor, int], None]]</code> <p>Function to call with intermediate results.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Inpainted image(s).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If image and mask dimensions don't match.</p> Source code in <code>image_gen\\base.py</code> <pre><code>def imputation(self, x: Tensor, mask: Tensor, n_steps: int = 500,\n               seed: Optional[int] = None,\n               class_labels: Optional[Union[int, Tensor]] = None,\n               progress_callback: Optional[Callable[[Tensor, int], None]] = None) -&gt; Tensor:\n    \"\"\"Performs image inpainting with mask-guided generation.\n\n    Args:\n        x: Input image(s) with missing regions.\n        mask: Binary mask where 1 indicates pixels to generate (missing regions).\n        n_steps: Number of sampling steps.\n        seed: Random seed for reproducibility.\n        class_labels: Class labels for conditional generation.\n        progress_callback: Function to call with intermediate results.\n\n    Returns:\n        Inpainted image(s).\n\n    Raises:\n        ValueError: If image and mask dimensions don't match.\n    \"\"\"\n    if x.shape[-2:] != mask.shape[-2:]:\n        raise ValueError(\n            \"Image and mask must have same spatial dimensions\")\n    if mask.shape[1] != 1:\n        raise ValueError(\"Mask must be single-channel\")\n\n    batch_size, original_channels, _, _ = x.shape\n\n    input_min = x.min()\n    input_max = x.max()\n\n    x_normalized = (x - input_min) / (input_max - input_min + 1e-8) * 2 - 1\n    x_normalized = x_normalized.to(self.device)\n\n    # Convert to grayscale if model expects 1 channel but input has more\n    if self.num_channels == 1 and original_channels != 1:\n        x_normalized = x_normalized.mean(dim=1, keepdim=True)\n\n    generate_mask = mask.to(self.device).bool()\n    generate_mask = generate_mask.expand(-1,\n                                         1, -1, -1).to(self.device)\n    preserve_mask = ~generate_mask\n\n    with torch.no_grad():\n        x_init = x_normalized.clone().to(self.device)\n        noise = torch.randn_like(x_normalized).to(self.device)\n        x_T = torch.where(generate_mask, noise, x_init)\n        t_T = torch.ones(batch_size, device=self.device)\n        x_T, _ = self.diffusion.forward_process(x_T, t_T)\n\n    def inpaint_guidance(x_t: Tensor, t: Tensor) -&gt; Tensor:\n        \"\"\"Preserves known pixels in the image during sampling.\"\"\"\n        with torch.no_grad():\n            return torch.where(preserve_mask, x_normalized, x_t)\n\n    score_func = self._class_conditional_score(class_labels, batch_size)\n\n    self.model.eval()\n    with torch.no_grad():\n        samples_normalized = self.sampler(\n            x_T=x_T,\n            score_model=score_func,\n            n_steps=n_steps,\n            guidance=inpaint_guidance,\n            callback=progress_callback,\n            seed=seed\n        )\n\n    combined_normalized = torch.where(\n        generate_mask, samples_normalized, x_normalized)\n\n    result = (combined_normalized + 1) / 2 * \\\n        (input_max - input_min) + input_min\n\n    self.model.train()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\n    return result\n</code></pre>"},{"location":"api/generative_model/#image_gen.base.GenerativeModel.load","title":"<code>load(path, override=True, unsafe=False)</code>","text":"<p>Loads a saved model from the specified path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the saved model file.</p> required <code>override</code> <code>bool</code> <p>If True, overwrites the current sampler with the saved one.</p> <code>True</code> <code>unsafe</code> <code>bool</code> <p>If True, allows loading custom code components (potentially unsafe).</p> <code>False</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the model state dictionary cannot be loaded properly.</p> Source code in <code>image_gen\\base.py</code> <pre><code>def load(self, path: str, override: bool = True, unsafe: bool = False) -&gt; None:\n    \"\"\"Loads a saved model from the specified path.\n\n    Args:\n        path: Path to the saved model file.\n        override: If True, overwrites the current sampler with the saved one.\n        unsafe: If True, allows loading custom code components (potentially unsafe).\n\n    Raises:\n        RuntimeError: If the model state dictionary cannot be loaded properly.\n    \"\"\"\n    self._model = None\n\n    checkpoint = torch.load(path)\n    self._version = checkpoint.get('model_version')\n\n    self._custom_sampler = None\n    self._custom_diffusion = None\n    self._custom_schedule = None\n    self._rebuild_diffusion(checkpoint, unsafe=unsafe)\n    if override:\n        self._rebuild_sampler(checkpoint, unsafe=unsafe)\n\n    self._stored_labels = checkpoint.get('stored_labels')\n    self._num_classes = (\n        len(self.stored_labels) if self.stored_labels is not None else None\n    )\n    self._label_map = checkpoint.get('label_map')\n\n    # Default to grayscale if channels not specified\n    checkpoint_channels = checkpoint.get('num_channels', 1)\n    self._shape = checkpoint.get('shape', (32, 32))\n\n    self._build_default_model(shape=(checkpoint_channels, *self._shape))\n\n    try:\n        # Load only keys that exist in both models\n        model_dict = self.model.state_dict()\n        # Filter checkpoint keys that exist in the current model\n        pretrained_dict = {\n            k: v\n            for k, v in checkpoint['model_state'].items()\n            if k in model_dict\n        }\n        model_dict.update(pretrained_dict)\n        self.model.load_state_dict(model_dict, strict=False)\n    except RuntimeError as original_error:\n        try:\n            # Try with keys without \"module.\" prefix (happens with DataParallel)\n            new_state_dict = {\n                k.replace('module.', ''): v\n                for k, v in checkpoint['model_state'].items()\n            }\n            model_dict = self.model.state_dict()\n            pretrained_dict = {\n                k: v\n                for k, v in new_state_dict.items()\n                if k in model_dict\n            }\n            model_dict.update(pretrained_dict)\n            self.model.load_state_dict(model_dict, strict=False)\n        except RuntimeError as secondary_error:\n            # Log both errors for better debugging\n            error_msg = (\n                f\"Failed to load model state. Original error: {original_error}. \"\n                f\"Secondary error: {secondary_error}\"\n            )\n            print(f\"Warning: {error_msg}\")\n</code></pre>"},{"location":"api/generative_model/#image_gen.base.GenerativeModel.loss_function","title":"<code>loss_function(x0, eps=1e-05, class_labels=None)</code>","text":"<p>Computes the loss for training the score model.</p> <p>Parameters:</p> Name Type Description Default <code>x0</code> <code>Tensor</code> <p>The input data.</p> required <code>eps</code> <code>float</code> <p>Small constant to avoid numerical issues.</p> <code>1e-05</code> <code>class_labels</code> <code>Optional[Tensor]</code> <p>Class labels for conditional generation.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The computed loss value.</p> Source code in <code>image_gen\\base.py</code> <pre><code>def loss_function(self, x0: torch.Tensor, eps: float = 1e-5,\n                  class_labels: Optional[Tensor] = None) -&gt; torch.Tensor:\n    \"\"\"Computes the loss for training the score model.\n\n    Args:\n        x0: The input data.\n        eps: Small constant to avoid numerical issues.\n        class_labels: Class labels for conditional generation.\n\n    Returns:\n        The computed loss value.\n    \"\"\"\n    t = torch.rand(x0.shape[0], device=x0.device) * (1.0 - eps) + eps\n    xt, noise = self.diffusion.forward_process(x0, t)\n    score = self.model(xt, t, class_label=class_labels)\n    loss_per_example = self.diffusion.compute_loss(score, noise, t)\n    return torch.mean(loss_per_example)\n</code></pre>"},{"location":"api/generative_model/#image_gen.base.GenerativeModel.save","title":"<code>save(path)</code>","text":"<p>Saves the model to the specified path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path where to save the model.</p> required Source code in <code>image_gen\\base.py</code> <pre><code>def save(self, path: str) -&gt; None:\n    \"\"\"Saves the model to the specified path.\n\n    Args:\n        path: Path where to save the model.\n    \"\"\"\n    save_data = {\n        'model_state': self.model.state_dict(),\n        'shape': self.shape,\n        'diffusion_type': self.diffusion.__class__.__name__.lower(),\n        'sampler_type': self.sampler.__class__.__name__.lower(),\n        'num_channels': self.num_channels,\n        'stored_labels': self.stored_labels,\n        'label_map': self._label_map,\n        'model_version': MODEL_VERSION,\n    }\n\n    if hasattr(self.diffusion, 'config'):\n        save_data['diffusion_config'] = self.diffusion.config()\n    if save_data[\"diffusion_type\"] not in GenerativeModel.DIFFUSION_MAP:\n        save_data[\"diffusion_code\"] = get_class_source(\n            self.diffusion.__class__)\n\n    if self.diffusion.NEEDS_NOISE_SCHEDULE:\n        save_data['noise_schedule_type'] = self.diffusion.schedule.__class__.__name__.lower()\n        if hasattr(self.diffusion.schedule, 'config'):\n            save_data['noise_schedule_config'] = self.diffusion.schedule.config()\n        if save_data[\"noise_schedule_type\"] not in GenerativeModel.NOISE_SCHEDULE_MAP:\n            save_data[\"noise_schedule_code\"] = get_class_source(\n                self.noise_schedule.__class__)\n\n    if hasattr(self.sampler, 'config'):\n        save_data['sampler_config'] = self.sampler.config()\n    if save_data[\"sampler_type\"] not in GenerativeModel.SAMPLER_MAP:\n        save_data[\"sampler_code\"] = get_class_source(\n            self.sampler.__class__)\n\n    torch.save(save_data, path)\n</code></pre>"},{"location":"api/generative_model/#image_gen.base.GenerativeModel.score","title":"<code>score(real, generated, metrics=['bpd', 'fid', 'is'], *args, **kwargs)</code>","text":"<p>Evaluates the model using various metrics.</p> <p>Parameters:</p> Name Type Description Default <code>real</code> <code>Tensor</code> <p>Real data samples.</p> required <code>generated</code> <code>Tensor</code> <p>Generated data samples.</p> required <code>metrics</code> <code>List[Union[str, BaseMetric]]</code> <p>List of metrics to compute. Can be strings or BaseMetric instances.</p> <code>['bpd', 'fid', 'is']</code> <code>*args</code> <code>Any</code> <p>Additional arguments for metrics.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for metrics.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dictionary mapping metric names to scores.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If metrics is empty or not a list.</p> Source code in <code>image_gen\\base.py</code> <pre><code>def score(self, real: Tensor, generated: Tensor,\n          metrics: List[Union[str, BaseMetric]] = [\"bpd\", \"fid\", \"is\"],\n          *args: Any, **kwargs: Any) -&gt; Dict[str, float]:\n    \"\"\"Evaluates the model using various metrics.\n\n    Args:\n        real: Real data samples.\n        generated: Generated data samples.\n        metrics: List of metrics to compute. Can be strings or BaseMetric instances.\n        *args: Additional arguments for metrics.\n        **kwargs: Additional keyword arguments for metrics.\n\n    Returns:\n        Dictionary mapping metric names to scores.\n\n    Raises:\n        Exception: If metrics is empty or not a list.\n    \"\"\"\n    if not isinstance(metrics, list) or len(metrics) == 0:\n        raise Exception(\n            \"Scores must be a non-empty list.\")\n\n    calculated_scores = {}\n    for score in metrics:\n        # Instantiate the class\n        if isinstance(score, str) and score.lower() in GenerativeModel.METRIC_MAP:\n            score = GenerativeModel.METRIC_MAP[score.lower()](self)\n        elif isinstance(score, type):\n            score = score(self)\n\n        if not isinstance(score, BaseMetric):\n            warnings.warn(f'\"{score}\" is not a metric, skipping...')\n            continue\n\n        if score.name in calculated_scores:\n            warnings.warn(\n                f'A score with the name of \"{score.name}\" has already been calculated, but it will be overwritten.')\n        calculated_scores[score.name] = score(\n            real, generated, *args, **kwargs)\n\n    return calculated_scores\n</code></pre>"},{"location":"api/generative_model/#image_gen.base.GenerativeModel.set_labels","title":"<code>set_labels(labels)</code>","text":"<p>Sets string labels for the model's classes.</p> <p>Parameters:</p> Name Type Description Default <code>labels</code> <code>List[str]</code> <p>List of string labels, one per class.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the number of labels doesn't match the number of classes.</p> Source code in <code>image_gen\\base.py</code> <pre><code>def set_labels(self, labels: List[str]) -&gt; None:\n    \"\"\"Sets string labels for the model's classes.\n\n    Args:\n        labels: List of string labels, one per class.\n\n    Raises:\n        ValueError: If the number of labels doesn't match the number of classes.\n    \"\"\"\n    # Check if model has class conditioning\n    if not hasattr(self, 'num_classes') or self.num_classes is None:\n        warnings.warn(\n            \"Model not initialized for class conditioning - labels will have no effect\")\n        return\n\n    # Check if we have stored numeric labels\n    if not hasattr(self, 'stored_labels') or self.stored_labels is None:\n        warnings.warn(\n            \"No class labels stored from training - cannot map string labels\")\n        return\n\n    # Validate input length\n    if len(labels) != len(self.stored_labels):\n        raise ValueError(\n            f\"Length mismatch: got {len(labels)} string labels, \"\n            f\"but model has {len(self.stored_labels)} classes. \"\n            f\"Current numeric labels: {self.stored_labels}\"\n        )\n\n    # Create new mapping\n    self._label_map = {\n        string_label: numeric_label\n        for numeric_label, string_label in zip(self.stored_labels, labels)\n    }\n</code></pre>"},{"location":"api/generative_model/#image_gen.base.GenerativeModel.train","title":"<code>train(dataset, epochs=100, batch_size=32, lr=0.001)</code>","text":"<p>Trains the score model.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Union[Dataset, List[Union[Tensor, Tuple[Tensor, Tensor]]]]</code> <p>The dataset to train on. Can be a torch Dataset or a list of tensors or (tensor, label) tuples.</p> required <code>epochs</code> <code>int</code> <p>Number of training epochs.</p> <code>100</code> <code>batch_size</code> <code>int</code> <p>Batch size for training.</p> <code>32</code> <code>lr</code> <code>float</code> <p>Learning rate for the optimizer.</p> <code>0.001</code> Source code in <code>image_gen\\base.py</code> <pre><code>def train(\n    self,\n    dataset: Union[\n        torch.utils.data.Dataset,\n        List[Union[Tensor, Tuple[Tensor, Tensor]]]\n    ],\n    epochs: int = 100,\n    batch_size: int = 32,\n    lr: float = 1e-3\n) -&gt; None:\n    \"\"\"Trains the score model.\n\n    Args:\n        dataset: The dataset to train on. Can be a torch Dataset or a list\n            of tensors or (tensor, label) tuples.\n        epochs: Number of training epochs.\n        batch_size: Batch size for training.\n        lr: Learning rate for the optimizer.\n    \"\"\"\n    first = dataset[0]\n\n    has_labels = isinstance(first, (list, tuple)) and len(first) &gt; 1\n    if has_labels:\n        all_labels = [\n            label if isinstance(label, Tensor) else torch.tensor(label)\n            for _, label in dataset\n        ]\n        all_labels_tensor = torch.cat([lbl.view(-1) for lbl in all_labels])\n        self._stored_labels = sorted(all_labels_tensor.unique().tolist())\n\n        # Create mapping from original labels to 0-based indices\n        self._label_to_index = {\n            lbl: idx for idx, lbl in enumerate(self.stored_labels)\n        }\n        self._num_classes = len(self.stored_labels)\n\n        # Map all labels to indices\n        self._mapped_labels = torch.tensor([\n            self._label_to_index[lbl.item()]\n            for lbl in all_labels_tensor\n        ])\n    else:\n        self._num_classes = None\n\n    first = first[0] if isinstance(first, (list, tuple)) else first\n    self._build_default_model(shape=first.shape)\n\n    optimizer = Adam(self.model.parameters(), lr=lr)\n    dataloader = torch.utils.data.DataLoader(\n        dataset, batch_size=batch_size, shuffle=True)\n\n    epoch_bar = self._progress(range(epochs), desc='Training')\n    for epoch in epoch_bar:\n        avg_loss = 0.0\n        num_items = 0\n\n        batch_bar = self._progress(\n            dataloader, desc=f'Epoch {epoch + 1}', leave=False)\n        for batch in batch_bar:\n            if has_labels:\n                x0, original_labels = batch[0], batch[1]\n                # Convert original labels to mapped indices\n                labels = torch.tensor([\n                    self._label_to_index[lbl.item()]\n                    for lbl in original_labels\n                ], device=self.device)\n            else:\n                x0 = batch\n                labels = None\n\n            x0 = x0.to(self.device)\n\n            optimizer.zero_grad()\n\n            if self.num_classes is not None:\n                loss = self.loss_function(x0, class_labels=labels)\n            else:\n                loss = self.loss_function(x0)\n\n            loss.backward()\n            optimizer.step()\n\n            avg_loss += loss.item() * x0.shape[0]\n            num_items += x0.shape[0]\n</code></pre>"},{"location":"api/diffusion/","title":"Diffusion Processes","text":"<p>Diffusion processes define how noise is added to data during training and removed during generation. They form the core of diffusion-based generative models.</p>"},{"location":"api/diffusion/#base-diffusion","title":"Base Diffusion","text":"<p>The base class for all diffusion processes. It defines the interface that all diffusion implementations must follow.</p> <p>Main Methods: - <code>forward_sde(x, t)</code>: Calculates the drift and diffusion coefficients for the forward SDE at time <code>t</code>. - <code>forward_process(x0, t)</code>: Applies the forward diffusion process to input <code>x0</code> at time <code>t</code>. - <code>compute_loss(score, noise, t)</code>: Computes the loss between predicted and actual noise. - <code>backward_sde(x, t, score)</code>: Computes the backward SDE coefficients for sampling.</p> <p>View Implementation</p>"},{"location":"api/diffusion/#variance-exploding","title":"Variance Exploding","text":"<p>A diffusion process where noise increases exponentially over time. Suitable for image generation tasks.</p> <p>Main Methods: - <code>forward_sde(x, t)</code>: Implements the forward SDE for variance exploding diffusion. - <code>forward_process(x0, t)</code>: Applies the forward process with exponential noise increase. - <code>compute_loss(score, noise, t)</code>: Computes loss specific to variance exploding formulation.</p> <p>View Implementation</p>"},{"location":"api/diffusion/#variance-preserving","title":"Variance Preserving","text":"<p>Maintains a controlled level of variance throughout the diffusion process. Commonly used in various diffusion-based generative models.</p> <p>Main Methods: - <code>forward_sde(x, t)</code>: Implements the forward SDE for variance preserving diffusion. - <code>forward_process(x0, t)</code>: Applies the forward process while preserving variance. - <code>compute_loss(score, noise, t)</code>: Computes loss specific to variance preserving formulation.</p> <p>View Implementation</p>"},{"location":"api/diffusion/#sub-variance-preserving","title":"Sub-Variance Preserving","text":"<p>A variant of variance preserving diffusion with modified noise characteristics.</p> <p>Main Methods: - <code>forward_sde(x, t)</code>: Implements the forward SDE for sub-variance preserving diffusion. - <code>forward_process(x0, t)</code>: Applies the forward process with controlled variance. - <code>compute_loss(score, noise, t)</code>: Computes loss specific to this diffusion variant.</p> <p>View Implementation</p>"},{"location":"api/diffusion/base/","title":"Base Diffusion","text":"<p>Abstract base class for diffusion models.</p> <p>This module defines the interface for diffusion models used in image generation. All diffusion implementations should inherit from this base class and implement the required abstract methods.</p>"},{"location":"api/diffusion/base/#image_gen.diffusion.base.BaseDiffusion","title":"<code>BaseDiffusion</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for diffusion models.</p> <p>This class defines the interface for diffusion models and provides common functionality for forward and backward processes.</p> <p>Attributes:</p> Name Type Description <code>NEEDS_NOISE_SCHEDULE</code> <p>Class constant indicating if a noise schedule is required.</p> Source code in <code>image_gen\\diffusion\\base.py</code> <pre><code>class BaseDiffusion(ABC):\n    \"\"\"Abstract base class for diffusion models.\n\n    This class defines the interface for diffusion models and provides common\n    functionality for forward and backward processes.\n\n    Attributes:\n        NEEDS_NOISE_SCHEDULE: Class constant indicating if a noise schedule is required.\n    \"\"\"\n\n    NEEDS_NOISE_SCHEDULE = True\n\n    def __init__(self, schedule: BaseNoiseSchedule, *_, **__):\n        \"\"\"Initialize the diffusion model.\n\n        Args:\n            schedule: A noise schedule that controls noise addition over time.\n        \"\"\"\n        self.schedule = schedule\n\n    @abstractmethod\n    def forward_sde(self, x: Tensor, t: Tensor, *args: Any, **kwargs: Any) -&gt; Tuple[Tensor, Tensor]:\n        \"\"\"Calculate drift and diffusion coefficients for forward SDE.\n\n        Args:\n            x: The input tensor representing current state.\n            t: Time steps tensor.\n            *args: Additional positional arguments.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            A tuple of (drift, diffusion) tensors.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def forward_process(\n        self, x0: Tensor, t: Tensor, *args: Any, **kwargs: Any\n    ) -&gt; Tuple[Tensor, Tensor]:\n        \"\"\"Apply the forward diffusion process.\n\n        Args:\n            x0: The input tensor representing initial state.\n            t: Time steps tensor.\n            *args: Additional positional arguments.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            A tuple of (noisy_sample, noise) tensors.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def compute_loss(\n        self, score: Tensor, noise: Tensor, t: Tensor, *args: Any, **kwargs: Any\n    ) -&gt; Tensor:\n        \"\"\"Compute loss between predicted and actual noise.\n\n        Args:\n            score: The predicted noise tensor.\n            noise: The actual noise tensor.\n            t: Time steps tensor.\n            *args: Additional positional arguments.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            A tensor representing the computed loss.\n        \"\"\"\n        pass\n\n    def backward_sde(\n        self, x: Tensor, t: Tensor, score: Tensor, *_, **__\n    ) -&gt; Tuple[Tensor, Tensor]:\n        \"\"\"Compute the backward SDE coefficients.\n\n        Args:\n            x: The input tensor representing current state.\n            t: Time steps tensor.\n            score: The score function output.\n\n        Returns:\n            A tuple of (drift, diffusion) tensors for the backward process.\n        \"\"\"\n        f, g = self.forward_sde(x, t)\n        g_squared = g**2\n        if g_squared.shape != score.shape:\n            g_squared = g_squared.expand_as(score)\n\n        return f - g_squared * score, g\n\n    @property\n    def schedule(self) -&gt; BaseNoiseSchedule:\n        \"\"\"Get the noise schedule.\n\n        Returns:\n            The noise schedule object.\n        \"\"\"\n        return self._schedule\n\n    @schedule.setter\n    def schedule(self, value: BaseNoiseSchedule):\n        \"\"\"Set the noise schedule.\n\n        The schedule can only be set once during initialization.\n\n        Args:\n            value: The noise schedule object to set.\n\n        Raises:\n            AttributeError: If trying to change the schedule after initialization.\n        \"\"\"\n        # Schedule shouldn't be allowed to change once the class is initialized\n        if not hasattr(self, '_schedule'):\n            self._schedule = value\n            return\n        raise AttributeError(\"Attribute 'schedule' is not settable\")\n\n    def config(self) -&gt; dict:\n        \"\"\"Get configuration parameters for the diffusion model.\n\n        Returns:\n            A dictionary containing configuration parameters.\n        \"\"\"\n        return {}\n\n    def __str__(self) -&gt; str:\n        \"\"\"Get string representation of the diffusion model.\n\n        Returns:\n            A string describing the model with its configuration parameters.\n        \"\"\"\n        config = self.config()\n        params = \", \".join(f\"{k}: {v}\" for k, v in config.items())\n        return f\"{self._class_name}({params})\"\n\n    @property\n    def _class_name(self) -&gt; str:\n        \"\"\"Get the class name.\n\n        This will be automatically overridden in custom classes made by users.\n\n        Returns:\n            The name of the class.\n        \"\"\"\n        return self.__class__.__name__\n</code></pre>"},{"location":"api/diffusion/base/#image_gen.diffusion.base.BaseDiffusion.schedule","title":"<code>schedule</code>  <code>property</code> <code>writable</code>","text":"<p>Get the noise schedule.</p> <p>Returns:</p> Type Description <code>BaseNoiseSchedule</code> <p>The noise schedule object.</p>"},{"location":"api/diffusion/base/#image_gen.diffusion.base.BaseDiffusion.__init__","title":"<code>__init__(schedule, *_, **__)</code>","text":"<p>Initialize the diffusion model.</p> <p>Parameters:</p> Name Type Description Default <code>schedule</code> <code>BaseNoiseSchedule</code> <p>A noise schedule that controls noise addition over time.</p> required Source code in <code>image_gen\\diffusion\\base.py</code> <pre><code>def __init__(self, schedule: BaseNoiseSchedule, *_, **__):\n    \"\"\"Initialize the diffusion model.\n\n    Args:\n        schedule: A noise schedule that controls noise addition over time.\n    \"\"\"\n    self.schedule = schedule\n</code></pre>"},{"location":"api/diffusion/base/#image_gen.diffusion.base.BaseDiffusion.__str__","title":"<code>__str__()</code>","text":"<p>Get string representation of the diffusion model.</p> <p>Returns:</p> Type Description <code>str</code> <p>A string describing the model with its configuration parameters.</p> Source code in <code>image_gen\\diffusion\\base.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Get string representation of the diffusion model.\n\n    Returns:\n        A string describing the model with its configuration parameters.\n    \"\"\"\n    config = self.config()\n    params = \", \".join(f\"{k}: {v}\" for k, v in config.items())\n    return f\"{self._class_name}({params})\"\n</code></pre>"},{"location":"api/diffusion/base/#image_gen.diffusion.base.BaseDiffusion.backward_sde","title":"<code>backward_sde(x, t, score, *_, **__)</code>","text":"<p>Compute the backward SDE coefficients.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor representing current state.</p> required <code>t</code> <code>Tensor</code> <p>Time steps tensor.</p> required <code>score</code> <code>Tensor</code> <p>The score function output.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor]</code> <p>A tuple of (drift, diffusion) tensors for the backward process.</p> Source code in <code>image_gen\\diffusion\\base.py</code> <pre><code>def backward_sde(\n    self, x: Tensor, t: Tensor, score: Tensor, *_, **__\n) -&gt; Tuple[Tensor, Tensor]:\n    \"\"\"Compute the backward SDE coefficients.\n\n    Args:\n        x: The input tensor representing current state.\n        t: Time steps tensor.\n        score: The score function output.\n\n    Returns:\n        A tuple of (drift, diffusion) tensors for the backward process.\n    \"\"\"\n    f, g = self.forward_sde(x, t)\n    g_squared = g**2\n    if g_squared.shape != score.shape:\n        g_squared = g_squared.expand_as(score)\n\n    return f - g_squared * score, g\n</code></pre>"},{"location":"api/diffusion/base/#image_gen.diffusion.base.BaseDiffusion.compute_loss","title":"<code>compute_loss(score, noise, t, *args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Compute loss between predicted and actual noise.</p> <p>Parameters:</p> Name Type Description Default <code>score</code> <code>Tensor</code> <p>The predicted noise tensor.</p> required <code>noise</code> <code>Tensor</code> <p>The actual noise tensor.</p> required <code>t</code> <code>Tensor</code> <p>Time steps tensor.</p> required <code>*args</code> <code>Any</code> <p>Additional positional arguments.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor representing the computed loss.</p> Source code in <code>image_gen\\diffusion\\base.py</code> <pre><code>@abstractmethod\ndef compute_loss(\n    self, score: Tensor, noise: Tensor, t: Tensor, *args: Any, **kwargs: Any\n) -&gt; Tensor:\n    \"\"\"Compute loss between predicted and actual noise.\n\n    Args:\n        score: The predicted noise tensor.\n        noise: The actual noise tensor.\n        t: Time steps tensor.\n        *args: Additional positional arguments.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        A tensor representing the computed loss.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/diffusion/base/#image_gen.diffusion.base.BaseDiffusion.config","title":"<code>config()</code>","text":"<p>Get configuration parameters for the diffusion model.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing configuration parameters.</p> Source code in <code>image_gen\\diffusion\\base.py</code> <pre><code>def config(self) -&gt; dict:\n    \"\"\"Get configuration parameters for the diffusion model.\n\n    Returns:\n        A dictionary containing configuration parameters.\n    \"\"\"\n    return {}\n</code></pre>"},{"location":"api/diffusion/base/#image_gen.diffusion.base.BaseDiffusion.forward_process","title":"<code>forward_process(x0, t, *args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Apply the forward diffusion process.</p> <p>Parameters:</p> Name Type Description Default <code>x0</code> <code>Tensor</code> <p>The input tensor representing initial state.</p> required <code>t</code> <code>Tensor</code> <p>Time steps tensor.</p> required <code>*args</code> <code>Any</code> <p>Additional positional arguments.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor]</code> <p>A tuple of (noisy_sample, noise) tensors.</p> Source code in <code>image_gen\\diffusion\\base.py</code> <pre><code>@abstractmethod\ndef forward_process(\n    self, x0: Tensor, t: Tensor, *args: Any, **kwargs: Any\n) -&gt; Tuple[Tensor, Tensor]:\n    \"\"\"Apply the forward diffusion process.\n\n    Args:\n        x0: The input tensor representing initial state.\n        t: Time steps tensor.\n        *args: Additional positional arguments.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        A tuple of (noisy_sample, noise) tensors.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/diffusion/base/#image_gen.diffusion.base.BaseDiffusion.forward_sde","title":"<code>forward_sde(x, t, *args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Calculate drift and diffusion coefficients for forward SDE.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor representing current state.</p> required <code>t</code> <code>Tensor</code> <p>Time steps tensor.</p> required <code>*args</code> <code>Any</code> <p>Additional positional arguments.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor]</code> <p>A tuple of (drift, diffusion) tensors.</p> Source code in <code>image_gen\\diffusion\\base.py</code> <pre><code>@abstractmethod\ndef forward_sde(self, x: Tensor, t: Tensor, *args: Any, **kwargs: Any) -&gt; Tuple[Tensor, Tensor]:\n    \"\"\"Calculate drift and diffusion coefficients for forward SDE.\n\n    Args:\n        x: The input tensor representing current state.\n        t: Time steps tensor.\n        *args: Additional positional arguments.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        A tuple of (drift, diffusion) tensors.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/diffusion/sub_vp/","title":"Sub-Variance Preserving","text":"<p>Sub-Variance Preserving diffusion model implementation.</p> <p>This module extends the basic variance preserving diffusion model with  a modified diffusion process that uses a different noise formulation.</p>"},{"location":"api/diffusion/sub_vp/#image_gen.diffusion.sub_vp.SubVariancePreserving","title":"<code>SubVariancePreserving</code>","text":"<p>               Bases: <code>BaseDiffusion</code></p> <p>Sub-Variance Preserving diffusion model implementation.</p> <p>This class implements a variant of the variance preserving diffusion model with modified noise characteristics. It maintains a controlled level of variance throughout the diffusion process with a different formulation for the diffusion coefficient.</p> Source code in <code>image_gen\\diffusion\\sub_vp.py</code> <pre><code>class SubVariancePreserving(BaseDiffusion):\n    \"\"\"Sub-Variance Preserving diffusion model implementation.\n\n    This class implements a variant of the variance preserving diffusion model\n    with modified noise characteristics. It maintains a controlled level of\n    variance throughout the diffusion process with a different formulation\n    for the diffusion coefficient.\n    \"\"\"\n\n    def forward_sde(self, x: Tensor, t: Tensor, *args: Any, **kwargs: Any) -&gt; Tuple[\n            Tensor, Tensor]:\n        \"\"\"Calculate drift and diffusion coefficients for forward SDE.\n\n        Args:\n            x: The input tensor representing current state.\n            t: Time steps tensor.\n            *args: Additional positional arguments.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            A tuple of (drift, diffusion) tensors.\n        \"\"\"\n        beta_t = self.schedule(t, *args, **kwargs).view(-1, 1, 1, 1)\n        integral_beta_t = self.schedule.integral_beta(t, *args, **kwargs).view(\n            -1, 1, 1, 1)\n        exponential_term = torch.exp(-2 * integral_beta_t)\n        g_squared = beta_t * (1 - exponential_term)\n        diffusion = torch.sqrt(g_squared)\n        drift = -0.5 * beta_t * x\n        return drift, diffusion\n\n    def forward_process(self, x0: Tensor, t: Tensor, *args: Any, **kwargs: Any) -&gt; Tuple[\n            Tensor, Tensor]:\n        \"\"\"Apply the forward diffusion process.\n\n        Adds noise to the input according to the sub-variance preserving \n        schedule.\n\n        Args:\n            x0: The input tensor representing initial state.\n            t: Time steps tensor.\n            *args: Additional positional arguments.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            A tuple of (noisy_sample, noise) tensors.\n        \"\"\"\n        integral_beta = self.schedule.integral_beta(t, *args, **kwargs)\n        alpha_bar_t = torch.exp(-integral_beta).view(-1, 1, 1, 1)\n        mu_x0 = torch.sqrt(alpha_bar_t) * x0\n        sigma_t = torch.sqrt(1 - alpha_bar_t)\n        noise = torch.randn_like(x0)\n        xt = mu_x0 + sigma_t * noise\n        return xt, noise\n\n    def compute_loss(self, score: Tensor, noise: Tensor, t: Tensor,\n                     *args: Any, **kwargs: Any) -&gt; Tensor:\n        \"\"\"Compute loss between predicted score and actual noise.\n\n        Args:\n            score: The predicted noise tensor.\n            noise: The actual noise tensor.\n            t: Time steps tensor.\n            *args: Additional positional arguments.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            A tensor representing the computed loss.\n        \"\"\"\n        integral_beta = self.schedule.integral_beta(t, *args, **kwargs)\n        alpha_bar_t = torch.exp(-integral_beta)\n        sigma_t = torch.sqrt(1 - alpha_bar_t)\n        sigma_t = sigma_t.view(score.shape[0], *([1] * (score.dim() - 1)))\n        loss = (sigma_t * score + noise) ** 2\n        return loss.sum(dim=tuple(range(1, loss.dim())))\n\n    def config(self) -&gt; dict:\n        \"\"\"Get configuration parameters for the diffusion model.\n\n        Returns:\n            A dictionary containing configuration parameters.\n        \"\"\"\n        return self.schedule.config() if hasattr(self.schedule, \"config\") else {}\n</code></pre>"},{"location":"api/diffusion/sub_vp/#image_gen.diffusion.sub_vp.SubVariancePreserving.compute_loss","title":"<code>compute_loss(score, noise, t, *args, **kwargs)</code>","text":"<p>Compute loss between predicted score and actual noise.</p> <p>Parameters:</p> Name Type Description Default <code>score</code> <code>Tensor</code> <p>The predicted noise tensor.</p> required <code>noise</code> <code>Tensor</code> <p>The actual noise tensor.</p> required <code>t</code> <code>Tensor</code> <p>Time steps tensor.</p> required <code>*args</code> <code>Any</code> <p>Additional positional arguments.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor representing the computed loss.</p> Source code in <code>image_gen\\diffusion\\sub_vp.py</code> <pre><code>def compute_loss(self, score: Tensor, noise: Tensor, t: Tensor,\n                 *args: Any, **kwargs: Any) -&gt; Tensor:\n    \"\"\"Compute loss between predicted score and actual noise.\n\n    Args:\n        score: The predicted noise tensor.\n        noise: The actual noise tensor.\n        t: Time steps tensor.\n        *args: Additional positional arguments.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        A tensor representing the computed loss.\n    \"\"\"\n    integral_beta = self.schedule.integral_beta(t, *args, **kwargs)\n    alpha_bar_t = torch.exp(-integral_beta)\n    sigma_t = torch.sqrt(1 - alpha_bar_t)\n    sigma_t = sigma_t.view(score.shape[0], *([1] * (score.dim() - 1)))\n    loss = (sigma_t * score + noise) ** 2\n    return loss.sum(dim=tuple(range(1, loss.dim())))\n</code></pre>"},{"location":"api/diffusion/sub_vp/#image_gen.diffusion.sub_vp.SubVariancePreserving.config","title":"<code>config()</code>","text":"<p>Get configuration parameters for the diffusion model.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing configuration parameters.</p> Source code in <code>image_gen\\diffusion\\sub_vp.py</code> <pre><code>def config(self) -&gt; dict:\n    \"\"\"Get configuration parameters for the diffusion model.\n\n    Returns:\n        A dictionary containing configuration parameters.\n    \"\"\"\n    return self.schedule.config() if hasattr(self.schedule, \"config\") else {}\n</code></pre>"},{"location":"api/diffusion/sub_vp/#image_gen.diffusion.sub_vp.SubVariancePreserving.forward_process","title":"<code>forward_process(x0, t, *args, **kwargs)</code>","text":"<p>Apply the forward diffusion process.</p> <p>Adds noise to the input according to the sub-variance preserving  schedule.</p> <p>Parameters:</p> Name Type Description Default <code>x0</code> <code>Tensor</code> <p>The input tensor representing initial state.</p> required <code>t</code> <code>Tensor</code> <p>Time steps tensor.</p> required <code>*args</code> <code>Any</code> <p>Additional positional arguments.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor]</code> <p>A tuple of (noisy_sample, noise) tensors.</p> Source code in <code>image_gen\\diffusion\\sub_vp.py</code> <pre><code>def forward_process(self, x0: Tensor, t: Tensor, *args: Any, **kwargs: Any) -&gt; Tuple[\n        Tensor, Tensor]:\n    \"\"\"Apply the forward diffusion process.\n\n    Adds noise to the input according to the sub-variance preserving \n    schedule.\n\n    Args:\n        x0: The input tensor representing initial state.\n        t: Time steps tensor.\n        *args: Additional positional arguments.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        A tuple of (noisy_sample, noise) tensors.\n    \"\"\"\n    integral_beta = self.schedule.integral_beta(t, *args, **kwargs)\n    alpha_bar_t = torch.exp(-integral_beta).view(-1, 1, 1, 1)\n    mu_x0 = torch.sqrt(alpha_bar_t) * x0\n    sigma_t = torch.sqrt(1 - alpha_bar_t)\n    noise = torch.randn_like(x0)\n    xt = mu_x0 + sigma_t * noise\n    return xt, noise\n</code></pre>"},{"location":"api/diffusion/sub_vp/#image_gen.diffusion.sub_vp.SubVariancePreserving.forward_sde","title":"<code>forward_sde(x, t, *args, **kwargs)</code>","text":"<p>Calculate drift and diffusion coefficients for forward SDE.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor representing current state.</p> required <code>t</code> <code>Tensor</code> <p>Time steps tensor.</p> required <code>*args</code> <code>Any</code> <p>Additional positional arguments.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor]</code> <p>A tuple of (drift, diffusion) tensors.</p> Source code in <code>image_gen\\diffusion\\sub_vp.py</code> <pre><code>def forward_sde(self, x: Tensor, t: Tensor, *args: Any, **kwargs: Any) -&gt; Tuple[\n        Tensor, Tensor]:\n    \"\"\"Calculate drift and diffusion coefficients for forward SDE.\n\n    Args:\n        x: The input tensor representing current state.\n        t: Time steps tensor.\n        *args: Additional positional arguments.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        A tuple of (drift, diffusion) tensors.\n    \"\"\"\n    beta_t = self.schedule(t, *args, **kwargs).view(-1, 1, 1, 1)\n    integral_beta_t = self.schedule.integral_beta(t, *args, **kwargs).view(\n        -1, 1, 1, 1)\n    exponential_term = torch.exp(-2 * integral_beta_t)\n    g_squared = beta_t * (1 - exponential_term)\n    diffusion = torch.sqrt(g_squared)\n    drift = -0.5 * beta_t * x\n    return drift, diffusion\n</code></pre>"},{"location":"api/diffusion/ve/","title":"Variance Exploding","text":"<p>Variance Exploding diffusion model implementation.</p> <p>This module implements the Variance Exploding diffusion model and its  corresponding noise schedule, which is particularly effective for image  generation tasks.</p>"},{"location":"api/diffusion/ve/#image_gen.diffusion.ve.VarianceExploding","title":"<code>VarianceExploding</code>","text":"<p>               Bases: <code>BaseDiffusion</code></p> <p>Variance Exploding diffusion model implementation.</p> <p>This model implements diffusion using a variance exploding process, where the noise increases exponentially with time.</p> <p>Attributes:</p> Name Type Description <code>NEEDS_NOISE_SCHEDULE</code> <p>Class constant indicating if a custom noise schedule is required.</p> Source code in <code>image_gen\\diffusion\\ve.py</code> <pre><code>class VarianceExploding(BaseDiffusion):\n    \"\"\"Variance Exploding diffusion model implementation.\n\n    This model implements diffusion using a variance exploding process,\n    where the noise increases exponentially with time.\n\n    Attributes:\n        NEEDS_NOISE_SCHEDULE: Class constant indicating if a custom noise\n            schedule is required.\n    \"\"\"\n\n    NEEDS_NOISE_SCHEDULE = False\n\n    def __init__(self, *_, sigma: float = 25.0, **__):\n        \"\"\"Initialize the variance exploding diffusion model.\n\n        Args:\n            sigma: Base sigma value for variance control. Defaults to 25.0.\n        \"\"\"\n        super().__init__(VarianceExplodingSchedule(sigma))\n\n    def forward_sde(self, x: Tensor, t: Tensor, *_, **__) -&gt; Tuple[\n            Tensor, Tensor]:\n        \"\"\"Calculate drift and diffusion for the forward SDE.\n\n        Args:\n            x: Input tensor representing the current state.\n            t: Time steps tensor.\n\n        Returns:\n            Tuple of (drift, diffusion) tensors.\n        \"\"\"\n        drift = torch.zeros_like(x)\n        diffusion = (self.schedule.sigma ** t).view(-1, 1, 1, 1)\n        return drift, diffusion\n\n    def forward_process(self, x0: Tensor, t: Tensor, *args: Any, **kwargs: Any) -&gt; Tuple[\n            Tensor, Tensor]:\n        \"\"\"Apply the forward diffusion process.\n\n        Args:\n            x0: Input tensor representing initial state.\n            t: Time steps tensor.\n            *args: Additional positional arguments.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            Tuple of (noisy_sample, noise) tensors.\n        \"\"\"\n        sigma_t = self.schedule(t, *args, **kwargs)\n        sigma = sigma_t.view(x0.shape[0], *([1] * (x0.dim() - 1)))\n        noise = torch.randn_like(x0)\n        return x0 + sigma * noise, noise\n\n    def compute_loss(self, score: Tensor, noise: Tensor, t: Tensor,\n                     *args: Any, **kwargs: Any) -&gt; Tensor:\n        \"\"\"Compute loss between predicted score and actual noise.\n\n        Args:\n            score: Predicted score tensor.\n            noise: Actual noise tensor.\n            t: Time steps tensor.\n            *args: Additional positional arguments.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            Loss tensor.\n        \"\"\"\n        sigma_t = self.schedule(t, *args, **kwargs)\n        sigma_t = sigma_t.view(score.shape[0], *([1] * (score.dim() - 1)))\n        loss = (sigma_t * score + noise) ** 2\n        return loss.sum(dim=tuple(range(1, loss.dim())))\n\n    def config(self) -&gt; dict:\n        \"\"\"Get configuration parameters for the diffusion model.\n\n        Returns:\n            Dictionary containing configuration parameters.\n        \"\"\"\n        return self.schedule.config()\n</code></pre>"},{"location":"api/diffusion/ve/#image_gen.diffusion.ve.VarianceExploding.__init__","title":"<code>__init__(*_, sigma=25.0, **__)</code>","text":"<p>Initialize the variance exploding diffusion model.</p> <p>Parameters:</p> Name Type Description Default <code>sigma</code> <code>float</code> <p>Base sigma value for variance control. Defaults to 25.0.</p> <code>25.0</code> Source code in <code>image_gen\\diffusion\\ve.py</code> <pre><code>def __init__(self, *_, sigma: float = 25.0, **__):\n    \"\"\"Initialize the variance exploding diffusion model.\n\n    Args:\n        sigma: Base sigma value for variance control. Defaults to 25.0.\n    \"\"\"\n    super().__init__(VarianceExplodingSchedule(sigma))\n</code></pre>"},{"location":"api/diffusion/ve/#image_gen.diffusion.ve.VarianceExploding.compute_loss","title":"<code>compute_loss(score, noise, t, *args, **kwargs)</code>","text":"<p>Compute loss between predicted score and actual noise.</p> <p>Parameters:</p> Name Type Description Default <code>score</code> <code>Tensor</code> <p>Predicted score tensor.</p> required <code>noise</code> <code>Tensor</code> <p>Actual noise tensor.</p> required <code>t</code> <code>Tensor</code> <p>Time steps tensor.</p> required <code>*args</code> <code>Any</code> <p>Additional positional arguments.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Loss tensor.</p> Source code in <code>image_gen\\diffusion\\ve.py</code> <pre><code>def compute_loss(self, score: Tensor, noise: Tensor, t: Tensor,\n                 *args: Any, **kwargs: Any) -&gt; Tensor:\n    \"\"\"Compute loss between predicted score and actual noise.\n\n    Args:\n        score: Predicted score tensor.\n        noise: Actual noise tensor.\n        t: Time steps tensor.\n        *args: Additional positional arguments.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        Loss tensor.\n    \"\"\"\n    sigma_t = self.schedule(t, *args, **kwargs)\n    sigma_t = sigma_t.view(score.shape[0], *([1] * (score.dim() - 1)))\n    loss = (sigma_t * score + noise) ** 2\n    return loss.sum(dim=tuple(range(1, loss.dim())))\n</code></pre>"},{"location":"api/diffusion/ve/#image_gen.diffusion.ve.VarianceExploding.config","title":"<code>config()</code>","text":"<p>Get configuration parameters for the diffusion model.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing configuration parameters.</p> Source code in <code>image_gen\\diffusion\\ve.py</code> <pre><code>def config(self) -&gt; dict:\n    \"\"\"Get configuration parameters for the diffusion model.\n\n    Returns:\n        Dictionary containing configuration parameters.\n    \"\"\"\n    return self.schedule.config()\n</code></pre>"},{"location":"api/diffusion/ve/#image_gen.diffusion.ve.VarianceExploding.forward_process","title":"<code>forward_process(x0, t, *args, **kwargs)</code>","text":"<p>Apply the forward diffusion process.</p> <p>Parameters:</p> Name Type Description Default <code>x0</code> <code>Tensor</code> <p>Input tensor representing initial state.</p> required <code>t</code> <code>Tensor</code> <p>Time steps tensor.</p> required <code>*args</code> <code>Any</code> <p>Additional positional arguments.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor]</code> <p>Tuple of (noisy_sample, noise) tensors.</p> Source code in <code>image_gen\\diffusion\\ve.py</code> <pre><code>def forward_process(self, x0: Tensor, t: Tensor, *args: Any, **kwargs: Any) -&gt; Tuple[\n        Tensor, Tensor]:\n    \"\"\"Apply the forward diffusion process.\n\n    Args:\n        x0: Input tensor representing initial state.\n        t: Time steps tensor.\n        *args: Additional positional arguments.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        Tuple of (noisy_sample, noise) tensors.\n    \"\"\"\n    sigma_t = self.schedule(t, *args, **kwargs)\n    sigma = sigma_t.view(x0.shape[0], *([1] * (x0.dim() - 1)))\n    noise = torch.randn_like(x0)\n    return x0 + sigma * noise, noise\n</code></pre>"},{"location":"api/diffusion/ve/#image_gen.diffusion.ve.VarianceExploding.forward_sde","title":"<code>forward_sde(x, t, *_, **__)</code>","text":"<p>Calculate drift and diffusion for the forward SDE.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor representing the current state.</p> required <code>t</code> <code>Tensor</code> <p>Time steps tensor.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor]</code> <p>Tuple of (drift, diffusion) tensors.</p> Source code in <code>image_gen\\diffusion\\ve.py</code> <pre><code>def forward_sde(self, x: Tensor, t: Tensor, *_, **__) -&gt; Tuple[\n        Tensor, Tensor]:\n    \"\"\"Calculate drift and diffusion for the forward SDE.\n\n    Args:\n        x: Input tensor representing the current state.\n        t: Time steps tensor.\n\n    Returns:\n        Tuple of (drift, diffusion) tensors.\n    \"\"\"\n    drift = torch.zeros_like(x)\n    diffusion = (self.schedule.sigma ** t).view(-1, 1, 1, 1)\n    return drift, diffusion\n</code></pre>"},{"location":"api/diffusion/ve/#image_gen.diffusion.ve.VarianceExplodingSchedule","title":"<code>VarianceExplodingSchedule</code>","text":"<p>               Bases: <code>BaseNoiseSchedule</code></p> <p>Variance Exploding noise schedule.</p> <p>This schedule models noise that increases exponentially over time, creating a \"variance exploding\" effect.</p> <p>Attributes:</p> Name Type Description <code>sigma</code> <p>Base sigma value that controls the rate of variance explosion.</p> Source code in <code>image_gen\\diffusion\\ve.py</code> <pre><code>class VarianceExplodingSchedule(BaseNoiseSchedule):\n    \"\"\"Variance Exploding noise schedule.\n\n    This schedule models noise that increases exponentially over time,\n    creating a \"variance exploding\" effect.\n\n    Attributes:\n        sigma: Base sigma value that controls the rate of variance explosion.\n    \"\"\"\n\n    def __init__(self, sigma: float, *_, **__):\n        \"\"\"Initialize the variance exploding noise schedule.\n\n        Args:\n            sigma: Base sigma value for the schedule.\n        \"\"\"\n        self.sigma = sigma\n\n    def __call__(self, t: Tensor, *_, **__) -&gt; Tensor:\n        \"\"\"Calculate the noise magnitude at time t.\n\n        Args:\n            t: Time step tensor.\n\n        Returns:\n            Tensor containing noise magnitudes at time t.\n        \"\"\"\n        log_sigma = torch.log(torch.tensor(\n            self.sigma, dtype=torch.float32, device=t.device))\n        return torch.sqrt(0.5 * (self.sigma ** (2 * t) - 1.0) / log_sigma)\n\n    def integral_beta(self, t: Tensor, *_, **__) -&gt; Tensor:\n        \"\"\"Calculate the integrated noise intensity up to time t.\n\n        Args:\n            t: Time step tensor.\n\n        Returns:\n            Tensor containing integrated noise values.\n        \"\"\"\n        return 0.5 * (self.sigma ** (2 * t) - 1) / np.log(self.sigma)\n\n    def config(self) -&gt; dict:\n        \"\"\"Get configuration parameters for the schedule.\n\n        Returns:\n            Dictionary containing configuration parameters.\n        \"\"\"\n        return {\n            \"sigma\": self.sigma\n        }\n</code></pre>"},{"location":"api/diffusion/ve/#image_gen.diffusion.ve.VarianceExplodingSchedule.__call__","title":"<code>__call__(t, *_, **__)</code>","text":"<p>Calculate the noise magnitude at time t.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Tensor</code> <p>Time step tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor containing noise magnitudes at time t.</p> Source code in <code>image_gen\\diffusion\\ve.py</code> <pre><code>def __call__(self, t: Tensor, *_, **__) -&gt; Tensor:\n    \"\"\"Calculate the noise magnitude at time t.\n\n    Args:\n        t: Time step tensor.\n\n    Returns:\n        Tensor containing noise magnitudes at time t.\n    \"\"\"\n    log_sigma = torch.log(torch.tensor(\n        self.sigma, dtype=torch.float32, device=t.device))\n    return torch.sqrt(0.5 * (self.sigma ** (2 * t) - 1.0) / log_sigma)\n</code></pre>"},{"location":"api/diffusion/ve/#image_gen.diffusion.ve.VarianceExplodingSchedule.__init__","title":"<code>__init__(sigma, *_, **__)</code>","text":"<p>Initialize the variance exploding noise schedule.</p> <p>Parameters:</p> Name Type Description Default <code>sigma</code> <code>float</code> <p>Base sigma value for the schedule.</p> required Source code in <code>image_gen\\diffusion\\ve.py</code> <pre><code>def __init__(self, sigma: float, *_, **__):\n    \"\"\"Initialize the variance exploding noise schedule.\n\n    Args:\n        sigma: Base sigma value for the schedule.\n    \"\"\"\n    self.sigma = sigma\n</code></pre>"},{"location":"api/diffusion/ve/#image_gen.diffusion.ve.VarianceExplodingSchedule.config","title":"<code>config()</code>","text":"<p>Get configuration parameters for the schedule.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing configuration parameters.</p> Source code in <code>image_gen\\diffusion\\ve.py</code> <pre><code>def config(self) -&gt; dict:\n    \"\"\"Get configuration parameters for the schedule.\n\n    Returns:\n        Dictionary containing configuration parameters.\n    \"\"\"\n    return {\n        \"sigma\": self.sigma\n    }\n</code></pre>"},{"location":"api/diffusion/ve/#image_gen.diffusion.ve.VarianceExplodingSchedule.integral_beta","title":"<code>integral_beta(t, *_, **__)</code>","text":"<p>Calculate the integrated noise intensity up to time t.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Tensor</code> <p>Time step tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor containing integrated noise values.</p> Source code in <code>image_gen\\diffusion\\ve.py</code> <pre><code>def integral_beta(self, t: Tensor, *_, **__) -&gt; Tensor:\n    \"\"\"Calculate the integrated noise intensity up to time t.\n\n    Args:\n        t: Time step tensor.\n\n    Returns:\n        Tensor containing integrated noise values.\n    \"\"\"\n    return 0.5 * (self.sigma ** (2 * t) - 1) / np.log(self.sigma)\n</code></pre>"},{"location":"api/diffusion/vp/","title":"Variance Preserving","text":"<p>Variance Preserving diffusion model implementation.</p> <p>This module implements the Variance Preserving diffusion model which  is commonly used in diffusion-based generative models. It maintains a certain level of variance throughout the diffusion process.</p>"},{"location":"api/diffusion/vp/#image_gen.diffusion.vp.VariancePreserving","title":"<code>VariancePreserving</code>","text":"<p>               Bases: <code>BaseDiffusion</code></p> <p>Variance Preserving diffusion model implementation.</p> <p>This class implements a diffusion model that preserves variance throughout the noise addition process. This approach is commonly used in various diffusion-based generative models.</p> Source code in <code>image_gen\\diffusion\\vp.py</code> <pre><code>class VariancePreserving(BaseDiffusion):\n    \"\"\"Variance Preserving diffusion model implementation.\n\n    This class implements a diffusion model that preserves variance throughout\n    the noise addition process. This approach is commonly used in various\n    diffusion-based generative models.\n    \"\"\"\n\n    def forward_sde(self, x: Tensor, t: Tensor, *args: Any, **kwargs: Any) -&gt; Tuple[\n            Tensor, Tensor]:\n        \"\"\"Calculate drift and diffusion coefficients for forward SDE.\n\n        Args:\n            x: The input tensor representing current state.\n            t: Time steps tensor.\n            *args: Additional positional arguments.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            A tuple of (drift, diffusion) tensors.\n        \"\"\"\n        beta_t = self.schedule(t, *args, **kwargs).view(-1, 1, 1, 1)\n        drift = -0.5 * beta_t * x\n        diffusion = torch.sqrt(beta_t)\n        return drift, diffusion\n\n    def forward_process(self, x0: Tensor, t: Tensor, *args: Any, **kwargs: Any) -&gt; Tuple[\n            Tensor, Tensor]:\n        \"\"\"Apply the forward diffusion process.\n\n        Adds noise to the input according to the variance preserving schedule.\n\n        Args:\n            x0: The input tensor representing initial state.\n            t: Time steps tensor.\n            *args: Additional positional arguments.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            A tuple of (noisy_sample, noise) tensors.\n        \"\"\"\n        integral = self.schedule.integral_beta(t, *args, **kwargs)\n        alpha_bar_t = torch.exp(-integral).view(-1, 1, 1, 1)\n\n        noise = torch.randn_like(x0)\n        xt = (torch.sqrt(alpha_bar_t) * x0 +\n              torch.sqrt(1.0 - alpha_bar_t) * noise)\n\n        return xt, noise\n\n    def compute_loss(self, score: Tensor, noise: Tensor, t: Tensor,\n                     *args: Any, **kwargs: Any) -&gt; Tensor:\n        \"\"\"Compute loss between predicted score and actual noise.\n\n        Args:\n            score: The predicted noise tensor.\n            noise: The actual noise tensor.\n            t: Time steps tensor.\n            *args: Additional positional arguments.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            A tensor representing the computed loss.\n        \"\"\"\n        integral_beta = self.schedule.integral_beta(t, *args, **kwargs)\n        alpha_bar_t = torch.exp(-integral_beta)\n        sigma_t = torch.sqrt(1 - alpha_bar_t)\n        sigma_t = sigma_t.view(score.shape[0], *([1] * (score.dim() - 1)))\n        loss = (sigma_t * score + noise) ** 2\n        return loss.sum(dim=tuple(range(1, loss.dim())))\n\n    def config(self) -&gt; dict:\n        \"\"\"Get configuration parameters for the diffusion model.\n\n        Returns:\n            A dictionary containing configuration parameters.\n        \"\"\"\n        return self.schedule.config() if hasattr(self.schedule, \"config\") else {}\n</code></pre>"},{"location":"api/diffusion/vp/#image_gen.diffusion.vp.VariancePreserving.compute_loss","title":"<code>compute_loss(score, noise, t, *args, **kwargs)</code>","text":"<p>Compute loss between predicted score and actual noise.</p> <p>Parameters:</p> Name Type Description Default <code>score</code> <code>Tensor</code> <p>The predicted noise tensor.</p> required <code>noise</code> <code>Tensor</code> <p>The actual noise tensor.</p> required <code>t</code> <code>Tensor</code> <p>Time steps tensor.</p> required <code>*args</code> <code>Any</code> <p>Additional positional arguments.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor representing the computed loss.</p> Source code in <code>image_gen\\diffusion\\vp.py</code> <pre><code>def compute_loss(self, score: Tensor, noise: Tensor, t: Tensor,\n                 *args: Any, **kwargs: Any) -&gt; Tensor:\n    \"\"\"Compute loss between predicted score and actual noise.\n\n    Args:\n        score: The predicted noise tensor.\n        noise: The actual noise tensor.\n        t: Time steps tensor.\n        *args: Additional positional arguments.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        A tensor representing the computed loss.\n    \"\"\"\n    integral_beta = self.schedule.integral_beta(t, *args, **kwargs)\n    alpha_bar_t = torch.exp(-integral_beta)\n    sigma_t = torch.sqrt(1 - alpha_bar_t)\n    sigma_t = sigma_t.view(score.shape[0], *([1] * (score.dim() - 1)))\n    loss = (sigma_t * score + noise) ** 2\n    return loss.sum(dim=tuple(range(1, loss.dim())))\n</code></pre>"},{"location":"api/diffusion/vp/#image_gen.diffusion.vp.VariancePreserving.config","title":"<code>config()</code>","text":"<p>Get configuration parameters for the diffusion model.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing configuration parameters.</p> Source code in <code>image_gen\\diffusion\\vp.py</code> <pre><code>def config(self) -&gt; dict:\n    \"\"\"Get configuration parameters for the diffusion model.\n\n    Returns:\n        A dictionary containing configuration parameters.\n    \"\"\"\n    return self.schedule.config() if hasattr(self.schedule, \"config\") else {}\n</code></pre>"},{"location":"api/diffusion/vp/#image_gen.diffusion.vp.VariancePreserving.forward_process","title":"<code>forward_process(x0, t, *args, **kwargs)</code>","text":"<p>Apply the forward diffusion process.</p> <p>Adds noise to the input according to the variance preserving schedule.</p> <p>Parameters:</p> Name Type Description Default <code>x0</code> <code>Tensor</code> <p>The input tensor representing initial state.</p> required <code>t</code> <code>Tensor</code> <p>Time steps tensor.</p> required <code>*args</code> <code>Any</code> <p>Additional positional arguments.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor]</code> <p>A tuple of (noisy_sample, noise) tensors.</p> Source code in <code>image_gen\\diffusion\\vp.py</code> <pre><code>def forward_process(self, x0: Tensor, t: Tensor, *args: Any, **kwargs: Any) -&gt; Tuple[\n        Tensor, Tensor]:\n    \"\"\"Apply the forward diffusion process.\n\n    Adds noise to the input according to the variance preserving schedule.\n\n    Args:\n        x0: The input tensor representing initial state.\n        t: Time steps tensor.\n        *args: Additional positional arguments.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        A tuple of (noisy_sample, noise) tensors.\n    \"\"\"\n    integral = self.schedule.integral_beta(t, *args, **kwargs)\n    alpha_bar_t = torch.exp(-integral).view(-1, 1, 1, 1)\n\n    noise = torch.randn_like(x0)\n    xt = (torch.sqrt(alpha_bar_t) * x0 +\n          torch.sqrt(1.0 - alpha_bar_t) * noise)\n\n    return xt, noise\n</code></pre>"},{"location":"api/diffusion/vp/#image_gen.diffusion.vp.VariancePreserving.forward_sde","title":"<code>forward_sde(x, t, *args, **kwargs)</code>","text":"<p>Calculate drift and diffusion coefficients for forward SDE.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor representing current state.</p> required <code>t</code> <code>Tensor</code> <p>Time steps tensor.</p> required <code>*args</code> <code>Any</code> <p>Additional positional arguments.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor]</code> <p>A tuple of (drift, diffusion) tensors.</p> Source code in <code>image_gen\\diffusion\\vp.py</code> <pre><code>def forward_sde(self, x: Tensor, t: Tensor, *args: Any, **kwargs: Any) -&gt; Tuple[\n        Tensor, Tensor]:\n    \"\"\"Calculate drift and diffusion coefficients for forward SDE.\n\n    Args:\n        x: The input tensor representing current state.\n        t: Time steps tensor.\n        *args: Additional positional arguments.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        A tuple of (drift, diffusion) tensors.\n    \"\"\"\n    beta_t = self.schedule(t, *args, **kwargs).view(-1, 1, 1, 1)\n    drift = -0.5 * beta_t * x\n    diffusion = torch.sqrt(beta_t)\n    return drift, diffusion\n</code></pre>"},{"location":"api/metrics/","title":"Metrics","text":"<p>Metrics for evaluating the quality of generative models. These metrics help quantify the performance of the models.</p>"},{"location":"api/metrics/#base-metric","title":"Base Metric","text":"<p>The base class for all evaluation metrics. It defines the interface that all metric implementations must follow.</p> <p>Main Methods: - <code>__call__(real, generated)</code>: Computes the metric value between real and generated samples.</p> <p>View Implementation</p>"},{"location":"api/metrics/#bits-per-dimension","title":"Bits Per Dimension","text":"<p>Evaluates probabilistic generative models based on their log-likelihood. Lower values indicate better models.</p> <p>Main Methods: - <code>__call__(real, _)</code>: Computes bits per dimension for the real data.</p> <p>View Implementation</p>"},{"location":"api/metrics/#frechet-inception-distance","title":"Fr\u00e9chet Inception Distance","text":"<p>Measures the distance between feature representations of real and generated images using the Inception-v3 model. Lower values indicate better quality and diversity.</p> <p>Main Methods: - <code>_get_inception()</code>: Creates and prepares the Inception-v3 model for feature extraction. - <code>_get_activations(images)</code>: Extracts Inception features from input images. - <code>_calculate_fid(real, gen)</code>: Calculates the FID score from feature activations. - <code>__call__(real, generated)</code>: Computes the FID score between real and generated images.</p> <p>View Implementation</p>"},{"location":"api/metrics/#inception-score","title":"Inception Score","text":"<p>Evaluates the quality and diversity of generated images using the Inception-v3 model. Higher values indicate better quality and diversity.</p> <p>Main Methods: - <code>_get_inception()</code>: Creates and prepares the Inception-v3 model for feature extraction. - <code>_get_predictions(images)</code>: Gets softmax predictions from the Inception model. - <code>_calculate_is(predictions)</code>: Calculates the Inception Score from softmax predictions. - <code>__call__(_, generated)</code>: Computes the Inception Score for generated images.</p> <p>View Implementation</p>"},{"location":"api/metrics/base/","title":"Base Metric","text":"<p>Base metric class for image generation evaluation.</p>"},{"location":"api/metrics/base/#image_gen.metrics.base.BaseMetric","title":"<code>BaseMetric</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for all evaluation metrics.</p> <p>All metrics must inherit from this class and implement the required methods.</p> <p>Attributes:</p> Name Type Description <code>model</code> <p>The generative model being evaluated.</p> Source code in <code>image_gen\\metrics\\base.py</code> <pre><code>class BaseMetric(ABC):\n    \"\"\"Abstract base class for all evaluation metrics.\n\n    All metrics must inherit from this class and implement the required methods.\n\n    Attributes:\n        model: The generative model being evaluated.\n    \"\"\"\n\n    def __init__(self, model: GenerativeModel):\n        \"\"\"Initialize the metric with a generative model.\n\n        Args:\n            model: The generative model to be evaluated with this metric.\n        \"\"\"\n        self.model = model\n\n    @abstractmethod\n    def __call__(self, real: Tensor, generated: Tensor, *args: Any, **kwargs: Any) -&gt; float:\n        \"\"\"Compute the metric value between real and generated samples.\n\n        Args:\n            real: Tensor containing real samples.\n            generated: Tensor containing generated samples.\n            *args: Additional positional arguments.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            float: The computed metric value.\n        \"\"\"\n        pass\n\n    @property\n    @abstractmethod\n    def name(self) -&gt; str:\n        \"\"\"Get the name of the metric.\n\n        Returns:\n            str: The name of the metric.\n        \"\"\"\n        pass\n\n    @property\n    @abstractmethod\n    def is_lower_better(self) -&gt; bool:\n        \"\"\"Indicates whether a lower metric value is better.\n\n        Returns:\n            bool: True if lower values indicate better performance, False otherwise.\n        \"\"\"\n        pass\n\n    def config(self) -&gt; dict:\n        \"\"\"Get the configuration parameters for this metric.\n\n        Returns:\n            dict: Dictionary containing configuration parameters.\n        \"\"\"\n        return {}\n\n    def __str__(self) -&gt; str:\n        \"\"\"Get a string representation of the metric.\n\n        Returns:\n            str: String representation including class name and parameters.\n        \"\"\"\n        config = self.config()\n        params = \", \".join(f\"{k}: {v}\" for k, v in config.items())\n        return f\"{self._class_name}({params})\"\n\n    @property\n    def _class_name(self) -&gt; str:\n        \"\"\"Get the class name.\n\n        This property will be automatically overridden in custom classes made by users.\n\n        Returns:\n            str: The class name.\n        \"\"\"\n        return self.__class__.__name__\n</code></pre>"},{"location":"api/metrics/base/#image_gen.metrics.base.BaseMetric.is_lower_better","title":"<code>is_lower_better</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Indicates whether a lower metric value is better.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if lower values indicate better performance, False otherwise.</p>"},{"location":"api/metrics/base/#image_gen.metrics.base.BaseMetric.name","title":"<code>name</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Get the name of the metric.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The name of the metric.</p>"},{"location":"api/metrics/base/#image_gen.metrics.base.BaseMetric.__call__","title":"<code>__call__(real, generated, *args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Compute the metric value between real and generated samples.</p> <p>Parameters:</p> Name Type Description Default <code>real</code> <code>Tensor</code> <p>Tensor containing real samples.</p> required <code>generated</code> <code>Tensor</code> <p>Tensor containing generated samples.</p> required <code>*args</code> <code>Any</code> <p>Additional positional arguments.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The computed metric value.</p> Source code in <code>image_gen\\metrics\\base.py</code> <pre><code>@abstractmethod\ndef __call__(self, real: Tensor, generated: Tensor, *args: Any, **kwargs: Any) -&gt; float:\n    \"\"\"Compute the metric value between real and generated samples.\n\n    Args:\n        real: Tensor containing real samples.\n        generated: Tensor containing generated samples.\n        *args: Additional positional arguments.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        float: The computed metric value.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/metrics/base/#image_gen.metrics.base.BaseMetric.__init__","title":"<code>__init__(model)</code>","text":"<p>Initialize the metric with a generative model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>GenerativeModel</code> <p>The generative model to be evaluated with this metric.</p> required Source code in <code>image_gen\\metrics\\base.py</code> <pre><code>def __init__(self, model: GenerativeModel):\n    \"\"\"Initialize the metric with a generative model.\n\n    Args:\n        model: The generative model to be evaluated with this metric.\n    \"\"\"\n    self.model = model\n</code></pre>"},{"location":"api/metrics/base/#image_gen.metrics.base.BaseMetric.__str__","title":"<code>__str__()</code>","text":"<p>Get a string representation of the metric.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>String representation including class name and parameters.</p> Source code in <code>image_gen\\metrics\\base.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Get a string representation of the metric.\n\n    Returns:\n        str: String representation including class name and parameters.\n    \"\"\"\n    config = self.config()\n    params = \", \".join(f\"{k}: {v}\" for k, v in config.items())\n    return f\"{self._class_name}({params})\"\n</code></pre>"},{"location":"api/metrics/base/#image_gen.metrics.base.BaseMetric.config","title":"<code>config()</code>","text":"<p>Get the configuration parameters for this metric.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary containing configuration parameters.</p> Source code in <code>image_gen\\metrics\\base.py</code> <pre><code>def config(self) -&gt; dict:\n    \"\"\"Get the configuration parameters for this metric.\n\n    Returns:\n        dict: Dictionary containing configuration parameters.\n    \"\"\"\n    return {}\n</code></pre>"},{"location":"api/metrics/bpd/","title":"Bits Per Dimension","text":"<p>Module for Bits Per Dimension metric implementation.</p>"},{"location":"api/metrics/bpd/#image_gen.metrics.bpd.BitsPerDimension","title":"<code>BitsPerDimension</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Bits per dimension (BPD) metric for evaluating density models.</p> <p>This metric evaluates probabilistic generative models based on their log-likelihood. Lower values indicate better models.</p> <p>Attributes:</p> Name Type Description <code>model</code> <p>The generative model being evaluated.</p> Source code in <code>image_gen\\metrics\\bpd.py</code> <pre><code>class BitsPerDimension(BaseMetric):\n    \"\"\"Bits per dimension (BPD) metric for evaluating density models.\n\n    This metric evaluates probabilistic generative models based on their\n    log-likelihood. Lower values indicate better models.\n\n    Attributes:\n        model: The generative model being evaluated.\n    \"\"\"\n\n    def __call__(\n        self,\n        real: Union[Tensor, torch.utils.data.Dataset],\n        _generated: Any,\n        *_,\n        **__\n    ) -&gt; float:\n        \"\"\"Computes bits per dimension for the real data.\n\n        Args:\n            real: Tensor or Dataset-like object (Dataset, Subset, etc.)\n            _generated: Not used for BPD, included for API compatibility\n\n        Returns:\n            float: The computed BPD value (lower is better).\n        \"\"\"\n        # If input is not a Tensor, assume it's a Dataset-like and load it\n        if not isinstance(real, Tensor):\n            dataloader = DataLoader(real, batch_size=64, shuffle=False)\n            real = next(iter(dataloader))[0]  # Get first batch only\n\n        real = real.to(self.model.device)\n\n        # Scale images to [-1, 1] range if they're in [0, 1]\n        if real.min() &gt;= 0 and real.max() &lt;= 1:\n            real = real * 2 - 1\n\n        # We use the model's loss function as a proxy for NLL\n        with torch.no_grad():\n            # Sample multiple random times for more accurate estimate\n            losses = []\n            # Average over multiple time samples\n            for _ in range(10):\n                loss = self.model.loss_function(real)\n                losses.append(loss.detach().cpu())\n\n            # Take the mean loss\n            mean_loss = torch.stack(losses).mean()\n\n        # Convert to bits per dimension\n        batch_size, channels, height, width = real.shape\n        num_dims = channels * height * width\n        bpd = mean_loss / np.log(2) / num_dims\n\n        return bpd.item()\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"Get the name of the metric.\n\n        Returns:\n            str: The name of the metric.\n        \"\"\"\n        return \"Bits Per Dimension\"\n\n    @property\n    def is_lower_better(self) -&gt; bool:\n        \"\"\"Indicates whether a lower metric value is better.\n\n        Returns:\n            bool: True if lower values indicate better performance.\n        \"\"\"\n        return True\n</code></pre>"},{"location":"api/metrics/bpd/#image_gen.metrics.bpd.BitsPerDimension.is_lower_better","title":"<code>is_lower_better</code>  <code>property</code>","text":"<p>Indicates whether a lower metric value is better.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if lower values indicate better performance.</p>"},{"location":"api/metrics/bpd/#image_gen.metrics.bpd.BitsPerDimension.name","title":"<code>name</code>  <code>property</code>","text":"<p>Get the name of the metric.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The name of the metric.</p>"},{"location":"api/metrics/bpd/#image_gen.metrics.bpd.BitsPerDimension.__call__","title":"<code>__call__(real, _generated, *_, **__)</code>","text":"<p>Computes bits per dimension for the real data.</p> <p>Parameters:</p> Name Type Description Default <code>real</code> <code>Union[Tensor, Dataset]</code> <p>Tensor or Dataset-like object (Dataset, Subset, etc.)</p> required <code>_generated</code> <code>Any</code> <p>Not used for BPD, included for API compatibility</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The computed BPD value (lower is better).</p> Source code in <code>image_gen\\metrics\\bpd.py</code> <pre><code>def __call__(\n    self,\n    real: Union[Tensor, torch.utils.data.Dataset],\n    _generated: Any,\n    *_,\n    **__\n) -&gt; float:\n    \"\"\"Computes bits per dimension for the real data.\n\n    Args:\n        real: Tensor or Dataset-like object (Dataset, Subset, etc.)\n        _generated: Not used for BPD, included for API compatibility\n\n    Returns:\n        float: The computed BPD value (lower is better).\n    \"\"\"\n    # If input is not a Tensor, assume it's a Dataset-like and load it\n    if not isinstance(real, Tensor):\n        dataloader = DataLoader(real, batch_size=64, shuffle=False)\n        real = next(iter(dataloader))[0]  # Get first batch only\n\n    real = real.to(self.model.device)\n\n    # Scale images to [-1, 1] range if they're in [0, 1]\n    if real.min() &gt;= 0 and real.max() &lt;= 1:\n        real = real * 2 - 1\n\n    # We use the model's loss function as a proxy for NLL\n    with torch.no_grad():\n        # Sample multiple random times for more accurate estimate\n        losses = []\n        # Average over multiple time samples\n        for _ in range(10):\n            loss = self.model.loss_function(real)\n            losses.append(loss.detach().cpu())\n\n        # Take the mean loss\n        mean_loss = torch.stack(losses).mean()\n\n    # Convert to bits per dimension\n    batch_size, channels, height, width = real.shape\n    num_dims = channels * height * width\n    bpd = mean_loss / np.log(2) / num_dims\n\n    return bpd.item()\n</code></pre>"},{"location":"api/metrics/fid/","title":"Fr\u00e9chet Inception Distance","text":"<p>Module for Fr\u00e9chet Inception Distance (FID) metric implementation.</p>"},{"location":"api/metrics/fid/#image_gen.metrics.fid.FrechetInceptionDistance","title":"<code>FrechetInceptionDistance</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Fr\u00e9chet Inception Distance (FID) for evaluating generative models.</p> <p>This metric measures the distance between the feature representations of real and generated images using the Inception-v3 model. Lower values indicate better quality and diversity.</p> <p>Attributes:</p> Name Type Description <code>model</code> <p>The generative model being evaluated.</p> <code>dims</code> <p>Dimensionality of Inception features to use.</p> <code>inception</code> <p>Pretrained Inception-v3 model used for feature extraction.</p> Source code in <code>image_gen\\metrics\\fid.py</code> <pre><code>class FrechetInceptionDistance(BaseMetric):\n    \"\"\"Fr\u00e9chet Inception Distance (FID) for evaluating generative models.\n\n    This metric measures the distance between the feature representations\n    of real and generated images using the Inception-v3 model.\n    Lower values indicate better quality and diversity.\n\n    Attributes:\n        model: The generative model being evaluated.\n        dims: Dimensionality of Inception features to use.\n        inception: Pretrained Inception-v3 model used for feature extraction.\n    \"\"\"\n\n    def __init__(self, model: GenerativeModel, dims: int = 2048):\n        \"\"\"Initialize the FID metric with model and feature dimensions.\n\n        Args:\n            model: The generative model to be evaluated.\n            dims: The feature dimension to use from the Inception model.\n        \"\"\"\n        super().__init__(model)\n        self.dims = dims\n        self.inception = self._get_inception()\n\n    def _get_inception(self):\n        \"\"\"Create and prepare the Inception-v3 model for feature extraction.\n\n        Returns:\n            The prepared Inception-v3 model with identity final layer.\n        \"\"\"\n        inception = inception_v3(\n            weights=Inception_V3_Weights.IMAGENET1K_V1, transform_input=False)\n        inception.fc = nn.Identity()  # Remove final FC layer to get features\n        inception.eval()\n        inception.to(self.model.device)\n        return inception\n\n    def _get_activations(self, images: Tensor) -&gt; np.ndarray:\n        \"\"\"Extract Inception features from input images.\n\n        Args:\n            images: Batch of images to process.\n\n        Returns:\n            NumPy array containing feature activations.\n        \"\"\"\n        # Convert grayscale to RGB if needed\n        if images.shape[1] == 1:\n            images = images.repeat(1, 3, 1, 1)\n\n        # Resize images to Inception input size\n        if images.shape[2] != 299 or images.shape[3] != 299:\n            images = F.interpolate(\n                images,\n                size=(299, 299),\n                mode='bilinear',\n                align_corners=True\n            )\n\n        # Scale from [-1, 1] to [0, 1] range if needed\n        if images.min() &lt; 0:\n            images = (images + 1) / 2\n\n        # Ensure values are in [0, 1]\n        images = torch.clamp(images, 0, 1)\n\n        # Extract features in smaller batches to avoid OOM errors\n        batch_size = 32\n        activations = []\n\n        for i in range(0, images.shape[0], batch_size):\n            batch = images[i:i+batch_size]\n            with torch.no_grad():\n                try:\n                    batch_activations = self.inception(batch)\n                    activations.append(batch_activations)\n                except Exception as e:\n                    print(f\"Error processing batch: {e}\")\n                    # Return a fallback value if feature extraction fails\n                    return np.random.randn(images.shape[0], self.dims)\n\n        if not activations:\n            return np.random.randn(images.shape[0], self.dims)\n\n        activations = torch.cat(activations, 0)\n        return activations.detach().cpu().numpy()\n\n    def _calculate_fid(\n        self,\n        real_activations: np.ndarray,\n        gen_activations: np.ndarray\n    ) -&gt; float:\n        \"\"\"Calculate FID score from feature activations.\n\n        Args:\n            real_activations: Feature activations from real images.\n            gen_activations: Feature activations from generated images.\n\n        Returns:\n            The calculated FID score.\n        \"\"\"\n        # Calculate mean and covariance\n        mu1, sigma1 = real_activations.mean(\n            axis=0), np.cov(real_activations, rowvar=False)\n        mu2, sigma2 = gen_activations.mean(\n            axis=0), np.cov(gen_activations, rowvar=False)\n\n        # Handle numerical stability\n        eps = 1e-6\n        sigma1 += np.eye(sigma1.shape[0]) * eps\n        sigma2 += np.eye(sigma2.shape[0]) * eps\n\n        # Calculate FID\n        ssdiff = np.sum((mu1 - mu2) ** 2)\n\n        # Use scipy's sqrtm with error handling\n        try:\n            covmean_sqrt = linalg.sqrtm(sigma1.dot(sigma2), disp=False)[0]\n            if np.iscomplexobj(covmean_sqrt):\n                covmean_sqrt = covmean_sqrt.real\n            fid = ssdiff + np.trace(sigma1 + sigma2 - 2 * covmean_sqrt)\n        except Exception as e:\n            print(f\"Error calculating matrix square root: {e}\")\n            # Fallback to simpler calculation if sqrtm fails\n            fid = ssdiff + np.trace(sigma1 + sigma2)\n\n        return float(fid)\n\n    def __call__(\n        self,\n        real: Union[Tensor, torch.utils.data.Dataset],\n        generated: Union[Tensor, torch.utils.data.Dataset],\n        *_,\n        **__\n    ) -&gt; float:\n        \"\"\"Compute the FID score between real and generated images.\n\n        Args:\n            real: Tensor or Dataset-like containing real images (B, C, H, W).\n            generated: Tensor or Dataset-like containing generated images \n                      (B, C, H, W).\n\n        Returns:\n            The computed FID score (lower is better).\n        \"\"\"\n        def to_tensor(data):\n            \"\"\"Convert various inputs to tensors.\n\n            Args:\n                data: Input data (tensor or dataset).\n\n            Returns:\n                Tensor representation of the input data.\n            \"\"\"\n            if isinstance(data, Tensor):\n                return data\n            dataloader = DataLoader(data, batch_size=64, shuffle=False)\n            batch = next(iter(dataloader))\n            return batch[0] if isinstance(batch, (list, tuple)) else batch\n\n        real = to_tensor(real).to(self.model.device)\n        generated = to_tensor(generated).to(self.model.device)\n\n        # Process in batches to avoid memory issues\n        real_activations = self._get_activations(real)\n        gen_activations = self._get_activations(generated)\n\n        # Ensure we have enough samples for covariance calculation\n        if real_activations.shape[0] &lt; 2 or gen_activations.shape[0] &lt; 2:\n            print(\"Warning: Need at least 2 samples for FID calculation\")\n            return float('nan')\n\n        # If dimensions don't match (rare case), resize activations\n        if real_activations.shape[0] != gen_activations.shape[0]:\n            min_size = min(real_activations.shape[0], gen_activations.shape[0])\n            real_activations = real_activations[:min_size]\n            gen_activations = gen_activations[:min_size]\n\n        return self._calculate_fid(real_activations, gen_activations)\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"Get the name of the metric.\n\n        Returns:\n            The name of the metric.\n        \"\"\"\n        return \"Fr\u00e9chet Inception Distance\"\n\n    @property\n    def is_lower_better(self) -&gt; bool:\n        \"\"\"Indicates whether a lower metric value is better.\n\n        Returns:\n            True if lower values indicate better performance.\n        \"\"\"\n        return True\n</code></pre>"},{"location":"api/metrics/fid/#image_gen.metrics.fid.FrechetInceptionDistance.is_lower_better","title":"<code>is_lower_better</code>  <code>property</code>","text":"<p>Indicates whether a lower metric value is better.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if lower values indicate better performance.</p>"},{"location":"api/metrics/fid/#image_gen.metrics.fid.FrechetInceptionDistance.name","title":"<code>name</code>  <code>property</code>","text":"<p>Get the name of the metric.</p> <p>Returns:</p> Type Description <code>str</code> <p>The name of the metric.</p>"},{"location":"api/metrics/fid/#image_gen.metrics.fid.FrechetInceptionDistance.__call__","title":"<code>__call__(real, generated, *_, **__)</code>","text":"<p>Compute the FID score between real and generated images.</p> <p>Parameters:</p> Name Type Description Default <code>real</code> <code>Union[Tensor, Dataset]</code> <p>Tensor or Dataset-like containing real images (B, C, H, W).</p> required <code>generated</code> <code>Union[Tensor, Dataset]</code> <p>Tensor or Dataset-like containing generated images        (B, C, H, W).</p> required <p>Returns:</p> Type Description <code>float</code> <p>The computed FID score (lower is better).</p> Source code in <code>image_gen\\metrics\\fid.py</code> <pre><code>def __call__(\n    self,\n    real: Union[Tensor, torch.utils.data.Dataset],\n    generated: Union[Tensor, torch.utils.data.Dataset],\n    *_,\n    **__\n) -&gt; float:\n    \"\"\"Compute the FID score between real and generated images.\n\n    Args:\n        real: Tensor or Dataset-like containing real images (B, C, H, W).\n        generated: Tensor or Dataset-like containing generated images \n                  (B, C, H, W).\n\n    Returns:\n        The computed FID score (lower is better).\n    \"\"\"\n    def to_tensor(data):\n        \"\"\"Convert various inputs to tensors.\n\n        Args:\n            data: Input data (tensor or dataset).\n\n        Returns:\n            Tensor representation of the input data.\n        \"\"\"\n        if isinstance(data, Tensor):\n            return data\n        dataloader = DataLoader(data, batch_size=64, shuffle=False)\n        batch = next(iter(dataloader))\n        return batch[0] if isinstance(batch, (list, tuple)) else batch\n\n    real = to_tensor(real).to(self.model.device)\n    generated = to_tensor(generated).to(self.model.device)\n\n    # Process in batches to avoid memory issues\n    real_activations = self._get_activations(real)\n    gen_activations = self._get_activations(generated)\n\n    # Ensure we have enough samples for covariance calculation\n    if real_activations.shape[0] &lt; 2 or gen_activations.shape[0] &lt; 2:\n        print(\"Warning: Need at least 2 samples for FID calculation\")\n        return float('nan')\n\n    # If dimensions don't match (rare case), resize activations\n    if real_activations.shape[0] != gen_activations.shape[0]:\n        min_size = min(real_activations.shape[0], gen_activations.shape[0])\n        real_activations = real_activations[:min_size]\n        gen_activations = gen_activations[:min_size]\n\n    return self._calculate_fid(real_activations, gen_activations)\n</code></pre>"},{"location":"api/metrics/fid/#image_gen.metrics.fid.FrechetInceptionDistance.__init__","title":"<code>__init__(model, dims=2048)</code>","text":"<p>Initialize the FID metric with model and feature dimensions.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>GenerativeModel</code> <p>The generative model to be evaluated.</p> required <code>dims</code> <code>int</code> <p>The feature dimension to use from the Inception model.</p> <code>2048</code> Source code in <code>image_gen\\metrics\\fid.py</code> <pre><code>def __init__(self, model: GenerativeModel, dims: int = 2048):\n    \"\"\"Initialize the FID metric with model and feature dimensions.\n\n    Args:\n        model: The generative model to be evaluated.\n        dims: The feature dimension to use from the Inception model.\n    \"\"\"\n    super().__init__(model)\n    self.dims = dims\n    self.inception = self._get_inception()\n</code></pre>"},{"location":"api/metrics/inception/","title":"Inception Score","text":"<p>Module for Inception Score metric implementation.</p>"},{"location":"api/metrics/inception/#image_gen.metrics.inception.InceptionScore","title":"<code>InceptionScore</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Inception Score (IS) for evaluating generative models.</p> <p>This metric evaluates the quality and diversity of generated images using the Inception-v3 model's predictions. Higher values indicate better quality and diversity.</p> <p>Attributes:</p> Name Type Description <code>model</code> <p>The generative model being evaluated.</p> <code>n_splits</code> <p>Number of splits for estimating mean and standard deviation.</p> <code>inception</code> <p>Pretrained Inception-v3 model.</p> Source code in <code>image_gen\\metrics\\inception.py</code> <pre><code>class InceptionScore(BaseMetric):\n    \"\"\"Inception Score (IS) for evaluating generative models.\n\n    This metric evaluates the quality and diversity of generated images\n    using the Inception-v3 model's predictions.\n    Higher values indicate better quality and diversity.\n\n    Attributes:\n        model: The generative model being evaluated.\n        n_splits: Number of splits for estimating mean and standard deviation.\n        inception: Pretrained Inception-v3 model.\n    \"\"\"\n\n    def __init__(self, model: GenerativeModel, n_splits: int = 10):\n        \"\"\"Initialize the Inception Score metric.\n\n        Args:\n            model: The generative model to be evaluated.\n            n_splits: Number of splits for calculating score statistics.\n        \"\"\"\n        super().__init__(model)\n        self.n_splits = n_splits\n        self.inception = self._get_inception()\n\n    def _get_inception(self):\n        \"\"\"Create and prepare the Inception-v3 model.\n\n        Returns:\n            The prepared Inception-v3 model.\n        \"\"\"\n        inception = inception_v3(\n            weights=Inception_V3_Weights.IMAGENET1K_V1, transform_input=False)\n        inception.eval()\n        inception.to(self.model.device)\n        return inception\n\n    def _get_predictions(self, images: Tensor) -&gt; np.ndarray:\n        \"\"\"Get softmax predictions from the Inception model.\n\n        Args:\n            images: Batch of images to process.\n\n        Returns:\n            NumPy array containing softmax predictions.\n        \"\"\"\n        # Convert grayscale to RGB if needed\n        if images.shape[1] == 1:\n            images = images.repeat(1, 3, 1, 1)\n\n        # Resize images to Inception input size\n        if images.shape[2] != 299 or images.shape[3] != 299:\n            images = F.interpolate(\n                images,\n                size=(299, 299),\n                mode='bilinear',\n                align_corners=True\n            )\n\n        # Scale from [-1, 1] to [0, 1] range if needed\n        if images.min() &lt; 0:\n            images = (images + 1) / 2\n\n        # Ensure values are in [0, 1]\n        images = torch.clamp(images, 0, 1)\n\n        # Extract features in smaller batches to avoid OOM errors\n        batch_size = 32\n        predictions = []\n\n        for i in range(0, images.shape[0], batch_size):\n            batch = images[i:i+batch_size]\n            with torch.no_grad():\n                try:\n                    # Get predictions with error handling\n                    pred = self.inception(batch)\n                    pred = F.softmax(pred, dim=1)\n                    predictions.append(pred)\n                except Exception as e:\n                    print(f\"Error during inference: {e}\")\n                    # Return fallback predictions if inference fails\n                    return np.ones((images.shape[0], 1000)) / 1000\n\n        if not predictions:\n            return np.ones((images.shape[0], 1000)) / 1000\n\n        predictions = torch.cat(predictions, 0)\n        return predictions.detach().cpu().numpy()\n\n    def _calculate_is(self, predictions: np.ndarray) -&gt; Tuple[float, float]:\n        \"\"\"Calculate Inception Score from softmax predictions.\n\n        Args:\n            predictions: Softmax predictions from the Inception model.\n\n        Returns:\n            Tuple of (mean, std) of Inception Score.\n        \"\"\"\n        # Ensure we have enough samples for splitting\n        n_splits = min(self.n_splits, len(predictions) // 2)\n        if n_splits &lt; 1:\n            n_splits = 1\n\n        # Split predictions to calculate mean and std\n        scores = []\n        splits = np.array_split(predictions, n_splits)\n\n        for split in splits:\n            # Calculate KL divergence\n            p_y = np.mean(split, axis=0)\n            # Avoid log(0) by adding small epsilon\n            kl_divergences = split * (\n                np.log(split + 1e-10) - np.log(p_y + 1e-10)\n            )\n            kl_d = np.mean(np.sum(kl_divergences, axis=1))\n            scores.append(np.exp(kl_d))\n\n        if len(scores) == 1:\n            return float(scores[0]), 0.0\n        return float(np.mean(scores)), float(np.std(scores))\n\n    def __call__(\n        self,\n        _real: Any,\n        generated: Union[Tensor, torch.utils.data.Dataset],\n        *_,\n        **__\n    ) -&gt; float:\n        \"\"\"Compute the Inception Score for generated images.\n\n        Args:\n            _real: Not used for IS, included for API compatibility.\n            generated: Tensor of generated images (B, C, H, W).\n\n        Returns:\n            The computed Inception Score (higher is better).\n\n        Raises:\n            ValueError: If no generated images are provided.\n        \"\"\"\n        # Move to device\n        generated = generated.to(self.model.device)\n\n        # Ensure minimum batch size\n        if generated.shape[0] &lt; 2:\n            print(\"Warning: Need at least 2 samples for IS calculation\")\n            return 1.0  # Default score for insufficient samples\n\n        # Get predictions\n        all_predictions = self._get_predictions(generated)\n\n        # Calculate IS\n        is_mean, _ = self._calculate_is(all_predictions)\n\n        # Return just the mean for compatibility with the BaseMetric interface\n        return is_mean\n\n    def calculate_with_std(\n        self,\n        generated: Tensor\n    ) -&gt; Tuple[float, float]:\n        \"\"\"Calculate Inception Score with standard deviation.\n\n        This method provides additional information compared to __call__.\n\n        Args:\n            generated: Tensor of generated images.\n\n        Returns:\n            Tuple of (mean, std) of Inception Score.\n        \"\"\"\n        # Move to device\n        generated = generated.to(self.model.device)\n\n        # Get predictions\n        all_predictions = self._get_predictions(generated)\n\n        # Calculate IS with standard deviation\n        return self._calculate_is(all_predictions)\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"Get the name of the metric.\n\n        Returns:\n            The name of the metric.\n        \"\"\"\n        return \"Inception Score\"\n\n    @property\n    def is_lower_better(self) -&gt; bool:\n        \"\"\"Indicates whether a lower metric value is better.\n\n        Returns:\n            False since higher Inception Score values are better.\n        \"\"\"\n        return False\n</code></pre>"},{"location":"api/metrics/inception/#image_gen.metrics.inception.InceptionScore.is_lower_better","title":"<code>is_lower_better</code>  <code>property</code>","text":"<p>Indicates whether a lower metric value is better.</p> <p>Returns:</p> Type Description <code>bool</code> <p>False since higher Inception Score values are better.</p>"},{"location":"api/metrics/inception/#image_gen.metrics.inception.InceptionScore.name","title":"<code>name</code>  <code>property</code>","text":"<p>Get the name of the metric.</p> <p>Returns:</p> Type Description <code>str</code> <p>The name of the metric.</p>"},{"location":"api/metrics/inception/#image_gen.metrics.inception.InceptionScore.__call__","title":"<code>__call__(_real, generated, *_, **__)</code>","text":"<p>Compute the Inception Score for generated images.</p> <p>Parameters:</p> Name Type Description Default <code>_real</code> <code>Any</code> <p>Not used for IS, included for API compatibility.</p> required <code>generated</code> <code>Union[Tensor, Dataset]</code> <p>Tensor of generated images (B, C, H, W).</p> required <p>Returns:</p> Type Description <code>float</code> <p>The computed Inception Score (higher is better).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no generated images are provided.</p> Source code in <code>image_gen\\metrics\\inception.py</code> <pre><code>def __call__(\n    self,\n    _real: Any,\n    generated: Union[Tensor, torch.utils.data.Dataset],\n    *_,\n    **__\n) -&gt; float:\n    \"\"\"Compute the Inception Score for generated images.\n\n    Args:\n        _real: Not used for IS, included for API compatibility.\n        generated: Tensor of generated images (B, C, H, W).\n\n    Returns:\n        The computed Inception Score (higher is better).\n\n    Raises:\n        ValueError: If no generated images are provided.\n    \"\"\"\n    # Move to device\n    generated = generated.to(self.model.device)\n\n    # Ensure minimum batch size\n    if generated.shape[0] &lt; 2:\n        print(\"Warning: Need at least 2 samples for IS calculation\")\n        return 1.0  # Default score for insufficient samples\n\n    # Get predictions\n    all_predictions = self._get_predictions(generated)\n\n    # Calculate IS\n    is_mean, _ = self._calculate_is(all_predictions)\n\n    # Return just the mean for compatibility with the BaseMetric interface\n    return is_mean\n</code></pre>"},{"location":"api/metrics/inception/#image_gen.metrics.inception.InceptionScore.__init__","title":"<code>__init__(model, n_splits=10)</code>","text":"<p>Initialize the Inception Score metric.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>GenerativeModel</code> <p>The generative model to be evaluated.</p> required <code>n_splits</code> <code>int</code> <p>Number of splits for calculating score statistics.</p> <code>10</code> Source code in <code>image_gen\\metrics\\inception.py</code> <pre><code>def __init__(self, model: GenerativeModel, n_splits: int = 10):\n    \"\"\"Initialize the Inception Score metric.\n\n    Args:\n        model: The generative model to be evaluated.\n        n_splits: Number of splits for calculating score statistics.\n    \"\"\"\n    super().__init__(model)\n    self.n_splits = n_splits\n    self.inception = self._get_inception()\n</code></pre>"},{"location":"api/metrics/inception/#image_gen.metrics.inception.InceptionScore.calculate_with_std","title":"<code>calculate_with_std(generated)</code>","text":"<p>Calculate Inception Score with standard deviation.</p> <p>This method provides additional information compared to call.</p> <p>Parameters:</p> Name Type Description Default <code>generated</code> <code>Tensor</code> <p>Tensor of generated images.</p> required <p>Returns:</p> Type Description <code>Tuple[float, float]</code> <p>Tuple of (mean, std) of Inception Score.</p> Source code in <code>image_gen\\metrics\\inception.py</code> <pre><code>def calculate_with_std(\n    self,\n    generated: Tensor\n) -&gt; Tuple[float, float]:\n    \"\"\"Calculate Inception Score with standard deviation.\n\n    This method provides additional information compared to __call__.\n\n    Args:\n        generated: Tensor of generated images.\n\n    Returns:\n        Tuple of (mean, std) of Inception Score.\n    \"\"\"\n    # Move to device\n    generated = generated.to(self.model.device)\n\n    # Get predictions\n    all_predictions = self._get_predictions(generated)\n\n    # Calculate IS with standard deviation\n    return self._calculate_is(all_predictions)\n</code></pre>"},{"location":"api/noise/","title":"Noise Schedules","text":"<p>Noise schedules control how noise is added during the diffusion process. They determine the amount of noise added at each timestep.</p>"},{"location":"api/noise/#base-noise-schedule","title":"Base Noise Schedule","text":"<p>The base class for all noise schedules. It defines the interface for noise schedule implementations.</p> <p>Main Methods: - <code>__call__(t)</code>: Calculates noise magnitude at time <code>t</code>. - <code>integral_beta(t)</code>: Calculates the integral of the noise function up to time <code>t</code>.</p> <p>View Implementation</p>"},{"location":"api/noise/#linear-noise-schedule","title":"Linear Noise Schedule","text":"<p>A simple noise schedule where noise increases linearly from <code>beta_min</code> to <code>beta_max</code>.</p> <p>Main Methods: - <code>__call__(t)</code>: Returns linearly increasing noise values. - <code>integral_beta(t)</code>: Computes the integral of the linear noise function.</p> <p>View Implementation</p>"},{"location":"api/noise/#cosine-noise-schedule","title":"Cosine Noise Schedule","text":"<p>A noise schedule using a cosine function for smoother transitions between noise levels.</p> <p>Main Methods: - <code>__call__(t)</code>: Returns noise values following a cosine curve. - <code>integral_beta(t)</code>: Computes the integral of the cosine noise function.</p> <p>View Implementation</p>"},{"location":"api/noise/base/","title":"Base Noise Schedule","text":"<p>Base class for noise schedules in diffusion models.</p>"},{"location":"api/noise/base/#image_gen.noise.base.BaseNoiseSchedule","title":"<code>BaseNoiseSchedule</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class defining the interface for noise schedules.</p> <p>All noise schedule implementations should inherit from this class and implement the required abstract methods.</p> Source code in <code>image_gen\\noise\\base.py</code> <pre><code>class BaseNoiseSchedule(ABC):\n    \"\"\"Abstract base class defining the interface for noise schedules.\n\n    All noise schedule implementations should inherit from this class\n    and implement the required abstract methods.\n    \"\"\"\n\n    @abstractmethod\n    def __call__(self, t: Tensor, *args: Any, **kwargs: Any) -&gt; Tensor:\n        \"\"\"Calculate noise at specific timesteps.\n\n        Args:\n            t: Tensor containing timestep values.\n            *args: Additional positional arguments.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            Tensor: Noise values corresponding to the input timesteps.\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def integral_beta(self, t: Tensor, *args: Any, **kwargs: Any) -&gt; Tensor:\n        \"\"\"Calculate the integral of the noise function up to timestep t.\n\n        Args:\n            t: Tensor containing timestep values.\n            *args: Additional positional arguments.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            Tensor: Integrated noise values corresponding to the input timesteps.\n        \"\"\"\n        ...\n\n    def config(self) -&gt; dict:\n        \"\"\"Get the configuration parameters of the noise schedule.\n\n        Returns:\n            dict: Configuration parameters of the noise schedule.\n        \"\"\"\n        return {}\n\n    def __str__(self) -&gt; str:\n        \"\"\"Generate a string representation of the noise schedule.\n\n        Returns:\n            str: String representation including class name and parameters.\n        \"\"\"\n        config = self.config()\n        params = \", \".join(f\"{k}: {v}\" for k, v in config.items())\n        return f\"{self._class_name}({params})\"\n\n    @property\n    def _class_name(self) -&gt; str:\n        \"\"\"Get the class name of the noise schedule.\n\n        This property will be automatically overridden in custom classes\n        made by users.\n\n        Returns:\n            str: Name of the class.\n        \"\"\"\n        # This will be automatically overridden in custom classes made by users\n        return self.__class__.__name__\n</code></pre>"},{"location":"api/noise/base/#image_gen.noise.base.BaseNoiseSchedule.__call__","title":"<code>__call__(t, *args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Calculate noise at specific timesteps.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Tensor</code> <p>Tensor containing timestep values.</p> required <code>*args</code> <code>Any</code> <p>Additional positional arguments.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>Noise values corresponding to the input timesteps.</p> Source code in <code>image_gen\\noise\\base.py</code> <pre><code>@abstractmethod\ndef __call__(self, t: Tensor, *args: Any, **kwargs: Any) -&gt; Tensor:\n    \"\"\"Calculate noise at specific timesteps.\n\n    Args:\n        t: Tensor containing timestep values.\n        *args: Additional positional arguments.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        Tensor: Noise values corresponding to the input timesteps.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/noise/base/#image_gen.noise.base.BaseNoiseSchedule.__str__","title":"<code>__str__()</code>","text":"<p>Generate a string representation of the noise schedule.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>String representation including class name and parameters.</p> Source code in <code>image_gen\\noise\\base.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Generate a string representation of the noise schedule.\n\n    Returns:\n        str: String representation including class name and parameters.\n    \"\"\"\n    config = self.config()\n    params = \", \".join(f\"{k}: {v}\" for k, v in config.items())\n    return f\"{self._class_name}({params})\"\n</code></pre>"},{"location":"api/noise/base/#image_gen.noise.base.BaseNoiseSchedule.config","title":"<code>config()</code>","text":"<p>Get the configuration parameters of the noise schedule.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Configuration parameters of the noise schedule.</p> Source code in <code>image_gen\\noise\\base.py</code> <pre><code>def config(self) -&gt; dict:\n    \"\"\"Get the configuration parameters of the noise schedule.\n\n    Returns:\n        dict: Configuration parameters of the noise schedule.\n    \"\"\"\n    return {}\n</code></pre>"},{"location":"api/noise/base/#image_gen.noise.base.BaseNoiseSchedule.integral_beta","title":"<code>integral_beta(t, *args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Calculate the integral of the noise function up to timestep t.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Tensor</code> <p>Tensor containing timestep values.</p> required <code>*args</code> <code>Any</code> <p>Additional positional arguments.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>Integrated noise values corresponding to the input timesteps.</p> Source code in <code>image_gen\\noise\\base.py</code> <pre><code>@abstractmethod\ndef integral_beta(self, t: Tensor, *args: Any, **kwargs: Any) -&gt; Tensor:\n    \"\"\"Calculate the integral of the noise function up to timestep t.\n\n    Args:\n        t: Tensor containing timestep values.\n        *args: Additional positional arguments.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        Tensor: Integrated noise values corresponding to the input timesteps.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/noise/cosine/","title":"Cosine Noise Schedule","text":"<p>Implementation of a cosine noise schedule for diffusion models.</p> <p>This module provides a cosine-based noise scheduling as proposed in \"Improved Denoising Diffusion Probabilistic Models\" (Nichol &amp; Dhariwal, 2021). The schedule offers smoother transitions between noise levels.</p>"},{"location":"api/noise/cosine/#image_gen.noise.cosine.CosineNoiseSchedule","title":"<code>CosineNoiseSchedule</code>","text":"<p>               Bases: <code>BaseNoiseSchedule</code></p> <p>Cosine noise schedule implementation for diffusion models.</p> <p>This noise schedule uses a cosine function to define the noise levels over time, which typically results in better sample quality compared to linear schedules.</p> Source code in <code>image_gen\\noise\\cosine.py</code> <pre><code>class CosineNoiseSchedule(BaseNoiseSchedule):\n    \"\"\"Cosine noise schedule implementation for diffusion models.\n\n    This noise schedule uses a cosine function to define the noise levels over time,\n    which typically results in better sample quality compared to linear schedules.\n    \"\"\"\n\n    def __init__(\n        self,\n        *_,\n        s: float = 0.008,\n        beta_min: float = 1e-4,\n        beta_max: float = 20.0,\n        **__\n    ):\n        \"\"\"Initialize the cosine noise schedule.\n\n        Args:\n            s: Small offset to prevent alpha_bar(t) from being too small near t=1.\n                Defaults to 0.008.\n            beta_min: Minimum noise level for numerical stability.\n                Defaults to 0.0001.\n            beta_max: Maximum noise level for numerical stability.\n                Defaults to 20.0.\n        \"\"\"\n        self.s = s\n        self.min_beta = beta_min\n        self.max_beta = beta_max\n\n    def alpha_bar(self, t: Tensor) -&gt; Tensor:\n        \"\"\"Compute the cumulative product of (1-beta) up to time t.\n\n        Uses the cosine formula: alpha_bar(t) = cos^2((t/T + s)/(1 + s) * \u03c0/2)\n\n        Args:\n            t: Tensor of timesteps in [0, 1] range.\n\n        Returns:\n            Tensor: alpha_bar values at the specified timesteps.\n        \"\"\"\n        return torch.cos((t + self.s) / (1.0 + self.s) * math.pi * 0.5) ** 2\n\n    def __call__(self, t: Tensor, *_, **__) -&gt; Tensor:\n        \"\"\"Compute beta(t) at timestep t.\n\n        For cosine schedule, beta(t) is derived from the derivative of alpha_bar(t):\n        beta(t) = -d(log(alpha_bar))/dt = -d(alpha_bar)/dt / alpha_bar\n\n        Args:\n            t: Tensor of timesteps in [0, 1] range.\n\n        Returns:\n            Tensor: Beta values at specified timesteps.\n        \"\"\"\n        # Compute f(t) = (t + s)/(1 + s) * \u03c0/2\n        f_t = (t + self.s) / (1.0 + self.s) * math.pi * 0.5\n\n        # Compute d(alpha_bar)/dt = d(cos^2(f(t)))/dt\n        # = 2 * cos(f(t)) * (-sin(f(t))) * d(f(t))/dt\n        # = -\u03c0 * sin(f(t)) * cos(f(t)) / (1 + s)\n        dalpha_bar_dt = -math.pi * \\\n            torch.sin(f_t) * torch.cos(f_t) / (1.0 + self.s)\n\n        # Compute alpha_bar(t)\n        alpha_bar_t = self.alpha_bar(t)\n\n        # Compute beta(t) = -d(log(alpha_bar))/dt = -d(alpha_bar)/dt / alpha_bar\n        beta_t = -dalpha_bar_dt / torch.clamp(alpha_bar_t, min=1e-8)\n\n        # Ensure numerical stability\n        beta_t = torch.clamp(beta_t, min=self.min_beta, max=self.max_beta)\n\n        return beta_t\n\n    def integral_beta(self, t: Tensor, *_, **__) -&gt; Tensor:\n        \"\"\"Compute the integral of beta from 0 to t.\n\n        For cosine schedule, this equals -log(alpha_bar(t)) which represents\n        the total amount of noise added up to time t.\n\n        Args:\n            t: Tensor of timesteps in [0, 1] range.\n\n        Returns:\n            Tensor: Integrated beta values from 0 to t.\n        \"\"\"\n        alpha_bar_t = self.alpha_bar(t)\n        return -torch.log(torch.clamp(alpha_bar_t, min=1e-8))\n\n    def config(self) -&gt; dict:\n        \"\"\"Get the configuration parameters of the noise schedule.\n\n        Returns:\n            dict: Dictionary containing the configuration parameters.\n        \"\"\"\n        return {\n            \"s\": self.s,\n            \"min_beta\": self.min_beta,\n            \"max_beta\": self.max_beta\n        }\n</code></pre>"},{"location":"api/noise/cosine/#image_gen.noise.cosine.CosineNoiseSchedule.__call__","title":"<code>__call__(t, *_, **__)</code>","text":"<p>Compute beta(t) at timestep t.</p> <p>For cosine schedule, beta(t) is derived from the derivative of alpha_bar(t): beta(t) = -d(log(alpha_bar))/dt = -d(alpha_bar)/dt / alpha_bar</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Tensor</code> <p>Tensor of timesteps in [0, 1] range.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>Beta values at specified timesteps.</p> Source code in <code>image_gen\\noise\\cosine.py</code> <pre><code>def __call__(self, t: Tensor, *_, **__) -&gt; Tensor:\n    \"\"\"Compute beta(t) at timestep t.\n\n    For cosine schedule, beta(t) is derived from the derivative of alpha_bar(t):\n    beta(t) = -d(log(alpha_bar))/dt = -d(alpha_bar)/dt / alpha_bar\n\n    Args:\n        t: Tensor of timesteps in [0, 1] range.\n\n    Returns:\n        Tensor: Beta values at specified timesteps.\n    \"\"\"\n    # Compute f(t) = (t + s)/(1 + s) * \u03c0/2\n    f_t = (t + self.s) / (1.0 + self.s) * math.pi * 0.5\n\n    # Compute d(alpha_bar)/dt = d(cos^2(f(t)))/dt\n    # = 2 * cos(f(t)) * (-sin(f(t))) * d(f(t))/dt\n    # = -\u03c0 * sin(f(t)) * cos(f(t)) / (1 + s)\n    dalpha_bar_dt = -math.pi * \\\n        torch.sin(f_t) * torch.cos(f_t) / (1.0 + self.s)\n\n    # Compute alpha_bar(t)\n    alpha_bar_t = self.alpha_bar(t)\n\n    # Compute beta(t) = -d(log(alpha_bar))/dt = -d(alpha_bar)/dt / alpha_bar\n    beta_t = -dalpha_bar_dt / torch.clamp(alpha_bar_t, min=1e-8)\n\n    # Ensure numerical stability\n    beta_t = torch.clamp(beta_t, min=self.min_beta, max=self.max_beta)\n\n    return beta_t\n</code></pre>"},{"location":"api/noise/cosine/#image_gen.noise.cosine.CosineNoiseSchedule.__init__","title":"<code>__init__(*_, s=0.008, beta_min=0.0001, beta_max=20.0, **__)</code>","text":"<p>Initialize the cosine noise schedule.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>float</code> <p>Small offset to prevent alpha_bar(t) from being too small near t=1. Defaults to 0.008.</p> <code>0.008</code> <code>beta_min</code> <code>float</code> <p>Minimum noise level for numerical stability. Defaults to 0.0001.</p> <code>0.0001</code> <code>beta_max</code> <code>float</code> <p>Maximum noise level for numerical stability. Defaults to 20.0.</p> <code>20.0</code> Source code in <code>image_gen\\noise\\cosine.py</code> <pre><code>def __init__(\n    self,\n    *_,\n    s: float = 0.008,\n    beta_min: float = 1e-4,\n    beta_max: float = 20.0,\n    **__\n):\n    \"\"\"Initialize the cosine noise schedule.\n\n    Args:\n        s: Small offset to prevent alpha_bar(t) from being too small near t=1.\n            Defaults to 0.008.\n        beta_min: Minimum noise level for numerical stability.\n            Defaults to 0.0001.\n        beta_max: Maximum noise level for numerical stability.\n            Defaults to 20.0.\n    \"\"\"\n    self.s = s\n    self.min_beta = beta_min\n    self.max_beta = beta_max\n</code></pre>"},{"location":"api/noise/cosine/#image_gen.noise.cosine.CosineNoiseSchedule.alpha_bar","title":"<code>alpha_bar(t)</code>","text":"<p>Compute the cumulative product of (1-beta) up to time t.</p> <p>Uses the cosine formula: alpha_bar(t) = cos^2((t/T + s)/(1 + s) * \u03c0/2)</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Tensor</code> <p>Tensor of timesteps in [0, 1] range.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>alpha_bar values at the specified timesteps.</p> Source code in <code>image_gen\\noise\\cosine.py</code> <pre><code>def alpha_bar(self, t: Tensor) -&gt; Tensor:\n    \"\"\"Compute the cumulative product of (1-beta) up to time t.\n\n    Uses the cosine formula: alpha_bar(t) = cos^2((t/T + s)/(1 + s) * \u03c0/2)\n\n    Args:\n        t: Tensor of timesteps in [0, 1] range.\n\n    Returns:\n        Tensor: alpha_bar values at the specified timesteps.\n    \"\"\"\n    return torch.cos((t + self.s) / (1.0 + self.s) * math.pi * 0.5) ** 2\n</code></pre>"},{"location":"api/noise/cosine/#image_gen.noise.cosine.CosineNoiseSchedule.config","title":"<code>config()</code>","text":"<p>Get the configuration parameters of the noise schedule.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary containing the configuration parameters.</p> Source code in <code>image_gen\\noise\\cosine.py</code> <pre><code>def config(self) -&gt; dict:\n    \"\"\"Get the configuration parameters of the noise schedule.\n\n    Returns:\n        dict: Dictionary containing the configuration parameters.\n    \"\"\"\n    return {\n        \"s\": self.s,\n        \"min_beta\": self.min_beta,\n        \"max_beta\": self.max_beta\n    }\n</code></pre>"},{"location":"api/noise/cosine/#image_gen.noise.cosine.CosineNoiseSchedule.integral_beta","title":"<code>integral_beta(t, *_, **__)</code>","text":"<p>Compute the integral of beta from 0 to t.</p> <p>For cosine schedule, this equals -log(alpha_bar(t)) which represents the total amount of noise added up to time t.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Tensor</code> <p>Tensor of timesteps in [0, 1] range.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>Integrated beta values from 0 to t.</p> Source code in <code>image_gen\\noise\\cosine.py</code> <pre><code>def integral_beta(self, t: Tensor, *_, **__) -&gt; Tensor:\n    \"\"\"Compute the integral of beta from 0 to t.\n\n    For cosine schedule, this equals -log(alpha_bar(t)) which represents\n    the total amount of noise added up to time t.\n\n    Args:\n        t: Tensor of timesteps in [0, 1] range.\n\n    Returns:\n        Tensor: Integrated beta values from 0 to t.\n    \"\"\"\n    alpha_bar_t = self.alpha_bar(t)\n    return -torch.log(torch.clamp(alpha_bar_t, min=1e-8))\n</code></pre>"},{"location":"api/noise/linear/","title":"Linear Noise Schedule","text":"<p>Implementation of a linear noise schedule for diffusion models.</p>"},{"location":"api/noise/linear/#image_gen.noise.linear.LinearNoiseSchedule","title":"<code>LinearNoiseSchedule</code>","text":"<p>               Bases: <code>BaseNoiseSchedule</code></p> <p>Linear noise schedule that increases linearly over time.</p> <p>This class implements a simple linear noise schedule where the noise increases linearly from beta_min to beta_max over the diffusion process.</p> Source code in <code>image_gen\\noise\\linear.py</code> <pre><code>class LinearNoiseSchedule(BaseNoiseSchedule):\n    \"\"\"Linear noise schedule that increases linearly over time.\n\n    This class implements a simple linear noise schedule where the noise\n    increases linearly from beta_min to beta_max over the diffusion process.\n    \"\"\"\n\n    def __init__(\n        self,\n        *_,\n        beta_min: float = 0.0001,\n        beta_max: float = 20.0,\n        **__\n    ):\n        \"\"\"Initialize the linear noise schedule.\n\n        Args:\n            beta_min: Minimum noise value at t=0. Defaults to 0.0001.\n            beta_max: Maximum noise value at t=1. Defaults to 20.0.\n        \"\"\"\n        self.beta_min = beta_min\n        self.beta_max = beta_max\n\n    def __call__(self, t: Tensor, *_, **__) -&gt; Tensor:\n        \"\"\"Calculate noise at specific timesteps.\n\n        Args:\n            t: Tensor containing timestep values in range [0, 1].\n\n        Returns:\n            Tensor: Noise values corresponding to the input timesteps.\n        \"\"\"\n        return self.beta_min + t * (self.beta_max - self.beta_min)\n\n    def integral_beta(self, t: Tensor, *_, **__) -&gt; Tensor:\n        \"\"\"Calculate the integral of the noise function up to timestep t.\n\n        The analytical solution for the integral of a linear function\n        from 0 to t is: beta_min * t + 0.5 * (beta_max - beta_min) * t^2.\n\n        Args:\n            t: Tensor containing timestep values in range [0, 1].\n\n        Returns:\n            Tensor: Integrated noise values corresponding to the input timesteps.\n        \"\"\"\n        return self.beta_min * t + 0.5 * (self.beta_max - self.beta_min) * (t ** 2)\n\n    def config(self) -&gt; dict:\n        \"\"\"Get the configuration parameters of the noise schedule.\n\n        Returns:\n            dict: Dictionary containing the configuration parameters.\n        \"\"\"\n        return {\n            \"beta_min\": self.beta_min,\n            \"beta_max\": self.beta_max,\n        }\n</code></pre>"},{"location":"api/noise/linear/#image_gen.noise.linear.LinearNoiseSchedule.__call__","title":"<code>__call__(t, *_, **__)</code>","text":"<p>Calculate noise at specific timesteps.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Tensor</code> <p>Tensor containing timestep values in range [0, 1].</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>Noise values corresponding to the input timesteps.</p> Source code in <code>image_gen\\noise\\linear.py</code> <pre><code>def __call__(self, t: Tensor, *_, **__) -&gt; Tensor:\n    \"\"\"Calculate noise at specific timesteps.\n\n    Args:\n        t: Tensor containing timestep values in range [0, 1].\n\n    Returns:\n        Tensor: Noise values corresponding to the input timesteps.\n    \"\"\"\n    return self.beta_min + t * (self.beta_max - self.beta_min)\n</code></pre>"},{"location":"api/noise/linear/#image_gen.noise.linear.LinearNoiseSchedule.__init__","title":"<code>__init__(*_, beta_min=0.0001, beta_max=20.0, **__)</code>","text":"<p>Initialize the linear noise schedule.</p> <p>Parameters:</p> Name Type Description Default <code>beta_min</code> <code>float</code> <p>Minimum noise value at t=0. Defaults to 0.0001.</p> <code>0.0001</code> <code>beta_max</code> <code>float</code> <p>Maximum noise value at t=1. Defaults to 20.0.</p> <code>20.0</code> Source code in <code>image_gen\\noise\\linear.py</code> <pre><code>def __init__(\n    self,\n    *_,\n    beta_min: float = 0.0001,\n    beta_max: float = 20.0,\n    **__\n):\n    \"\"\"Initialize the linear noise schedule.\n\n    Args:\n        beta_min: Minimum noise value at t=0. Defaults to 0.0001.\n        beta_max: Maximum noise value at t=1. Defaults to 20.0.\n    \"\"\"\n    self.beta_min = beta_min\n    self.beta_max = beta_max\n</code></pre>"},{"location":"api/noise/linear/#image_gen.noise.linear.LinearNoiseSchedule.config","title":"<code>config()</code>","text":"<p>Get the configuration parameters of the noise schedule.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary containing the configuration parameters.</p> Source code in <code>image_gen\\noise\\linear.py</code> <pre><code>def config(self) -&gt; dict:\n    \"\"\"Get the configuration parameters of the noise schedule.\n\n    Returns:\n        dict: Dictionary containing the configuration parameters.\n    \"\"\"\n    return {\n        \"beta_min\": self.beta_min,\n        \"beta_max\": self.beta_max,\n    }\n</code></pre>"},{"location":"api/noise/linear/#image_gen.noise.linear.LinearNoiseSchedule.integral_beta","title":"<code>integral_beta(t, *_, **__)</code>","text":"<p>Calculate the integral of the noise function up to timestep t.</p> <p>The analytical solution for the integral of a linear function from 0 to t is: beta_min * t + 0.5 * (beta_max - beta_min) * t^2.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Tensor</code> <p>Tensor containing timestep values in range [0, 1].</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>Integrated noise values corresponding to the input timesteps.</p> Source code in <code>image_gen\\noise\\linear.py</code> <pre><code>def integral_beta(self, t: Tensor, *_, **__) -&gt; Tensor:\n    \"\"\"Calculate the integral of the noise function up to timestep t.\n\n    The analytical solution for the integral of a linear function\n    from 0 to t is: beta_min * t + 0.5 * (beta_max - beta_min) * t^2.\n\n    Args:\n        t: Tensor containing timestep values in range [0, 1].\n\n    Returns:\n        Tensor: Integrated noise values corresponding to the input timesteps.\n    \"\"\"\n    return self.beta_min * t + 0.5 * (self.beta_max - self.beta_min) * (t ** 2)\n</code></pre>"},{"location":"api/samplers/","title":"Samplers","text":"<p>Samplers implement different algorithms for generating samples from the learned distribution. They define how noise is removed during the sampling process.</p>"},{"location":"api/samplers/#base-sampler","title":"Base Sampler","text":"<p>The base class for all samplers. It defines the interface that all sampler implementations must follow.</p> <p>Main Methods: - <code>__call__(x_T, score_model, ...)</code>: Performs the sampling process from initial noise <code>x_T</code>.</p> <p>View Implementation</p>"},{"location":"api/samplers/#euler-maruyama","title":"Euler-Maruyama","text":"<p>Implements the Euler-Maruyama numerical method for solving stochastic differential equations.</p> <p>Main Methods: - <code>__call__(x_T, score_model, ...)</code>: Performs sampling using the Euler-Maruyama method.</p> <p>View Implementation</p>"},{"location":"api/samplers/#predictor-corrector","title":"Predictor-Corrector","text":"<p>Combines a predictor step with a corrector step based on Langevin dynamics for improved sampling quality.</p> <p>Main Methods: - <code>predictor_step(x_t, t_curr, ...)</code>: Performs a prediction step. - <code>corrector_step(x_t, t, ...)</code>: Performs a correction step. - <code>__call__(x_T, score_model, ...)</code>: Performs sampling using the predictor-corrector method.</p> <p>View Implementation</p>"},{"location":"api/samplers/#ode-probability-flow","title":"ODE Probability Flow","text":"<p>A deterministic sampling method based on the probability flow ordinary differential equation.</p> <p>Main Methods: - <code>__call__(x_T, score_model, ...)</code>: Performs sampling using the ODE probability flow method.</p> <p>View Implementation</p>"},{"location":"api/samplers/#exponential-integrator","title":"Exponential Integrator","text":"<p>An exponential integration scheme for solving stochastic differential equations with better stability properties.</p> <p>Main Methods: - <code>__call__(x_T, score_model, ...)</code>: Performs sampling using the exponential integrator method.</p> <p>View Implementation</p>"},{"location":"api/samplers/base/","title":"Base Sampler","text":"<p>Base sampler class for diffusion models.</p> <p>This module provides a base abstract class for all samplers used in diffusion models. It defines the common interface that all samplers should implement.</p>"},{"location":"api/samplers/base/#image_gen.samplers.base.BaseSampler","title":"<code>BaseSampler</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for all diffusion model samplers.</p> <p>All samplers inherit from this class and must implement the call method which performs the actual sampling process.</p> <p>Attributes:</p> Name Type Description <code>diffusion</code> <p>The diffusion model to sample from.</p> <code>verbose</code> <p>Whether to print progress information during sampling.</p> Source code in <code>image_gen\\samplers\\base.py</code> <pre><code>class BaseSampler(ABC):\n    \"\"\"Abstract base class for all diffusion model samplers.\n\n    All samplers inherit from this class and must implement the call method\n    which performs the actual sampling process.\n\n    Attributes:\n        diffusion: The diffusion model to sample from.\n        verbose: Whether to print progress information during sampling.\n    \"\"\"\n\n    def __init__(self, diffusion: BaseDiffusion, *_, verbose: bool = True, **__):\n        \"\"\"Initialize the sampler.\n\n        Args:\n            diffusion: The diffusion model to sample from.\n            verbose: Whether to print progress information during sampling.\n                Defaults to True.\n        \"\"\"\n        self.diffusion = diffusion\n        self.verbose = verbose\n\n    @abstractmethod\n    def __call__(\n            self,\n            x_T: Tensor,\n            score_model: Callable,\n            *args: Any,\n            n_steps: int = 500,\n            seed: Optional[int] = None,\n            callback: Optional[Callable[[Tensor, int], None]] = None,\n            callback_frequency: int = 50,\n            guidance: Optional[Callable[[\n                Tensor, Tensor, Tensor], Tensor]] = None,\n            **kwargs: Any\n    ) -&gt; Tensor:\n        \"\"\"Perform the sampling process.\n\n        Args:\n            x_T: The initial noise tensor to start sampling from.\n            score_model: The score model function that predicts the score.\n            *args: Additional positional arguments.\n            n_steps: Number of sampling steps. Defaults to 500.\n            seed: Random seed for reproducibility. Defaults to None.\n            callback: Optional function called during sampling to monitor progress.\n                It takes the current sample and step number as inputs.\n                Defaults to None.\n            callback_frequency: How often to call the callback function.\n                Defaults to 50.\n            guidance: Optional guidance function for conditional sampling.\n                Defaults to None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            A tuple containing the final sample and the sequence of all samples.\n        \"\"\"\n        ...\n\n    def config(self) -&gt; dict:\n        \"\"\"Return the configuration of the sampler.\n\n        Returns:\n            A dictionary with the sampler's configuration parameters.\n        \"\"\"\n        return {}\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return a string representation of the sampler.\n\n        Returns:\n            A string with the sampler's class name and its configuration.\n        \"\"\"\n        config = self.config()\n        params = \", \".join(f\"{k}: {v}\" for k, v in config.items())\n        return f\"{self._class_name}({params})\"\n\n    @property\n    def _class_name(self) -&gt; str:\n        \"\"\"Get the class name of the sampler.\n\n        This property is automatically overridden in custom classes made by users.\n\n        Returns:\n            The name of the sampler class.\n        \"\"\"\n        return self.__class__.__name__\n</code></pre>"},{"location":"api/samplers/base/#image_gen.samplers.base.BaseSampler.__call__","title":"<code>__call__(x_T, score_model, *args, n_steps=500, seed=None, callback=None, callback_frequency=50, guidance=None, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Perform the sampling process.</p> <p>Parameters:</p> Name Type Description Default <code>x_T</code> <code>Tensor</code> <p>The initial noise tensor to start sampling from.</p> required <code>score_model</code> <code>Callable</code> <p>The score model function that predicts the score.</p> required <code>*args</code> <code>Any</code> <p>Additional positional arguments.</p> <code>()</code> <code>n_steps</code> <code>int</code> <p>Number of sampling steps. Defaults to 500.</p> <code>500</code> <code>seed</code> <code>Optional[int]</code> <p>Random seed for reproducibility. Defaults to None.</p> <code>None</code> <code>callback</code> <code>Optional[Callable[[Tensor, int], None]]</code> <p>Optional function called during sampling to monitor progress. It takes the current sample and step number as inputs. Defaults to None.</p> <code>None</code> <code>callback_frequency</code> <code>int</code> <p>How often to call the callback function. Defaults to 50.</p> <code>50</code> <code>guidance</code> <code>Optional[Callable[[Tensor, Tensor, Tensor], Tensor]]</code> <p>Optional guidance function for conditional sampling. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>A tuple containing the final sample and the sequence of all samples.</p> Source code in <code>image_gen\\samplers\\base.py</code> <pre><code>@abstractmethod\ndef __call__(\n        self,\n        x_T: Tensor,\n        score_model: Callable,\n        *args: Any,\n        n_steps: int = 500,\n        seed: Optional[int] = None,\n        callback: Optional[Callable[[Tensor, int], None]] = None,\n        callback_frequency: int = 50,\n        guidance: Optional[Callable[[\n            Tensor, Tensor, Tensor], Tensor]] = None,\n        **kwargs: Any\n) -&gt; Tensor:\n    \"\"\"Perform the sampling process.\n\n    Args:\n        x_T: The initial noise tensor to start sampling from.\n        score_model: The score model function that predicts the score.\n        *args: Additional positional arguments.\n        n_steps: Number of sampling steps. Defaults to 500.\n        seed: Random seed for reproducibility. Defaults to None.\n        callback: Optional function called during sampling to monitor progress.\n            It takes the current sample and step number as inputs.\n            Defaults to None.\n        callback_frequency: How often to call the callback function.\n            Defaults to 50.\n        guidance: Optional guidance function for conditional sampling.\n            Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        A tuple containing the final sample and the sequence of all samples.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/samplers/base/#image_gen.samplers.base.BaseSampler.__init__","title":"<code>__init__(diffusion, *_, verbose=True, **__)</code>","text":"<p>Initialize the sampler.</p> <p>Parameters:</p> Name Type Description Default <code>diffusion</code> <code>BaseDiffusion</code> <p>The diffusion model to sample from.</p> required <code>verbose</code> <code>bool</code> <p>Whether to print progress information during sampling. Defaults to True.</p> <code>True</code> Source code in <code>image_gen\\samplers\\base.py</code> <pre><code>def __init__(self, diffusion: BaseDiffusion, *_, verbose: bool = True, **__):\n    \"\"\"Initialize the sampler.\n\n    Args:\n        diffusion: The diffusion model to sample from.\n        verbose: Whether to print progress information during sampling.\n            Defaults to True.\n    \"\"\"\n    self.diffusion = diffusion\n    self.verbose = verbose\n</code></pre>"},{"location":"api/samplers/base/#image_gen.samplers.base.BaseSampler.__str__","title":"<code>__str__()</code>","text":"<p>Return a string representation of the sampler.</p> <p>Returns:</p> Type Description <code>str</code> <p>A string with the sampler's class name and its configuration.</p> Source code in <code>image_gen\\samplers\\base.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return a string representation of the sampler.\n\n    Returns:\n        A string with the sampler's class name and its configuration.\n    \"\"\"\n    config = self.config()\n    params = \", \".join(f\"{k}: {v}\" for k, v in config.items())\n    return f\"{self._class_name}({params})\"\n</code></pre>"},{"location":"api/samplers/base/#image_gen.samplers.base.BaseSampler.config","title":"<code>config()</code>","text":"<p>Return the configuration of the sampler.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary with the sampler's configuration parameters.</p> Source code in <code>image_gen\\samplers\\base.py</code> <pre><code>def config(self) -&gt; dict:\n    \"\"\"Return the configuration of the sampler.\n\n    Returns:\n        A dictionary with the sampler's configuration parameters.\n    \"\"\"\n    return {}\n</code></pre>"},{"location":"api/samplers/euler_maruyama/","title":"Euler-Maruyama","text":"<p>Euler-Maruyama sampler implementation for diffusion models.</p> <p>This module provides an implementation of the Euler-Maruyama numerical method for sampling from diffusion models by solving the associated stochastic differential equation (SDE).</p>"},{"location":"api/samplers/euler_maruyama/#image_gen.samplers.euler_maruyama.EulerMaruyama","title":"<code>EulerMaruyama</code>","text":"<p>               Bases: <code>BaseSampler</code></p> <p>Euler-Maruyama numerical sampler for diffusion models.</p> <p>This sampler implements the Euler-Maruyama numerical scheme for solving the stochastic differential equation (SDE) associated with the reverse diffusion process.</p> Source code in <code>image_gen\\samplers\\euler_maruyama.py</code> <pre><code>class EulerMaruyama(BaseSampler):\n    \"\"\"Euler-Maruyama numerical sampler for diffusion models.\n\n    This sampler implements the Euler-Maruyama numerical scheme for solving\n    the stochastic differential equation (SDE) associated with the reverse\n    diffusion process.\n    \"\"\"\n\n    def __call__(\n            self,\n            x_T: Tensor,\n            score_model: Callable,\n            *_,\n            n_steps: int = 500,\n            seed: Optional[int] = None,\n            callback: Optional[Callable[[Tensor, int], None]] = None,\n            callback_frequency: int = 50,\n            guidance: Optional[Callable[[Tensor, Tensor], Tensor]] = None,\n            **__\n    ) -&gt; Tensor:\n        \"\"\"Perform sampling using the Euler-Maruyama method.\n\n        Args:\n            x_T: The initial noise tensor to start sampling from.\n            score_model: The score model function that predicts the score.\n            n_steps: Number of sampling steps. Defaults to 500.\n            seed: Random seed for reproducibility. Defaults to None.\n            callback: Optional function called during sampling to monitor \n                progress. It takes the current sample and step number as inputs.\n                Defaults to None.\n            callback_frequency: How often to call the callback function.\n                Defaults to 50.\n            guidance: Optional guidance function for conditional sampling.\n                Defaults to None.\n\n        Returns:\n            A tuple containing the final sample tensor and the final sample \n            tensor again (for compatibility with the base class interface).\n        \"\"\"\n        if seed is not None:\n            torch.manual_seed(seed)\n\n        device = x_T.device\n        x_t = x_T.clone()\n\n        # Create linearly spaced timesteps from 1.0 to 1e-3\n        times = torch.linspace(1.0, 1e-3, n_steps + 1, device=device)\n        dt = times[0] - times[1]\n\n        # Create progress bar if verbose mode is enabled\n        iterable = (\n            tqdm(range(n_steps), desc='Generating')\n            if self.verbose else range(n_steps)\n        )\n\n        for i in iterable:\n            t_curr = times[i]\n            t_batch = torch.full((x_T.shape[0],), t_curr, device=device)\n            t_for_score = t_batch\n\n            # Handle NaN/Inf values to prevent numerical instability\n            if torch.isnan(x_t).any() or torch.isinf(x_t).any():\n                if self.verbose:\n                    print(\n                        f\"Warning: NaN or Inf values detected in x_t at step {i}\"\n                    )\n                x_t = torch.nan_to_num(x_t, nan=0.0, posinf=1.0, neginf=-1.0)\n\n            try:\n                # Create a fresh detached copy for gradient computation\n                x_t_detached = x_t.detach().clone()\n                x_t_detached.requires_grad_(True)\n                score = score_model(x_t_detached, t_for_score)\n            except Exception as e:\n                print(f\"Error computing score at step {i}, t={t_curr}: {e}\")\n                score = torch.zeros_like(x_t)\n\n            # Compute drift and diffusion terms for the SDE\n            drift, diffusion = self.diffusion.backward_sde(\n                x_t, t_batch, score, n_steps=n_steps\n            )\n\n            # Handle numerical stability for the diffusion term\n            diffusion = torch.nan_to_num(diffusion, nan=1e-4)\n            noise = torch.randn_like(x_t)\n\n            # Update x_t using the Euler-Maruyama update rule\n            x_t = (\n                x_t +\n                drift * (-dt) +\n                diffusion * torch.sqrt(torch.abs(dt)) * noise\n            )\n\n            # Apply guidance if provided\n            if guidance is not None:\n                x_t = guidance(x_t, t_curr)\n\n            # Clamp values to prevent extreme values\n            x_t = torch.clamp(x_t, -10.0, 10.0)\n\n            # Call callback if provided and at the right frequency\n            if callback and i % callback_frequency == 0:\n                callback(x_t.detach().clone(), i)\n\n        return x_t\n</code></pre>"},{"location":"api/samplers/euler_maruyama/#image_gen.samplers.euler_maruyama.EulerMaruyama.__call__","title":"<code>__call__(x_T, score_model, *_, n_steps=500, seed=None, callback=None, callback_frequency=50, guidance=None, **__)</code>","text":"<p>Perform sampling using the Euler-Maruyama method.</p> <p>Parameters:</p> Name Type Description Default <code>x_T</code> <code>Tensor</code> <p>The initial noise tensor to start sampling from.</p> required <code>score_model</code> <code>Callable</code> <p>The score model function that predicts the score.</p> required <code>n_steps</code> <code>int</code> <p>Number of sampling steps. Defaults to 500.</p> <code>500</code> <code>seed</code> <code>Optional[int]</code> <p>Random seed for reproducibility. Defaults to None.</p> <code>None</code> <code>callback</code> <code>Optional[Callable[[Tensor, int], None]]</code> <p>Optional function called during sampling to monitor  progress. It takes the current sample and step number as inputs. Defaults to None.</p> <code>None</code> <code>callback_frequency</code> <code>int</code> <p>How often to call the callback function. Defaults to 50.</p> <code>50</code> <code>guidance</code> <code>Optional[Callable[[Tensor, Tensor], Tensor]]</code> <p>Optional guidance function for conditional sampling. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>A tuple containing the final sample tensor and the final sample </p> <code>Tensor</code> <p>tensor again (for compatibility with the base class interface).</p> Source code in <code>image_gen\\samplers\\euler_maruyama.py</code> <pre><code>def __call__(\n        self,\n        x_T: Tensor,\n        score_model: Callable,\n        *_,\n        n_steps: int = 500,\n        seed: Optional[int] = None,\n        callback: Optional[Callable[[Tensor, int], None]] = None,\n        callback_frequency: int = 50,\n        guidance: Optional[Callable[[Tensor, Tensor], Tensor]] = None,\n        **__\n) -&gt; Tensor:\n    \"\"\"Perform sampling using the Euler-Maruyama method.\n\n    Args:\n        x_T: The initial noise tensor to start sampling from.\n        score_model: The score model function that predicts the score.\n        n_steps: Number of sampling steps. Defaults to 500.\n        seed: Random seed for reproducibility. Defaults to None.\n        callback: Optional function called during sampling to monitor \n            progress. It takes the current sample and step number as inputs.\n            Defaults to None.\n        callback_frequency: How often to call the callback function.\n            Defaults to 50.\n        guidance: Optional guidance function for conditional sampling.\n            Defaults to None.\n\n    Returns:\n        A tuple containing the final sample tensor and the final sample \n        tensor again (for compatibility with the base class interface).\n    \"\"\"\n    if seed is not None:\n        torch.manual_seed(seed)\n\n    device = x_T.device\n    x_t = x_T.clone()\n\n    # Create linearly spaced timesteps from 1.0 to 1e-3\n    times = torch.linspace(1.0, 1e-3, n_steps + 1, device=device)\n    dt = times[0] - times[1]\n\n    # Create progress bar if verbose mode is enabled\n    iterable = (\n        tqdm(range(n_steps), desc='Generating')\n        if self.verbose else range(n_steps)\n    )\n\n    for i in iterable:\n        t_curr = times[i]\n        t_batch = torch.full((x_T.shape[0],), t_curr, device=device)\n        t_for_score = t_batch\n\n        # Handle NaN/Inf values to prevent numerical instability\n        if torch.isnan(x_t).any() or torch.isinf(x_t).any():\n            if self.verbose:\n                print(\n                    f\"Warning: NaN or Inf values detected in x_t at step {i}\"\n                )\n            x_t = torch.nan_to_num(x_t, nan=0.0, posinf=1.0, neginf=-1.0)\n\n        try:\n            # Create a fresh detached copy for gradient computation\n            x_t_detached = x_t.detach().clone()\n            x_t_detached.requires_grad_(True)\n            score = score_model(x_t_detached, t_for_score)\n        except Exception as e:\n            print(f\"Error computing score at step {i}, t={t_curr}: {e}\")\n            score = torch.zeros_like(x_t)\n\n        # Compute drift and diffusion terms for the SDE\n        drift, diffusion = self.diffusion.backward_sde(\n            x_t, t_batch, score, n_steps=n_steps\n        )\n\n        # Handle numerical stability for the diffusion term\n        diffusion = torch.nan_to_num(diffusion, nan=1e-4)\n        noise = torch.randn_like(x_t)\n\n        # Update x_t using the Euler-Maruyama update rule\n        x_t = (\n            x_t +\n            drift * (-dt) +\n            diffusion * torch.sqrt(torch.abs(dt)) * noise\n        )\n\n        # Apply guidance if provided\n        if guidance is not None:\n            x_t = guidance(x_t, t_curr)\n\n        # Clamp values to prevent extreme values\n        x_t = torch.clamp(x_t, -10.0, 10.0)\n\n        # Call callback if provided and at the right frequency\n        if callback and i % callback_frequency == 0:\n            callback(x_t.detach().clone(), i)\n\n    return x_t\n</code></pre>"},{"location":"api/samplers/exponential/","title":"Exponential Integrator","text":"<p>Exponential integrator sampler for diffusion models.</p> <p>This module provides an implementation of the exponential integrator method for sampling from diffusion models, which can be more stable than simpler numerical integration schemes.</p>"},{"location":"api/samplers/exponential/#image_gen.samplers.exponential.ExponentialIntegrator","title":"<code>ExponentialIntegrator</code>","text":"<p>               Bases: <code>BaseSampler</code></p> <p>Exponential integrator for diffusion process sampling.</p> <p>This sampler implements an exponential integration scheme for solving the stochastic differential equation associated with the reverse diffusion process. It can provide better stability properties than simpler methods like Euler-Maruyama.</p> <p>Attributes:</p> Name Type Description <code>diffusion</code> <p>The diffusion model to sample from.</p> <code>verbose</code> <p>Whether to print progress information during sampling.</p> <code>lambda_param</code> <p>The stabilization parameter for the exponential scheme.</p> Source code in <code>image_gen\\samplers\\exponential.py</code> <pre><code>class ExponentialIntegrator(BaseSampler):\n    \"\"\"Exponential integrator for diffusion process sampling.\n\n    This sampler implements an exponential integration scheme for solving\n    the stochastic differential equation associated with the reverse\n    diffusion process. It can provide better stability properties than simpler\n    methods like Euler-Maruyama.\n\n    Attributes:\n        diffusion: The diffusion model to sample from.\n        verbose: Whether to print progress information during sampling.\n        lambda_param: The stabilization parameter for the exponential scheme.\n    \"\"\"\n\n    def __init__(\n        self,\n        diffusion: BaseDiffusion,\n        *args: Any,\n        lambda_param: float = 1.0,\n        verbose: bool = True,\n        **kwargs: Any\n    ):\n        \"\"\"Initialize the exponential integrator sampler.\n\n        Args:\n            diffusion: The diffusion model to sample from.\n            *args: Additional positional arguments.\n            lambda_param: The lambda parameter for the exponential integration.\n                Defaults to 1.0.\n            verbose: Whether to print progress information during sampling.\n                Defaults to True.\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        super().__init__(diffusion, *args, verbose=verbose, **kwargs)\n        self.lambda_param = lambda_param\n\n    def __call__(\n        self,\n        x_T: Tensor,\n        score_model: Callable,\n        *_,\n        n_steps: int = 500,\n        seed: Optional[int] = None,\n        callback: Optional[Callable[[Tensor, int], None]] = None,\n        callback_frequency: int = 50,\n        guidance: Optional[Callable[[Tensor, Tensor], Tensor]] = None,\n        **__\n    ) -&gt; Tensor:\n        \"\"\"Perform sampling using the exponential integrator method.\n\n        Args:\n            x_T: The initial noise tensor to start sampling from.\n            score_model: The score model function that predicts the score.\n            n_steps: Number of sampling steps. Defaults to 500.\n            seed: Random seed for reproducibility. Defaults to None.\n            callback: Optional function called during sampling to monitor \n                progress. It takes the current sample and step number as inputs.\n                Defaults to None.\n            callback_frequency: How often to call the callback function.\n                Defaults to 50.\n            guidance: Optional guidance function for conditional sampling.\n                Defaults to None.\n\n        Returns:\n            A tuple containing the final sample tensor and the final sample\n            tensor again (for compatibility with the base class interface).\n        \"\"\"\n        if seed is not None:\n            torch.manual_seed(seed)\n\n        device = x_T.device\n        x_t = x_T.clone()\n\n        # Generate time steps from 1.0 to 1e-3\n        times = torch.linspace(1.0, 1e-3, n_steps + 1, device=device)\n        dt = times[0] - times[1]\n\n        # Create progress bar if verbose mode is enabled\n        iterable = (\n            tqdm(range(n_steps), desc='Generating')\n            if self.verbose else range(n_steps)\n        )\n\n        for i in iterable:\n            t_curr = times[i]\n            t_batch = torch.full((x_T.shape[0],), t_curr, device=device)\n\n            # Handle NaN/Inf values in x_t for numerical stability\n            if torch.isnan(x_t).any() or torch.isinf(x_t).any():\n                x_t = torch.nan_to_num(\n                    x_t, nan=0.0, posinf=1.0, neginf=-1.0\n                )\n\n            try:\n                # Create a fresh detached copy for gradient computation\n                x_t_detached = x_t.detach().clone()\n                x_t_detached.requires_grad_(True)\n                score = score_model(x_t_detached, t_batch)\n\n            except Exception as e:\n                print(f\"Error computing score at step {i}, t={t_curr}: {e}\")\n                score = torch.zeros_like(x_t)\n\n            # Get drift and diffusion from the backward SDE\n            drift, diffusion = self.diffusion.backward_sde(\n                x_t, t_batch, score, n_steps=n_steps\n            )\n            # Diffusion coefficient for the exponential formula\n            g = diffusion\n\n            # Compute exponential term for the integrator\n            exponential_term = torch.exp(self.lambda_param * dt)\n\n            # Compute the second term in the exponential integration formula\n            second_term = (\n                (g**2 / (2 * self.lambda_param)) *\n                (torch.exp(2 * self.lambda_param * dt) - 1) *\n                score\n            )\n\n            # Add noise term (stochastic component)\n            noise = torch.randn_like(x_t)\n            noise_term = g * torch.sqrt(torch.abs(dt)) * noise\n\n            # Update x_t using the exponential integrator step with noise\n            x_t = x_t * exponential_term + second_term + noise_term\n\n            # Apply guidance if provided\n            if guidance is not None:\n                x_t = guidance(x_t, t_curr)\n\n            # Clamp values to prevent explosion\n            x_t = torch.clamp(x_t, -10.0, 10.0)\n\n            # Invoke callback if needed\n            if callback and i % callback_frequency == 0:\n                callback(x_t.detach().clone(), i)\n\n        return x_t\n\n    def config(self) -&gt; dict:\n        \"\"\"Return the configuration of the sampler.\n\n        Returns:\n            A dictionary with the sampler's configuration parameters.\n        \"\"\"\n        config = super().config()\n        config.update({\n            \"lambda_param\": self.lambda_param\n        })\n        return config\n</code></pre>"},{"location":"api/samplers/exponential/#image_gen.samplers.exponential.ExponentialIntegrator.__call__","title":"<code>__call__(x_T, score_model, *_, n_steps=500, seed=None, callback=None, callback_frequency=50, guidance=None, **__)</code>","text":"<p>Perform sampling using the exponential integrator method.</p> <p>Parameters:</p> Name Type Description Default <code>x_T</code> <code>Tensor</code> <p>The initial noise tensor to start sampling from.</p> required <code>score_model</code> <code>Callable</code> <p>The score model function that predicts the score.</p> required <code>n_steps</code> <code>int</code> <p>Number of sampling steps. Defaults to 500.</p> <code>500</code> <code>seed</code> <code>Optional[int]</code> <p>Random seed for reproducibility. Defaults to None.</p> <code>None</code> <code>callback</code> <code>Optional[Callable[[Tensor, int], None]]</code> <p>Optional function called during sampling to monitor  progress. It takes the current sample and step number as inputs. Defaults to None.</p> <code>None</code> <code>callback_frequency</code> <code>int</code> <p>How often to call the callback function. Defaults to 50.</p> <code>50</code> <code>guidance</code> <code>Optional[Callable[[Tensor, Tensor], Tensor]]</code> <p>Optional guidance function for conditional sampling. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>A tuple containing the final sample tensor and the final sample</p> <code>Tensor</code> <p>tensor again (for compatibility with the base class interface).</p> Source code in <code>image_gen\\samplers\\exponential.py</code> <pre><code>def __call__(\n    self,\n    x_T: Tensor,\n    score_model: Callable,\n    *_,\n    n_steps: int = 500,\n    seed: Optional[int] = None,\n    callback: Optional[Callable[[Tensor, int], None]] = None,\n    callback_frequency: int = 50,\n    guidance: Optional[Callable[[Tensor, Tensor], Tensor]] = None,\n    **__\n) -&gt; Tensor:\n    \"\"\"Perform sampling using the exponential integrator method.\n\n    Args:\n        x_T: The initial noise tensor to start sampling from.\n        score_model: The score model function that predicts the score.\n        n_steps: Number of sampling steps. Defaults to 500.\n        seed: Random seed for reproducibility. Defaults to None.\n        callback: Optional function called during sampling to monitor \n            progress. It takes the current sample and step number as inputs.\n            Defaults to None.\n        callback_frequency: How often to call the callback function.\n            Defaults to 50.\n        guidance: Optional guidance function for conditional sampling.\n            Defaults to None.\n\n    Returns:\n        A tuple containing the final sample tensor and the final sample\n        tensor again (for compatibility with the base class interface).\n    \"\"\"\n    if seed is not None:\n        torch.manual_seed(seed)\n\n    device = x_T.device\n    x_t = x_T.clone()\n\n    # Generate time steps from 1.0 to 1e-3\n    times = torch.linspace(1.0, 1e-3, n_steps + 1, device=device)\n    dt = times[0] - times[1]\n\n    # Create progress bar if verbose mode is enabled\n    iterable = (\n        tqdm(range(n_steps), desc='Generating')\n        if self.verbose else range(n_steps)\n    )\n\n    for i in iterable:\n        t_curr = times[i]\n        t_batch = torch.full((x_T.shape[0],), t_curr, device=device)\n\n        # Handle NaN/Inf values in x_t for numerical stability\n        if torch.isnan(x_t).any() or torch.isinf(x_t).any():\n            x_t = torch.nan_to_num(\n                x_t, nan=0.0, posinf=1.0, neginf=-1.0\n            )\n\n        try:\n            # Create a fresh detached copy for gradient computation\n            x_t_detached = x_t.detach().clone()\n            x_t_detached.requires_grad_(True)\n            score = score_model(x_t_detached, t_batch)\n\n        except Exception as e:\n            print(f\"Error computing score at step {i}, t={t_curr}: {e}\")\n            score = torch.zeros_like(x_t)\n\n        # Get drift and diffusion from the backward SDE\n        drift, diffusion = self.diffusion.backward_sde(\n            x_t, t_batch, score, n_steps=n_steps\n        )\n        # Diffusion coefficient for the exponential formula\n        g = diffusion\n\n        # Compute exponential term for the integrator\n        exponential_term = torch.exp(self.lambda_param * dt)\n\n        # Compute the second term in the exponential integration formula\n        second_term = (\n            (g**2 / (2 * self.lambda_param)) *\n            (torch.exp(2 * self.lambda_param * dt) - 1) *\n            score\n        )\n\n        # Add noise term (stochastic component)\n        noise = torch.randn_like(x_t)\n        noise_term = g * torch.sqrt(torch.abs(dt)) * noise\n\n        # Update x_t using the exponential integrator step with noise\n        x_t = x_t * exponential_term + second_term + noise_term\n\n        # Apply guidance if provided\n        if guidance is not None:\n            x_t = guidance(x_t, t_curr)\n\n        # Clamp values to prevent explosion\n        x_t = torch.clamp(x_t, -10.0, 10.0)\n\n        # Invoke callback if needed\n        if callback and i % callback_frequency == 0:\n            callback(x_t.detach().clone(), i)\n\n    return x_t\n</code></pre>"},{"location":"api/samplers/exponential/#image_gen.samplers.exponential.ExponentialIntegrator.__init__","title":"<code>__init__(diffusion, *args, lambda_param=1.0, verbose=True, **kwargs)</code>","text":"<p>Initialize the exponential integrator sampler.</p> <p>Parameters:</p> Name Type Description Default <code>diffusion</code> <code>BaseDiffusion</code> <p>The diffusion model to sample from.</p> required <code>*args</code> <code>Any</code> <p>Additional positional arguments.</p> <code>()</code> <code>lambda_param</code> <code>float</code> <p>The lambda parameter for the exponential integration. Defaults to 1.0.</p> <code>1.0</code> <code>verbose</code> <code>bool</code> <p>Whether to print progress information during sampling. Defaults to True.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>image_gen\\samplers\\exponential.py</code> <pre><code>def __init__(\n    self,\n    diffusion: BaseDiffusion,\n    *args: Any,\n    lambda_param: float = 1.0,\n    verbose: bool = True,\n    **kwargs: Any\n):\n    \"\"\"Initialize the exponential integrator sampler.\n\n    Args:\n        diffusion: The diffusion model to sample from.\n        *args: Additional positional arguments.\n        lambda_param: The lambda parameter for the exponential integration.\n            Defaults to 1.0.\n        verbose: Whether to print progress information during sampling.\n            Defaults to True.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    super().__init__(diffusion, *args, verbose=verbose, **kwargs)\n    self.lambda_param = lambda_param\n</code></pre>"},{"location":"api/samplers/exponential/#image_gen.samplers.exponential.ExponentialIntegrator.config","title":"<code>config()</code>","text":"<p>Return the configuration of the sampler.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary with the sampler's configuration parameters.</p> Source code in <code>image_gen\\samplers\\exponential.py</code> <pre><code>def config(self) -&gt; dict:\n    \"\"\"Return the configuration of the sampler.\n\n    Returns:\n        A dictionary with the sampler's configuration parameters.\n    \"\"\"\n    config = super().config()\n    config.update({\n        \"lambda_param\": self.lambda_param\n    })\n    return config\n</code></pre>"},{"location":"api/samplers/ode/","title":"ODE Probability Flow","text":"<p>ODE-based probability flow sampler for diffusion models.</p> <p>This module provides an implementation of the Probability Flow ODE sampler for diffusion models, which is a deterministic sampling method based on the probability flow ordinary differential equation.</p>"},{"location":"api/samplers/ode/#image_gen.samplers.ode.ODEProbabilityFlow","title":"<code>ODEProbabilityFlow</code>","text":"<p>               Bases: <code>BaseSampler</code></p> <p>ODE-based probability flow sampler for diffusion models.</p> <p>This sampler implements the probability flow ordinary differential equation (ODE) approach to sampling from diffusion models. Unlike stochastic samplers, this is a deterministic method that follows the probability flow ODE.</p> Source code in <code>image_gen\\samplers\\ode.py</code> <pre><code>class ODEProbabilityFlow(BaseSampler):\n    \"\"\"ODE-based probability flow sampler for diffusion models.\n\n    This sampler implements the probability flow ordinary differential equation\n    (ODE) approach to sampling from diffusion models. Unlike stochastic samplers,\n    this is a deterministic method that follows the probability flow ODE.\n    \"\"\"\n\n    def __call__(\n            self,\n            x_T: Tensor,\n            score_model: Callable,\n            *_,\n            n_steps: int = 500,\n            seed: Optional[int] = None,\n            callback: Optional[Callable[[Tensor, int], None]] = None,\n            callback_frequency: int = 50,\n            guidance: Optional[Callable[[Tensor, Tensor], Tensor]] = None,\n            **__\n    ) -&gt; Tensor:\n        \"\"\"Perform sampling using the probability flow ODE method.\n\n        Args:\n            x_T: The initial noise tensor to start sampling from.\n            score_model: The score model function that predicts the score.\n            n_steps: Number of sampling steps. Defaults to 500.\n            seed: Random seed for reproducibility. Defaults to None.\n            callback: Optional function called during sampling to monitor \n                progress. It takes the current sample and step number as inputs.\n                Defaults to None.\n            callback_frequency: How often to call the callback function.\n                Defaults to 50.\n            guidance: Optional guidance function for conditional sampling.\n                Defaults to None.\n\n        Returns:\n            A tuple containing the final sample tensor and the final sample\n            tensor again (for compatibility with the base class interface).\n        \"\"\"\n        if seed is not None:\n            torch.manual_seed(seed)\n\n        device = x_T.device\n        x_t = x_T.clone()\n\n        # Create linearly spaced timesteps from 1.0 to 1e-3\n        times = torch.linspace(1.0, 1e-3, n_steps + 1, device=device)\n        dt = times[0] - times[1]\n\n        # Create progress bar if verbose mode is enabled\n        iterable = (\n            tqdm(range(n_steps), desc='Generating')\n            if self.verbose else range(n_steps)\n        )\n\n        for i in iterable:\n            t_curr = times[i]\n            t_batch = torch.full((x_T.shape[0],), t_curr, device=device)\n            t_for_score = t_batch\n\n            # Handle NaN/Inf values to ensure numerical stability\n            if torch.isnan(x_t).any() or torch.isinf(x_t).any():\n                if self.verbose:\n                    print(\n                        f\"Warning: NaN or Inf values detected in x_t at step {i}\"\n                    )\n                x_t = torch.nan_to_num(\n                    x_t, nan=0.0, posinf=1.0, neginf=-1.0\n                )\n\n            # Compute score using the provided model\n            try:\n                # Create a fresh detached copy for gradient computation\n                x_t_detached = x_t.detach().clone()\n                x_t_detached.requires_grad_(True)\n                score = score_model(x_t_detached, t_for_score)\n\n            except Exception as e:\n                print(f\"Error computing score at step {i}, t={t_curr}: {e}\")\n                score = torch.zeros_like(x_t)\n\n            # Get drift from backward SDE (ignore diffusion term for ODE)\n            drift, _ = self.diffusion.backward_sde(\n                x_t, t_batch, score, n_steps=n_steps\n            )\n\n            # For Probability Flow ODE, we use only the drift term (no noise)\n            x_t = x_t + drift * (-dt)\n\n            # Apply guidance if provided\n            if guidance is not None:\n                x_t = guidance(x_t, t_curr)\n\n            # Clamp values to prevent extreme values\n            x_t = torch.clamp(x_t, -10.0, 10.0)\n\n            # Call callback if provided and at the right frequency\n            if callback and i % callback_frequency == 0:\n                callback(x_t.detach().clone(), i)\n\n        return x_t\n\n    def config(self) -&gt; dict:\n        \"\"\"Return the configuration of the sampler.\n\n        Returns:\n            A dictionary with the sampler's configuration parameters.\n        \"\"\"\n        return {}\n</code></pre>"},{"location":"api/samplers/ode/#image_gen.samplers.ode.ODEProbabilityFlow.__call__","title":"<code>__call__(x_T, score_model, *_, n_steps=500, seed=None, callback=None, callback_frequency=50, guidance=None, **__)</code>","text":"<p>Perform sampling using the probability flow ODE method.</p> <p>Parameters:</p> Name Type Description Default <code>x_T</code> <code>Tensor</code> <p>The initial noise tensor to start sampling from.</p> required <code>score_model</code> <code>Callable</code> <p>The score model function that predicts the score.</p> required <code>n_steps</code> <code>int</code> <p>Number of sampling steps. Defaults to 500.</p> <code>500</code> <code>seed</code> <code>Optional[int]</code> <p>Random seed for reproducibility. Defaults to None.</p> <code>None</code> <code>callback</code> <code>Optional[Callable[[Tensor, int], None]]</code> <p>Optional function called during sampling to monitor  progress. It takes the current sample and step number as inputs. Defaults to None.</p> <code>None</code> <code>callback_frequency</code> <code>int</code> <p>How often to call the callback function. Defaults to 50.</p> <code>50</code> <code>guidance</code> <code>Optional[Callable[[Tensor, Tensor], Tensor]]</code> <p>Optional guidance function for conditional sampling. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>A tuple containing the final sample tensor and the final sample</p> <code>Tensor</code> <p>tensor again (for compatibility with the base class interface).</p> Source code in <code>image_gen\\samplers\\ode.py</code> <pre><code>def __call__(\n        self,\n        x_T: Tensor,\n        score_model: Callable,\n        *_,\n        n_steps: int = 500,\n        seed: Optional[int] = None,\n        callback: Optional[Callable[[Tensor, int], None]] = None,\n        callback_frequency: int = 50,\n        guidance: Optional[Callable[[Tensor, Tensor], Tensor]] = None,\n        **__\n) -&gt; Tensor:\n    \"\"\"Perform sampling using the probability flow ODE method.\n\n    Args:\n        x_T: The initial noise tensor to start sampling from.\n        score_model: The score model function that predicts the score.\n        n_steps: Number of sampling steps. Defaults to 500.\n        seed: Random seed for reproducibility. Defaults to None.\n        callback: Optional function called during sampling to monitor \n            progress. It takes the current sample and step number as inputs.\n            Defaults to None.\n        callback_frequency: How often to call the callback function.\n            Defaults to 50.\n        guidance: Optional guidance function for conditional sampling.\n            Defaults to None.\n\n    Returns:\n        A tuple containing the final sample tensor and the final sample\n        tensor again (for compatibility with the base class interface).\n    \"\"\"\n    if seed is not None:\n        torch.manual_seed(seed)\n\n    device = x_T.device\n    x_t = x_T.clone()\n\n    # Create linearly spaced timesteps from 1.0 to 1e-3\n    times = torch.linspace(1.0, 1e-3, n_steps + 1, device=device)\n    dt = times[0] - times[1]\n\n    # Create progress bar if verbose mode is enabled\n    iterable = (\n        tqdm(range(n_steps), desc='Generating')\n        if self.verbose else range(n_steps)\n    )\n\n    for i in iterable:\n        t_curr = times[i]\n        t_batch = torch.full((x_T.shape[0],), t_curr, device=device)\n        t_for_score = t_batch\n\n        # Handle NaN/Inf values to ensure numerical stability\n        if torch.isnan(x_t).any() or torch.isinf(x_t).any():\n            if self.verbose:\n                print(\n                    f\"Warning: NaN or Inf values detected in x_t at step {i}\"\n                )\n            x_t = torch.nan_to_num(\n                x_t, nan=0.0, posinf=1.0, neginf=-1.0\n            )\n\n        # Compute score using the provided model\n        try:\n            # Create a fresh detached copy for gradient computation\n            x_t_detached = x_t.detach().clone()\n            x_t_detached.requires_grad_(True)\n            score = score_model(x_t_detached, t_for_score)\n\n        except Exception as e:\n            print(f\"Error computing score at step {i}, t={t_curr}: {e}\")\n            score = torch.zeros_like(x_t)\n\n        # Get drift from backward SDE (ignore diffusion term for ODE)\n        drift, _ = self.diffusion.backward_sde(\n            x_t, t_batch, score, n_steps=n_steps\n        )\n\n        # For Probability Flow ODE, we use only the drift term (no noise)\n        x_t = x_t + drift * (-dt)\n\n        # Apply guidance if provided\n        if guidance is not None:\n            x_t = guidance(x_t, t_curr)\n\n        # Clamp values to prevent extreme values\n        x_t = torch.clamp(x_t, -10.0, 10.0)\n\n        # Call callback if provided and at the right frequency\n        if callback and i % callback_frequency == 0:\n            callback(x_t.detach().clone(), i)\n\n    return x_t\n</code></pre>"},{"location":"api/samplers/ode/#image_gen.samplers.ode.ODEProbabilityFlow.config","title":"<code>config()</code>","text":"<p>Return the configuration of the sampler.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary with the sampler's configuration parameters.</p> Source code in <code>image_gen\\samplers\\ode.py</code> <pre><code>def config(self) -&gt; dict:\n    \"\"\"Return the configuration of the sampler.\n\n    Returns:\n        A dictionary with the sampler's configuration parameters.\n    \"\"\"\n    return {}\n</code></pre>"},{"location":"api/samplers/predictor_corrector/","title":"Predictor-Corrector","text":"<p>Predictor-Corrector sampler for diffusion models.</p> <p>This module provides an implementation of the Predictor-Corrector sampling method for diffusion models, which combines a predictor step (similar to Euler-Maruyama) with a corrector step based on Langevin dynamics.</p>"},{"location":"api/samplers/predictor_corrector/#image_gen.samplers.predictor_corrector.PredictorCorrector","title":"<code>PredictorCorrector</code>","text":"<p>               Bases: <code>BaseSampler</code></p> <p>Predictor-Corrector sampler for diffusion models.</p> <p>This sampler implements the Predictor-Corrector method, which alternates between a prediction step and a correction step to improve sampling quality.</p> <p>Attributes:</p> Name Type Description <code>diffusion</code> <p>The diffusion model to sample from.</p> <code>verbose</code> <p>Whether to print progress information during sampling.</p> <code>corrector_steps</code> <p>Number of correction steps per prediction step.</p> <code>corrector_snr</code> <p>Signal-to-noise ratio for the corrector step.</p> Source code in <code>image_gen\\samplers\\predictor_corrector.py</code> <pre><code>class PredictorCorrector(BaseSampler):\n    \"\"\"Predictor-Corrector sampler for diffusion models.\n\n    This sampler implements the Predictor-Corrector method, which alternates\n    between a prediction step and a correction step to improve sampling quality.\n\n    Attributes:\n        diffusion: The diffusion model to sample from.\n        verbose: Whether to print progress information during sampling.\n        corrector_steps: Number of correction steps per prediction step.\n        corrector_snr: Signal-to-noise ratio for the corrector step.\n    \"\"\"\n\n    def __init__(\n        self,\n        diffusion: BaseDiffusion,\n        *args: Any,\n        verbose: bool = True,\n        corrector_steps: int = 1,\n        corrector_snr: float = 0.15,\n        **kwargs: Any\n    ):\n        \"\"\"Initialize the Predictor-Corrector sampler.\n\n        Args:\n            diffusion: The diffusion model to sample from.\n            *args: Additional positional arguments.\n            verbose: Whether to print progress information during sampling.\n                Defaults to True.\n            corrector_steps: Number of correction steps per prediction step.\n                Defaults to 1.\n            corrector_snr: Signal-to-noise ratio for the corrector step.\n                Controls the noise magnitude. Defaults to 0.15.\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        super().__init__(diffusion, *args, verbose=verbose, **kwargs)\n        self.corrector_steps = corrector_steps\n        self.corrector_snr = corrector_snr\n\n    def predictor_step(\n            self,\n            x_t: Tensor,\n            t_curr: Tensor,\n            t_next: Tensor,\n            score: Tensor,\n            *args: Any,\n            **kwargs: Any\n    ) -&gt; Tensor:\n        \"\"\"Perform a predictor step (similar to Euler-Maruyama).\n\n        Args:\n            x_t: Current state tensor.\n            t_curr: Current time step.\n            t_next: Next time step.\n            score: Score estimate at current step.\n            *args: Additional positional arguments.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            Updated tensor after prediction step.\n        \"\"\"\n        # Ensure dt has the correct dimensions for broadcasting\n        dt = (t_curr - t_next).view(-1, 1, 1, 1)\n\n        # Get drift and diffusion\n        drift, diffusion = self.diffusion.backward_sde(\n            x_t, t_curr, score, *args, **kwargs\n        )\n        diffusion = torch.nan_to_num(diffusion, nan=1e-4)\n        noise = torch.randn_like(x_t)\n\n        # Apply Euler step with correct dimensions\n        dt_sqrt = torch.sqrt(torch.abs(dt))\n        x_next = x_t + drift * (-dt) + diffusion * dt_sqrt * noise\n        return x_next\n\n    def corrector_step(\n            self,\n            x_t: Tensor,\n            t: Tensor,\n            score_model: Callable,\n            *_,\n            **__\n    ) -&gt; Tensor:\n        \"\"\"Perform a corrector step based on Langevin dynamics.\n\n        Args:\n            x_t: Current state tensor.\n            t: Current time step.\n            score_model: Model function that predicts the score.\n\n        Returns:\n            Updated tensor after correction step.\n        \"\"\"\n        try:\n            with torch.enable_grad():\n                x_t.requires_grad_(True)\n                score = score_model(x_t, t)\n                x_t.requires_grad_(False)\n\n            if torch.isnan(score).any():\n                score = torch.nan_to_num(score, nan=0.0)\n\n            # Estimate corrector noise scale based on SNR\n            noise_scale = torch.sqrt(\n                torch.tensor(2.0 * self.corrector_snr, device=x_t.device)\n            )\n            noise = torch.randn_like(x_t)\n\n            # Carefully compute score norm\n            # Use a small epsilon value to avoid division by zero\n            epsilon = 1e-10\n            score_norm = torch.norm(\n                score.view(score.shape[0], -1), dim=1, keepdim=True\n            ).view(-1, 1, 1, 1)\n            score_norm = torch.maximum(\n                score_norm, torch.tensor(epsilon, device=score_norm.device)\n            )\n\n            # Calculate step size with correct broadcasting\n            step_size = (\n                self.corrector_snr / (score_norm ** 2)\n            ).view(-1, 1, 1, 1)\n            step_size = torch.nan_to_num(step_size, nan=1e-10)\n\n            # Apply corrector step with proper broadcasting\n            sqrt_step = torch.sqrt(step_size)\n            x_t_corrected = (\n                x_t +\n                step_size * score +\n                noise_scale * sqrt_step * noise\n            )\n            return x_t_corrected\n\n        except IndexError as e:\n            if self.verbose:\n                print(\n                    f\"IndexError in corrector_step: {e}. Skipping correction.\"\n                )\n            # If an index error occurs, simply return unmodified x_t\n            return x_t\n\n    def __call__(\n            self,\n            x_T: Tensor,\n            score_model: Callable,\n            *_,\n            n_steps: int = 500,\n            seed: Optional[int] = None,\n            callback: Optional[Callable[[Tensor, int], None]] = None,\n            callback_frequency: int = 50,\n            guidance: Optional[Callable[[Tensor, Tensor], Tensor]] = None,\n            **__\n    ) -&gt; Tensor:\n        \"\"\"Perform sampling using the predictor-corrector method.\n\n        Args:\n            x_T: The initial noise tensor to start sampling from.\n            score_model: The score model function that predicts the score.\n            n_steps: Number of sampling steps. Defaults to 500.\n            seed: Random seed for reproducibility. Defaults to None.\n            callback: Optional function called during sampling to monitor \n                progress. It takes the current sample and step number as inputs.\n                Defaults to None.\n            callback_frequency: How often to call the callback function.\n                Defaults to 50.\n            guidance: Optional guidance function for conditional sampling.\n                Defaults to None.\n\n        Returns:\n            A tuple containing the final sample tensor and the final sample\n            tensor again (for compatibility with the base class interface).\n        \"\"\"\n        if seed is not None:\n            torch.manual_seed(seed)\n\n        device = x_T.device\n        x_t = x_T.clone()\n\n        # Generate time steps\n        times = torch.linspace(1.0, 1e-3, n_steps + 1, device=device)\n\n        # Create progress bar if verbose mode is enabled\n        iterable = (\n            tqdm(range(n_steps), desc='Generating')\n            if self.verbose else range(n_steps)\n        )\n\n        for i in iterable:\n            t_curr = times[i]\n            t_next = times[i + 1]\n\n            # Create time tensors with appropriate batch dimensions\n            batch_size = x_T.shape[0]\n            t_batch = torch.full((batch_size,), t_curr, device=device)\n            t_next_batch = torch.full((batch_size,), t_next, device=device)\n\n            # Handle NaN/Inf values for numerical stability\n            if torch.isnan(x_t).any() or torch.isinf(x_t).any():\n                if self.verbose:\n                    print(\n                        f\"Warning: NaN or Inf values detected in x_t at step {i}\"\n                    )\n                x_t = torch.nan_to_num(\n                    x_t, nan=0.0, posinf=1.0, neginf=-1.0\n                )\n\n            # Step 1: Predictor\n            try:\n                # Create a fresh detached copy for gradient computation\n                x_t_detached = x_t.detach().clone()\n                x_t_detached.requires_grad_(True)\n                score = score_model(x_t_detached, t_batch)\n\n            except Exception as e:\n                print(f\"Error computing score at step {i}, t={t_curr}: {e}\")\n                score = torch.zeros_like(x_t)\n\n            # Apply predictor step\n            x_t = self.predictor_step(\n                x_t, t_batch, t_next_batch, score, n_steps=n_steps\n            )\n\n            # Step 2: Corrector (Langevin MCMC)\n            # Ensure the corrector step properly handles class labels\n            try:\n                for j in range(self.corrector_steps):\n                    x_t = self.corrector_step(\n                        x_t, t_next_batch, score_model, n_steps=n_steps\n                    )\n            except Exception as e:\n                if self.verbose:\n                    print(\n                        f\"Error in corrector step: {e}. \"\n                        f\"Continuing without correction.\"\n                    )\n\n            # Apply guidance if provided\n            if guidance is not None:\n                try:\n                    x_t = guidance(x_t, t_next)\n                except Exception as e:\n                    if self.verbose:\n                        print(\n                            f\"Error in guidance: {e}. \"\n                            f\"Continuing without applying guidance.\"\n                        )\n\n            # Stabilization\n            x_t = torch.clamp(x_t, -10.0, 10.0)\n\n            # Call callback if provided and at the right frequency\n            if callback and i % callback_frequency == 0:\n                callback(x_t.detach().clone(), i)\n\n        return x_t\n\n    def config(self) -&gt; dict:\n        \"\"\"Return the configuration of the sampler.\n\n        Returns:\n            A dictionary with the sampler's configuration parameters.\n        \"\"\"\n        config = super().config()\n        config.update({\n            \"corrector_steps\": self.corrector_steps,\n            \"corrector_snr\": self.corrector_snr,\n        })\n        return config\n</code></pre>"},{"location":"api/samplers/predictor_corrector/#image_gen.samplers.predictor_corrector.PredictorCorrector.__call__","title":"<code>__call__(x_T, score_model, *_, n_steps=500, seed=None, callback=None, callback_frequency=50, guidance=None, **__)</code>","text":"<p>Perform sampling using the predictor-corrector method.</p> <p>Parameters:</p> Name Type Description Default <code>x_T</code> <code>Tensor</code> <p>The initial noise tensor to start sampling from.</p> required <code>score_model</code> <code>Callable</code> <p>The score model function that predicts the score.</p> required <code>n_steps</code> <code>int</code> <p>Number of sampling steps. Defaults to 500.</p> <code>500</code> <code>seed</code> <code>Optional[int]</code> <p>Random seed for reproducibility. Defaults to None.</p> <code>None</code> <code>callback</code> <code>Optional[Callable[[Tensor, int], None]]</code> <p>Optional function called during sampling to monitor  progress. It takes the current sample and step number as inputs. Defaults to None.</p> <code>None</code> <code>callback_frequency</code> <code>int</code> <p>How often to call the callback function. Defaults to 50.</p> <code>50</code> <code>guidance</code> <code>Optional[Callable[[Tensor, Tensor], Tensor]]</code> <p>Optional guidance function for conditional sampling. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>A tuple containing the final sample tensor and the final sample</p> <code>Tensor</code> <p>tensor again (for compatibility with the base class interface).</p> Source code in <code>image_gen\\samplers\\predictor_corrector.py</code> <pre><code>def __call__(\n        self,\n        x_T: Tensor,\n        score_model: Callable,\n        *_,\n        n_steps: int = 500,\n        seed: Optional[int] = None,\n        callback: Optional[Callable[[Tensor, int], None]] = None,\n        callback_frequency: int = 50,\n        guidance: Optional[Callable[[Tensor, Tensor], Tensor]] = None,\n        **__\n) -&gt; Tensor:\n    \"\"\"Perform sampling using the predictor-corrector method.\n\n    Args:\n        x_T: The initial noise tensor to start sampling from.\n        score_model: The score model function that predicts the score.\n        n_steps: Number of sampling steps. Defaults to 500.\n        seed: Random seed for reproducibility. Defaults to None.\n        callback: Optional function called during sampling to monitor \n            progress. It takes the current sample and step number as inputs.\n            Defaults to None.\n        callback_frequency: How often to call the callback function.\n            Defaults to 50.\n        guidance: Optional guidance function for conditional sampling.\n            Defaults to None.\n\n    Returns:\n        A tuple containing the final sample tensor and the final sample\n        tensor again (for compatibility with the base class interface).\n    \"\"\"\n    if seed is not None:\n        torch.manual_seed(seed)\n\n    device = x_T.device\n    x_t = x_T.clone()\n\n    # Generate time steps\n    times = torch.linspace(1.0, 1e-3, n_steps + 1, device=device)\n\n    # Create progress bar if verbose mode is enabled\n    iterable = (\n        tqdm(range(n_steps), desc='Generating')\n        if self.verbose else range(n_steps)\n    )\n\n    for i in iterable:\n        t_curr = times[i]\n        t_next = times[i + 1]\n\n        # Create time tensors with appropriate batch dimensions\n        batch_size = x_T.shape[0]\n        t_batch = torch.full((batch_size,), t_curr, device=device)\n        t_next_batch = torch.full((batch_size,), t_next, device=device)\n\n        # Handle NaN/Inf values for numerical stability\n        if torch.isnan(x_t).any() or torch.isinf(x_t).any():\n            if self.verbose:\n                print(\n                    f\"Warning: NaN or Inf values detected in x_t at step {i}\"\n                )\n            x_t = torch.nan_to_num(\n                x_t, nan=0.0, posinf=1.0, neginf=-1.0\n            )\n\n        # Step 1: Predictor\n        try:\n            # Create a fresh detached copy for gradient computation\n            x_t_detached = x_t.detach().clone()\n            x_t_detached.requires_grad_(True)\n            score = score_model(x_t_detached, t_batch)\n\n        except Exception as e:\n            print(f\"Error computing score at step {i}, t={t_curr}: {e}\")\n            score = torch.zeros_like(x_t)\n\n        # Apply predictor step\n        x_t = self.predictor_step(\n            x_t, t_batch, t_next_batch, score, n_steps=n_steps\n        )\n\n        # Step 2: Corrector (Langevin MCMC)\n        # Ensure the corrector step properly handles class labels\n        try:\n            for j in range(self.corrector_steps):\n                x_t = self.corrector_step(\n                    x_t, t_next_batch, score_model, n_steps=n_steps\n                )\n        except Exception as e:\n            if self.verbose:\n                print(\n                    f\"Error in corrector step: {e}. \"\n                    f\"Continuing without correction.\"\n                )\n\n        # Apply guidance if provided\n        if guidance is not None:\n            try:\n                x_t = guidance(x_t, t_next)\n            except Exception as e:\n                if self.verbose:\n                    print(\n                        f\"Error in guidance: {e}. \"\n                        f\"Continuing without applying guidance.\"\n                    )\n\n        # Stabilization\n        x_t = torch.clamp(x_t, -10.0, 10.0)\n\n        # Call callback if provided and at the right frequency\n        if callback and i % callback_frequency == 0:\n            callback(x_t.detach().clone(), i)\n\n    return x_t\n</code></pre>"},{"location":"api/samplers/predictor_corrector/#image_gen.samplers.predictor_corrector.PredictorCorrector.__init__","title":"<code>__init__(diffusion, *args, verbose=True, corrector_steps=1, corrector_snr=0.15, **kwargs)</code>","text":"<p>Initialize the Predictor-Corrector sampler.</p> <p>Parameters:</p> Name Type Description Default <code>diffusion</code> <code>BaseDiffusion</code> <p>The diffusion model to sample from.</p> required <code>*args</code> <code>Any</code> <p>Additional positional arguments.</p> <code>()</code> <code>verbose</code> <code>bool</code> <p>Whether to print progress information during sampling. Defaults to True.</p> <code>True</code> <code>corrector_steps</code> <code>int</code> <p>Number of correction steps per prediction step. Defaults to 1.</p> <code>1</code> <code>corrector_snr</code> <code>float</code> <p>Signal-to-noise ratio for the corrector step. Controls the noise magnitude. Defaults to 0.15.</p> <code>0.15</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>image_gen\\samplers\\predictor_corrector.py</code> <pre><code>def __init__(\n    self,\n    diffusion: BaseDiffusion,\n    *args: Any,\n    verbose: bool = True,\n    corrector_steps: int = 1,\n    corrector_snr: float = 0.15,\n    **kwargs: Any\n):\n    \"\"\"Initialize the Predictor-Corrector sampler.\n\n    Args:\n        diffusion: The diffusion model to sample from.\n        *args: Additional positional arguments.\n        verbose: Whether to print progress information during sampling.\n            Defaults to True.\n        corrector_steps: Number of correction steps per prediction step.\n            Defaults to 1.\n        corrector_snr: Signal-to-noise ratio for the corrector step.\n            Controls the noise magnitude. Defaults to 0.15.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    super().__init__(diffusion, *args, verbose=verbose, **kwargs)\n    self.corrector_steps = corrector_steps\n    self.corrector_snr = corrector_snr\n</code></pre>"},{"location":"api/samplers/predictor_corrector/#image_gen.samplers.predictor_corrector.PredictorCorrector.config","title":"<code>config()</code>","text":"<p>Return the configuration of the sampler.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary with the sampler's configuration parameters.</p> Source code in <code>image_gen\\samplers\\predictor_corrector.py</code> <pre><code>def config(self) -&gt; dict:\n    \"\"\"Return the configuration of the sampler.\n\n    Returns:\n        A dictionary with the sampler's configuration parameters.\n    \"\"\"\n    config = super().config()\n    config.update({\n        \"corrector_steps\": self.corrector_steps,\n        \"corrector_snr\": self.corrector_snr,\n    })\n    return config\n</code></pre>"},{"location":"api/samplers/predictor_corrector/#image_gen.samplers.predictor_corrector.PredictorCorrector.corrector_step","title":"<code>corrector_step(x_t, t, score_model, *_, **__)</code>","text":"<p>Perform a corrector step based on Langevin dynamics.</p> <p>Parameters:</p> Name Type Description Default <code>x_t</code> <code>Tensor</code> <p>Current state tensor.</p> required <code>t</code> <code>Tensor</code> <p>Current time step.</p> required <code>score_model</code> <code>Callable</code> <p>Model function that predicts the score.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Updated tensor after correction step.</p> Source code in <code>image_gen\\samplers\\predictor_corrector.py</code> <pre><code>def corrector_step(\n        self,\n        x_t: Tensor,\n        t: Tensor,\n        score_model: Callable,\n        *_,\n        **__\n) -&gt; Tensor:\n    \"\"\"Perform a corrector step based on Langevin dynamics.\n\n    Args:\n        x_t: Current state tensor.\n        t: Current time step.\n        score_model: Model function that predicts the score.\n\n    Returns:\n        Updated tensor after correction step.\n    \"\"\"\n    try:\n        with torch.enable_grad():\n            x_t.requires_grad_(True)\n            score = score_model(x_t, t)\n            x_t.requires_grad_(False)\n\n        if torch.isnan(score).any():\n            score = torch.nan_to_num(score, nan=0.0)\n\n        # Estimate corrector noise scale based on SNR\n        noise_scale = torch.sqrt(\n            torch.tensor(2.0 * self.corrector_snr, device=x_t.device)\n        )\n        noise = torch.randn_like(x_t)\n\n        # Carefully compute score norm\n        # Use a small epsilon value to avoid division by zero\n        epsilon = 1e-10\n        score_norm = torch.norm(\n            score.view(score.shape[0], -1), dim=1, keepdim=True\n        ).view(-1, 1, 1, 1)\n        score_norm = torch.maximum(\n            score_norm, torch.tensor(epsilon, device=score_norm.device)\n        )\n\n        # Calculate step size with correct broadcasting\n        step_size = (\n            self.corrector_snr / (score_norm ** 2)\n        ).view(-1, 1, 1, 1)\n        step_size = torch.nan_to_num(step_size, nan=1e-10)\n\n        # Apply corrector step with proper broadcasting\n        sqrt_step = torch.sqrt(step_size)\n        x_t_corrected = (\n            x_t +\n            step_size * score +\n            noise_scale * sqrt_step * noise\n        )\n        return x_t_corrected\n\n    except IndexError as e:\n        if self.verbose:\n            print(\n                f\"IndexError in corrector_step: {e}. Skipping correction.\"\n            )\n        # If an index error occurs, simply return unmodified x_t\n        return x_t\n</code></pre>"},{"location":"api/samplers/predictor_corrector/#image_gen.samplers.predictor_corrector.PredictorCorrector.predictor_step","title":"<code>predictor_step(x_t, t_curr, t_next, score, *args, **kwargs)</code>","text":"<p>Perform a predictor step (similar to Euler-Maruyama).</p> <p>Parameters:</p> Name Type Description Default <code>x_t</code> <code>Tensor</code> <p>Current state tensor.</p> required <code>t_curr</code> <code>Tensor</code> <p>Current time step.</p> required <code>t_next</code> <code>Tensor</code> <p>Next time step.</p> required <code>score</code> <code>Tensor</code> <p>Score estimate at current step.</p> required <code>*args</code> <code>Any</code> <p>Additional positional arguments.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Updated tensor after prediction step.</p> Source code in <code>image_gen\\samplers\\predictor_corrector.py</code> <pre><code>def predictor_step(\n        self,\n        x_t: Tensor,\n        t_curr: Tensor,\n        t_next: Tensor,\n        score: Tensor,\n        *args: Any,\n        **kwargs: Any\n) -&gt; Tensor:\n    \"\"\"Perform a predictor step (similar to Euler-Maruyama).\n\n    Args:\n        x_t: Current state tensor.\n        t_curr: Current time step.\n        t_next: Next time step.\n        score: Score estimate at current step.\n        *args: Additional positional arguments.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        Updated tensor after prediction step.\n    \"\"\"\n    # Ensure dt has the correct dimensions for broadcasting\n    dt = (t_curr - t_next).view(-1, 1, 1, 1)\n\n    # Get drift and diffusion\n    drift, diffusion = self.diffusion.backward_sde(\n        x_t, t_curr, score, *args, **kwargs\n    )\n    diffusion = torch.nan_to_num(diffusion, nan=1e-4)\n    noise = torch.randn_like(x_t)\n\n    # Apply Euler step with correct dimensions\n    dt_sqrt = torch.sqrt(torch.abs(dt))\n    x_next = x_t + drift * (-dt) + diffusion * dt_sqrt * noise\n    return x_next\n</code></pre>"},{"location":"examples/basic_usage/","title":"Basic Usage","text":"<p>This example demonstrates the basic usage of the GenerativeModel class for training and generating images.</p>"},{"location":"examples/basic_usage/#setup","title":"Setup","text":"<p>First, import the necessary modules and initialize the generative model.</p> <pre><code>import torch\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\nfrom image_gen import GenerativeModel\n\n# Initialize a generative model with Variance Exploding diffusion and Euler-Maruyama sampler\nmodel = GenerativeModel(diffusion=\"ve\", sampler=\"euler-maruyama\")\n</code></pre>"},{"location":"examples/basic_usage/#training","title":"Training","text":"<p>Load a dataset and train the model.</p> <pre><code># Load MNIST dataset\ndata = datasets.MNIST(\n    root='data',\n    train=True,\n    download=True,\n    transform=ToTensor()\n)\n\n# Train the model\nmodel.train(data, epochs=50, batch_size=32)\n</code></pre>"},{"location":"examples/basic_usage/#generation","title":"Generation","text":"<p>Generate new images using the trained model.</p> <pre><code># Generate 16 images\ngenerated_images = model.generate(num_samples=16, n_steps=500)\n\n# Display the generated images\nfrom image_gen.visualization import display_images\ndisplay_images(generated_images)\n</code></pre>"},{"location":"examples/basic_usage/#saving-and-loading","title":"Saving and Loading","text":"<p>Save the trained model and load it later for inference.</p> <pre><code># Save the model\nmodel.save(\"saved_models/mnist_model.pth\")\n\n# Load the model\nmodel.load(\"saved_models/mnist_model.pth\")\n</code></pre>"},{"location":"examples/basic_usage/#visualization","title":"Visualization","text":"<p>Visualize the generation process step by step.</p> <pre><code>from image_gen.visualization import create_evolution_widget\n\n# Create an animation showing the generation process\nanimation = create_evolution_widget(model)\nanimation\n</code></pre>"},{"location":"examples/colorization/","title":"Colorization","text":"<p>This example demonstrates how to perform image colorization using the generative model.</p>"},{"location":"examples/colorization/#setup","title":"Setup","text":"<p>Import the necessary modules and initialize the generative model.</p> <pre><code>import torch\nfrom torchvision import datasets, transforms\nfrom torchvision.transforms import ToTensor\nfrom image_gen import GenerativeModel\n\n# Initialize a generative model with Variance Exploding diffusion and Euler-Maruyama sampler\nmodel = GenerativeModel(diffusion=\"ve\", sampler=\"euler-maruyama\")\n</code></pre>"},{"location":"examples/colorization/#training","title":"Training","text":"<p>Load a dataset and train the model.</p> <pre><code># Load CIFAR-10 dataset\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\ndata = datasets.CIFAR10(\n    root='data',\n    train=True,\n    download=True,\n    transform=transform\n)\n\n# Select a subset for faster training\ntargets = torch.tensor(data.targets)\nidx = (targets == 1).nonzero().flatten()\ndata = torch.utils.data.Subset(data, idx)\n\n# Train the model\nmodel.train(data, epochs=500, batch_size=32)\n</code></pre>"},{"location":"examples/colorization/#colorization_1","title":"Colorization","text":"<p>Colorize a grayscale image using the trained model.</p> <pre><code># Generate a base image\ngenerated_image = model.generate(num_samples=1)\n\n# Convert to grayscale\ngray_image = torch.mean(generated_image, dim=1, keepdim=True)\n\n# Display original and grayscale images\ndisplay_images(generated_image)\ndisplay_images(gray_image)\n\n# Colorize the grayscale image\ncolorized = model.colorize(gray_image)\ndisplay_images(colorized)\n\n# Generate multiple color variations\ngray_batch = gray_image.repeat(16, 1, 1, 1)\ncolorized_batch = model.colorize(gray_batch)\ndisplay_images(colorized_batch)\n</code></pre>"},{"location":"examples/conditional_generation/","title":"Conditional Generation","text":"<p>This example demonstrates how to perform conditional generation using class labels.</p>"},{"location":"examples/conditional_generation/#setup","title":"Setup","text":"<p>Import the necessary modules and initialize the generative model.</p> <pre><code>import torch\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\nfrom image_gen import GenerativeModel\n\n# Initialize a generative model with Variance Preserving diffusion and Exponential Integrator sampler\nmodel = GenerativeModel(\n    diffusion=\"vp\",\n    sampler=\"exponential\",\n    noise_schedule=\"linear\"\n)\n</code></pre>"},{"location":"examples/conditional_generation/#training","title":"Training","text":"<p>Load a dataset and train the model.</p> <pre><code># Load MNIST dataset\ndata = datasets.MNIST(\n    root='data',\n    train=True,\n    download=True,\n    transform=ToTensor()\n)\n\n# Train the model\nmodel.train(data, epochs=50, batch_size=32)\n</code></pre>"},{"location":"examples/conditional_generation/#conditional-generation_1","title":"Conditional Generation","text":"<p>Generate images conditioned on specific class labels.</p> <pre><code># Generate 16 samples from class 7\nsamples = model.generate(num_samples=16, class_labels=7)\ndisplay_images(samples)\n\n# Generate specific classes for each sample\nlabels = torch.repeat_interleave(torch.arange(0, model.num_classes), 2)\nsamples = model.generate(num_samples=len(labels), class_labels=labels)\ndisplay_images(samples)\n</code></pre>"},{"location":"examples/conditional_generation/#visualization","title":"Visualization","text":"<p>Visualize the effect of different guidance scales.</p> <pre><code>from image_gen.visualization import create_evolution_widget\n\n# Create an animation showing the generation process for class 9\nanimation = create_evolution_widget(model, class_labels=9)\nanimation\n\n# Compare different guidance scales\nfig, axs = plt.subplots(2, 4, figsize=(15, 8))\nfor i, scale in enumerate([0, 0.5, 1, 2, 3, 5, 7.5, 10]):\n    samples = model.generate(num_samples=1, class_labels=6, guidance_scale=scale)\n    axs[i//4, i%4].imshow(samples[0].permute(1, 2, 0), cmap=\"gray\")\n    axs[i//4, i%4].set_title(f'Scale={scale}')\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"examples/imputation/","title":"Imputation","text":"<p>This example demonstrates how to perform image inpainting using the generative model.</p>"},{"location":"examples/imputation/#setup","title":"Setup","text":"<p>Import the necessary modules and initialize the generative model.</p> <pre><code>import torch\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\nfrom image_gen import GenerativeModel\n\n# Initialize a generative model with Variance Preserving diffusion and Exponential Integrator sampler\nmodel = GenerativeModel(\n    diffusion=\"vp\",\n    sampler=\"exponential\",\n    noise_schedule=\"linear\"\n)\n</code></pre>"},{"location":"examples/imputation/#training","title":"Training","text":"<p>Load a dataset and train the model.</p> <pre><code># Load MNIST dataset\ndata = datasets.MNIST(\n    root='data',\n    train=True,\n    download=True,\n    transform=ToTensor()\n)\n\n# Select a subset for faster training\nindices = torch.where(data.targets == 3)[0]\ndataset = torch.utils.data.Subset(data, indices)\n\n# Train the model\nmodel.train(dataset, epochs=50, batch_size=32)\n</code></pre>"},{"location":"examples/imputation/#imputation_1","title":"Imputation","text":"<p>Perform inpainting by providing a mask.</p> <pre><code># Generate a base image\nbase_image = model.generate(num_samples=1)\n\n# Create a center rectangle mask\nmask = torch.ones_like(base_image)\nh, w = base_image.shape[2], base_image.shape[3]\nmask[:, :, h//4:3*h//4, w//4:3*w//4] = 0\n\n# Create a batch of images with the same mask\nmask_batch = mask.repeat(16, 1, 1, 1)\nbase_image_batch = base_image.repeat(16, 1, 1, 1)\n\n# Perform inpainting\nresults_batch = model.imputation(base_image_batch, mask_batch, n_steps=500)\ndisplay_images(results_batch)\n</code></pre>"}]}